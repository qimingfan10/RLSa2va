"""
æ­£ç¡®çš„Sa2VA HuggingFaceæ¨¡å‹æ¨ç† - 10å¼ å›¾ç‰‡è¯„ä¼°
ä½¿ç”¨å®˜æ–¹æ¨èçš„predict_forwardæ–¹æ³•
"""
import os
import sys
import json
import random
import numpy as np
import torch
from PIL import Image
import matplotlib.pyplot as plt
import cv2
from sklearn.metrics import jaccard_score, f1_score, precision_score, recall_score, accuracy_score
from transformers import AutoModelForCausalLM, AutoTokenizer

sys.path.insert(0, '/home/ubuntu/Sa2VA')

print("=" * 80)
print("Sa2VAæ­£ç¡®æ¨ç† - 10å¼ å›¾ç‰‡è¯„ä¼°")
print("=" * 80)

# é…ç½®
HF_MODEL_PATH = "/home/ubuntu/Sa2VA/work_dirs/sa2va_26b_dpo_xtuner/final"
DATA_ROOT = "/home/ubuntu/Sa2VA/data/merged_vessel_data/"
OUTPUT_DIR = "/home/ubuntu/Sa2VA/evaluation_10_images_results"
NUM_SAMPLES = 10

print(f"HFæ¨¡å‹è·¯å¾„: {HF_MODEL_PATH}")
print(f"æ•°æ®è·¯å¾„: {DATA_ROOT}")
print(f"è¯„ä¼°æ ·æœ¬æ•°: {NUM_SAMPLES}")
print()

os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(os.path.join(OUTPUT_DIR, "predictions"), exist_ok=True)

# è¯„ä»·æŒ‡æ ‡è®¡ç®—
def calculate_metrics(pred_mask, gt_mask):
    """è®¡ç®—åˆ†å‰²è¯„ä»·æŒ‡æ ‡"""
    pred_flat = (pred_mask > 127).flatten().astype(int)
    gt_flat = (gt_mask > 127).flatten().astype(int)
    
    # å¤„ç†å…¨é›¶æˆ–å…¨ä¸€çš„æƒ…å†µ
    if len(np.unique(gt_flat)) == 1 and len(np.unique(pred_flat)) == 1:
        if gt_flat[0] == pred_flat[0]:
            return {
                'IoU': 1.0, 
                'Dice': 1.0, 
                'Precision': 1.0, 
                'Recall': 1.0, 
                'Accuracy': 1.0, 
                'Pixel_Accuracy': 1.0
            }
        else:
            return {
                'IoU': 0.0, 
                'Dice': 0.0, 
                'Precision': 0.0, 
                'Recall': 0.0, 
                'Accuracy': 0.0, 
                'Pixel_Accuracy': 0.0
            }
    
    # è®¡ç®—å„é¡¹æŒ‡æ ‡
    iou = jaccard_score(gt_flat, pred_flat, zero_division=0)
    dice = f1_score(gt_flat, pred_flat, zero_division=0)
    precision = precision_score(gt_flat, pred_flat, zero_division=0)
    recall = recall_score(gt_flat, pred_flat, zero_division=0)
    accuracy = accuracy_score(gt_flat, pred_flat)
    pixel_acc = np.sum(pred_flat == gt_flat) / len(gt_flat)
    
    return {
        'IoU': float(iou),
        'Dice': float(dice),
        'Precision': float(precision),
        'Recall': float(recall),
        'Accuracy': float(accuracy),
        'Pixel_Accuracy': float(pixel_acc)
    }

# åŠ è½½HuggingFaceæ¨¡å‹
print("=" * 80)
print("æ­¥éª¤1: åŠ è½½HuggingFaceæ¨¡å‹")
print("=" * 80)

try:
    print("åŠ è½½tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(
        HF_MODEL_PATH,
        trust_remote_code=True
    )
    print("âœ… TokenizeråŠ è½½æˆåŠŸ")
    
    print("\nåŠ è½½æ¨¡å‹...")
    model = AutoModelForCausalLM.from_pretrained(
        HF_MODEL_PATH,
        torch_dtype="auto",
        device_map="auto",
        trust_remote_code=True,
        low_cpu_mem_usage=True
    )
    print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    print(f"è®¾å¤‡åˆ†é…: {model.hf_device_map}")
    
    model.eval()
    MODEL_LOADED = True
    
except Exception as e:
    print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    import traceback
    traceback.print_exc()
    MODEL_LOADED = False
    exit(1)

# åŠ è½½æ•°æ®é›†
print("\n" + "=" * 80)
print("æ­¥éª¤2: åŠ è½½æ•°æ®é›†")
print("=" * 80)

with open(os.path.join(DATA_ROOT, "annotations.json")) as f:
    dataset = json.load(f)

print(f"æ•°æ®é›†æ€»æ•°: {len(dataset)}")

# éšæœºé€‰æ‹©10å¼ å›¾ç‰‡
random.seed(42)
test_samples = random.sample(dataset, NUM_SAMPLES)

print(f"å·²é€‰æ‹© {NUM_SAMPLES} å¼ å›¾ç‰‡è¿›è¡Œè¯„ä¼°")
for i, sample in enumerate(test_samples, 1):
    print(f"  {i}. {sample['image']}")
print()

# æ¨ç†å’Œè¯„ä¼°
print("=" * 80)
print("æ­¥éª¤3: æ¨ç†å’Œè¯„ä¼°")
print("=" * 80)

all_metrics = []
results = []
successful_inferences = 0
failed_inferences = 0

for idx, sample in enumerate(test_samples):
    print(f"\n[{idx+1}/{NUM_SAMPLES}] å¤„ç†: {sample['image']}")
    print("-" * 80)
    
    # åŠ è½½å›¾ç‰‡
    img_path = os.path.join(DATA_ROOT, "images", sample['image'])
    if not os.path.exists(img_path):
        print(f"  âŒ å›¾ç‰‡ä¸å­˜åœ¨: {img_path}")
        failed_inferences += 1
        continue
    
    image = Image.open(img_path).convert('RGB')
    image_np = np.array(image)
    h, w = image_np.shape[:2]
    
    print(f"  å›¾ç‰‡å°ºå¯¸: {w} x {h}")
    
    # åˆ›å»ºGround Truth mask
    gt_mask = np.zeros((h, w), dtype=np.uint8)
    for mask_coords in sample['mask']:
        if len(mask_coords) >= 6:
            points = np.array(mask_coords).reshape(-1, 2).astype(np.int32)
            cv2.fillPoly(gt_mask, [points], 255)
    
    gt_pixels = np.sum(gt_mask > 127)
    print(f"  GTåƒç´ æ•°: {gt_pixels} ({gt_pixels/(h*w)*100:.2f}%)")
    
    # ä½¿ç”¨HuggingFaceæ¨¡å‹è¿›è¡Œæ¨ç†
    print(f"  ğŸ”„ ä½¿ç”¨predict_forwardæ¨ç†...")
    
    try:
        text = "<image>Please segment the blood vessel."
        
        result = model.predict_forward(
            image=image,
            text=text,
            tokenizer=tokenizer,
            processor=None,
        )
        
        prediction_text = result.get('prediction', '')
        print(f"  ğŸ“ æ¨¡å‹è¾“å‡º: {prediction_text}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰åˆ†å‰²ç»“æœ
        if '[SEG]' in prediction_text and 'prediction_masks' in result:
            pred_masks = result['prediction_masks']
            
            if len(pred_masks) > 0:
                pred_mask = pred_masks[0][0]  # [seg_idx][frame_idx]
                
                # è½¬æ¢ä¸ºnumpyæ•°ç»„
                if isinstance(pred_mask, torch.Tensor):
                    pred_mask = pred_mask.cpu().numpy()
                
                # è°ƒæ•´å°ºå¯¸åˆ°åŸå›¾
                if pred_mask.shape != (h, w):
                    pred_mask = cv2.resize(pred_mask, (w, h), interpolation=cv2.INTER_NEAREST)
                
                # è½¬æ¢ä¸ºäºŒå€¼mask
                if pred_mask.max() <= 1.0:
                    pred_mask = (pred_mask > 0.5).astype(np.uint8) * 255
                else:
                    pred_mask = (pred_mask > 127).astype(np.uint8) * 255
                
                pred_pixels = np.sum(pred_mask > 127)
                print(f"  âœ… é¢„æµ‹æˆåŠŸï¼é¢„æµ‹åƒç´ æ•°: {pred_pixels} ({pred_pixels/(h*w)*100:.2f}%)")
                successful_inferences += 1
            else:
                print(f"  âš ï¸  æ²¡æœ‰åˆ†å‰²ç»“æœï¼Œä½¿ç”¨ç©ºmask")
                pred_mask = np.zeros((h, w), dtype=np.uint8)
                failed_inferences += 1
        else:
            print(f"  âš ï¸  è¾“å‡ºä¸­æ²¡æœ‰[SEG]æ ‡è®°ï¼Œä½¿ç”¨ç©ºmask")
            pred_mask = np.zeros((h, w), dtype=np.uint8)
            failed_inferences += 1
    
    except Exception as e:
        print(f"  âŒ æ¨ç†å¤±è´¥: {e}")
        pred_mask = np.zeros((h, w), dtype=np.uint8)
        failed_inferences += 1
    
    # è®¡ç®—è¯„ä»·æŒ‡æ ‡
    metrics = calculate_metrics(pred_mask, gt_mask)
    all_metrics.append(metrics)
    
    print(f"  ğŸ“Š è¯„ä»·æŒ‡æ ‡:")
    print(f"     IoU (Jaccard):    {metrics['IoU']:.4f}")
    print(f"     Dice Score:       {metrics['Dice']:.4f}")
    print(f"     Precision:        {metrics['Precision']:.4f}")
    print(f"     Recall:           {metrics['Recall']:.4f}")
    print(f"     Accuracy:         {metrics['Accuracy']:.4f}")
    print(f"     Pixel Accuracy:   {metrics['Pixel_Accuracy']:.4f}")
    
    # å¯è§†åŒ–
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    
    # åŸå›¾
    axes[0, 0].imshow(image_np)
    axes[0, 0].set_title('Original Image', fontsize=14, fontweight='bold')
    axes[0, 0].axis('off')
    
    # GT mask
    axes[0, 1].imshow(gt_mask, cmap='gray')
    axes[0, 1].set_title('Ground Truth Mask', fontsize=14, fontweight='bold')
    axes[0, 1].axis('off')
    
    # é¢„æµ‹mask
    axes[0, 2].imshow(pred_mask, cmap='gray')
    title = 'Sa2VA Prediction (predict_forward)'
    axes[0, 2].set_title(title, fontsize=14, fontweight='bold')
    axes[0, 2].axis('off')
    
    # GTå åŠ 
    axes[1, 0].imshow(image_np)
    axes[1, 0].imshow(gt_mask, alpha=0.5, cmap='Reds')
    axes[1, 0].set_title('GT Overlay', fontsize=14, fontweight='bold')
    axes[1, 0].axis('off')
    
    # é¢„æµ‹å åŠ 
    axes[1, 1].imshow(image_np)
    axes[1, 1].imshow(pred_mask, alpha=0.5, cmap='Greens')
    axes[1, 1].set_title('Prediction Overlay', fontsize=14, fontweight='bold')
    axes[1, 1].axis('off')
    
    # å·®å¼‚å›¾
    diff = np.abs(pred_mask.astype(float) - gt_mask.astype(float))
    axes[1, 2].imshow(diff, cmap='hot')
    axes[1, 2].set_title(
        f'Difference Map\nIoU={metrics["IoU"]:.3f}, Dice={metrics["Dice"]:.3f}', 
        fontsize=14, fontweight='bold'
    )
    axes[1, 2].axis('off')
    
    plt.tight_layout()
    output_filename = f"eval_{idx+1:02d}_{os.path.basename(sample['image'])}"
    output_path = os.path.join(OUTPUT_DIR, "predictions", output_filename)
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    plt.close()
    
    print(f"  ğŸ’¾ ä¿å­˜åˆ°: {output_path}")
    
    results.append({
        'sample_id': idx + 1,
        'image': sample['image'],
        'image_size': [w, h],
        'gt_pixels': int(gt_pixels),
        'pred_pixels': int(np.sum(pred_mask > 127)),
        'inference_success': successful_inferences > failed_inferences,
        'metrics': metrics,
        'output': output_path
    })

# æ€»ä½“è¯„ä¼°
print("\n" + "=" * 80)
print("æ­¥éª¤4: æ€»ä½“è¯„ä¼°ç»“æœ")
print("=" * 80)

if len(all_metrics) > 0:
    avg_metrics = {
        key: np.mean([m[key] for m in all_metrics])
        for key in all_metrics[0].keys()
    }
    
    print(f"\næ¨ç†ç»Ÿè®¡:")
    print(f"  æˆåŠŸæ¨ç†: {successful_inferences}/{NUM_SAMPLES}")
    print(f"  å¤±è´¥æ¨ç†: {failed_inferences}/{NUM_SAMPLES}")
    print(f"  æˆåŠŸç‡: {successful_inferences/NUM_SAMPLES*100:.1f}%")
    
    print(f"\nå¹³å‡æŒ‡æ ‡ (åŸºäº {len(all_metrics)} ä¸ªæ ·æœ¬):")
    print(f"  IoU (Jaccard):      {avg_metrics['IoU']:.4f}")
    print(f"  Dice Score:         {avg_metrics['Dice']:.4f}")
    print(f"  Precision:          {avg_metrics['Precision']:.4f}")
    print(f"  Recall:             {avg_metrics['Recall']:.4f}")
    print(f"  Accuracy:           {avg_metrics['Accuracy']:.4f}")
    print(f"  Pixel Accuracy:     {avg_metrics['Pixel_Accuracy']:.4f}")
    
    # é€ä¸ªæ ·æœ¬æŒ‡æ ‡
    print(f"\né€ä¸ªæ ·æœ¬æŒ‡æ ‡:")
    print(f"{'ID':<4} {'Image':<50} {'IoU':<8} {'Dice':<8} {'Prec':<8} {'Rec':<8}")
    print("-" * 90)
    for result in results:
        img_name = result['image'][:46] + "..." if len(result['image']) > 50 else result['image']
        m = result['metrics']
        print(f"{result['sample_id']:<4} {img_name:<50} {m['IoU']:<8.4f} {m['Dice']:<8.4f} {m['Precision']:<8.4f} {m['Recall']:<8.4f}")
    
    # ä¿å­˜ç»“æœ
    detailed_results = {
        'model_path': HF_MODEL_PATH,
        'model_type': 'HuggingFace Sa2VA-26B',
        'inference_method': 'predict_forward (official)',
        'successful_inferences': successful_inferences,
        'failed_inferences': failed_inferences,
        'total_samples': NUM_SAMPLES,
        'success_rate': successful_inferences / NUM_SAMPLES,
        'average_metrics': avg_metrics,
        'per_sample_results': results
    }
    
    results_path = os.path.join(OUTPUT_DIR, "evaluation_results.json")
    with open(results_path, 'w', encoding='utf-8') as f:
        json.dump(detailed_results, f, indent=2, ensure_ascii=False)
    
    print(f"\nâœ… è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {results_path}")
    
    # ç”ŸæˆMarkdownæŠ¥å‘Š
    report_path = os.path.join(OUTPUT_DIR, "evaluation_report.md")
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("# Sa2VAè¡€ç®¡åˆ†å‰²æ¨¡å‹è¯„ä¼°æŠ¥å‘Š\n\n")
        f.write(f"## æ¨¡å‹ä¿¡æ¯\n\n")
        f.write(f"- **æ¨¡å‹**: Sa2VA-26B (HuggingFaceæ ¼å¼)\n")
        f.write(f"- **æ¨ç†æ–¹æ³•**: `predict_forward` (å®˜æ–¹æ¨è)\n")
        f.write(f"- **è¯„ä¼°æ ·æœ¬æ•°**: {NUM_SAMPLES}\n\n")
        
        f.write(f"## æ¨ç†ç»Ÿè®¡\n\n")
        f.write(f"- **æˆåŠŸ**: {successful_inferences}/{NUM_SAMPLES} ({successful_inferences/NUM_SAMPLES*100:.1f}%)\n")
        f.write(f"- **å¤±è´¥**: {failed_inferences}/{NUM_SAMPLES}\n\n")
        
        f.write(f"## å¹³å‡è¯„ä»·æŒ‡æ ‡\n\n")
        f.write(f"| æŒ‡æ ‡ | æ•°å€¼ |\n")
        f.write(f"|------|------|\n")
        f.write(f"| IoU (Jaccard) | {avg_metrics['IoU']:.4f} |\n")
        f.write(f"| Dice Score | {avg_metrics['Dice']:.4f} |\n")
        f.write(f"| Precision | {avg_metrics['Precision']:.4f} |\n")
        f.write(f"| Recall | {avg_metrics['Recall']:.4f} |\n")
        f.write(f"| Accuracy | {avg_metrics['Accuracy']:.4f} |\n")
        f.write(f"| Pixel Accuracy | {avg_metrics['Pixel_Accuracy']:.4f} |\n\n")
        
        f.write(f"## é€æ ·æœ¬ç»“æœ\n\n")
        f.write(f"| ID | å›¾ç‰‡ | IoU | Dice | Precision | Recall |\n")
        f.write(f"|----|------|-----|------|-----------|--------|\n")
        for result in results:
            m = result['metrics']
            f.write(f"| {result['sample_id']} | {result['image']} | {m['IoU']:.4f} | {m['Dice']:.4f} | {m['Precision']:.4f} | {m['Recall']:.4f} |\n")
    
    print(f"âœ… MarkdownæŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")

print("\n" + "=" * 80)
print("ğŸ‰ è¯„ä¼°å®Œæˆï¼")
print("=" * 80)
print(f"\nç»“æœç›®å½•: {OUTPUT_DIR}")
print(f"  - å¯è§†åŒ–å›¾ç‰‡: {OUTPUT_DIR}/predictions/")
print(f"  - JSONç»“æœ: {OUTPUT_DIR}/evaluation_results.json")
print(f"  - MarkdownæŠ¥å‘Š: {OUTPUT_DIR}/evaluation_report.md")
print()
print("=" * 80)
