# Sa2VAè®­ç»ƒæ¨¡å‹æ¨ç†æŒ‡å—

## ğŸ“Š å½“å‰çŠ¶æ€

### âœ… å·²å®Œæˆ
- è®­ç»ƒå®Œæˆ (3672æ­¥ï¼ŒLoss: 13.76 â†’ 1.08)
- æ¨¡å‹æƒé‡å·²ä¿å­˜ (`iter_3672.pth`, 2.5GB)
- Ground Truthå¯è§†åŒ–å·²ç”Ÿæˆ (5ä¸ªæ ·æœ¬)

### ğŸ“ å¯è§†åŒ–ç»“æœä½ç½®
```bash
/home/ubuntu/Sa2VA/inference_results/predictions/
â”œâ”€â”€ sample_1_Chen_Fang_0000103366__1-4_1_04B2D3CF_frame_000034.jpg
â”œâ”€â”€ sample_2_Bai_Hui_Min_0000202318__1-3_1_04DB6FD9_frame_000045.jpg
â”œâ”€â”€ sample_3_Gong_Chao_0000838952__1-2_1_0487E196_frame_000033.jpg
â”œâ”€â”€ sample_4_Feng_Wan_Chang_0000889954__1-3_1_04CE6CAA_frame_000009.jpg
â””â”€â”€ sample_5_Fang_Kun__0000470101__1-3_1_04A2C7DE_frame_000059.jpg
```

æ¯å¼ å›¾ç‰‡åŒ…å«4ä¸ªå­å›¾ï¼š
1. **Original** - åŸå§‹å›¾åƒ
2. **Ground Truth** - æ ‡æ³¨çš„çœŸå®mask (çº¢è‰²)
3. **Prediction** - æ¨¡å‹é¢„æµ‹ç»“æœ (ç»¿è‰²)
4. **Overlay** - å åŠ å¯¹æ¯” (çº¢è‰²=GT, ç»¿è‰²=é¢„æµ‹)

---

## ğŸš€ ä½¿ç”¨è®­ç»ƒæƒé‡è¿›è¡Œå®é™…æ¨ç†

ç”±äºSa2VAæ˜¯mmengineæ ¼å¼çš„æ¨¡å‹ï¼Œæœ‰ä»¥ä¸‹å‡ ç§æ¨ç†æ–¹æ¡ˆï¼š

### æ–¹æ¡ˆ1: è½¬æ¢ä¸ºHuggingFaceæ ¼å¼ (æ¨è) â­

#### æ­¥éª¤1: è½¬æ¢æ¨¡å‹

```bash
cd /home/ubuntu/Sa2VA

# ä½¿ç”¨å®˜æ–¹è½¬æ¢è„šæœ¬
python tools/convert_to_hf.py \
    --model_path projects/sa2va/configs/sa2va_merged_vessel_finetune.py \
    --ckpt_path work_dirs/merged_vessel_segmentation/iter_3672.pth \
    --save_path models/sa2va_vessel_hf
```

**æ³¨æ„**: æ­¤æ­¥éª¤éœ€è¦åœ¨`topo-sarl`ç¯å¢ƒä¸­è¿è¡Œï¼Œå› ä¸ºéœ€è¦mmengineã€‚

#### æ­¥éª¤2: ä½¿ç”¨HuggingFaceæ¨¡å‹æ¨ç†

```python
from transformers import AutoModel, AutoTokenizer
from PIL import Image
import torch

# åŠ è½½æ¨¡å‹
model = AutoModel.from_pretrained(
    "models/sa2va_vessel_hf", 
    trust_remote_code=True
)
tokenizer = AutoTokenizer.from_pretrained(
    "models/sa2va_vessel_hf",
    trust_remote_code=True
)

device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
model = model.to(device)
model.eval()

# åŠ è½½å›¾ç‰‡
image = Image.open("path/to/image.jpg").convert('RGB')

# æ¨ç†
with torch.no_grad():
    result = model.predict_forward(
        image=image,
        text="blood vessel",
        tokenizer=tokenizer
    )
    
    # è·å–é¢„æµ‹mask
    pred_masks = result['prediction_masks']
```

---

### æ–¹æ¡ˆ2: ä½¿ç”¨mmengine Runner

å¦‚æœåœ¨`topo-sarl`ç¯å¢ƒä¸­ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨mmengineï¼š

```python
from mmengine.config import Config
from mmengine.runner import Runner
import torch

# åŠ è½½é…ç½®
cfg = Config.fromfile('projects/sa2va/configs/sa2va_merged_vessel_finetune.py')

# è®¾ç½®checkpoint
cfg.load_from = 'work_dirs/merged_vessel_segmentation/iter_3672.pth'

# åˆ›å»ºrunner
runner = Runner.from_cfg(cfg)

# è¿›è¡Œæ¨ç†
# (éœ€è¦æ ¹æ®Sa2VAçš„å…·ä½“APIå®ç°)
```

---

### æ–¹æ¡ˆ3: ä½¿ç”¨å®˜æ–¹è¯„ä¼°è„šæœ¬

Sa2VAæä¾›äº†è¯„ä¼°è„šæœ¬ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨ï¼š

```bash
cd /home/ubuntu/Sa2VA

# ä½¿ç”¨å®˜æ–¹è¯„ä¼°è„šæœ¬
python projects/sa2va/evaluation/sa2va_eval_refcoco.py \
    --model_path models/sa2va_vessel_hf \
    --data_path data/merged_vessel_data \
    --output_dir evaluation_results
```

---

## ğŸ”§ ç¯å¢ƒè¦æ±‚

### è½¬æ¢æ¨¡å‹éœ€è¦
```bash
# åœ¨topo-sarlç¯å¢ƒä¸­
- mmengine
- torch
- transformers
- å®Œæ•´çš„Sa2VAä¾èµ–
```

### HuggingFaceæ¨ç†éœ€è¦
```bash
# å¯ä»¥åœ¨æ™®é€šPythonç¯å¢ƒ
- transformers
- torch
- PIL
- numpy
```

---

## ğŸ“ å¿«é€Ÿå¼€å§‹è„šæœ¬

### ä¸€é”®è½¬æ¢å’Œæ¨ç†

```bash
chmod +x /home/ubuntu/Sa2VA/convert_and_inference.sh
/home/ubuntu/Sa2VA/convert_and_inference.sh
```

è¿™ä¸ªè„šæœ¬ä¼šï¼š
1. æ£€æŸ¥HuggingFaceæ¨¡å‹æ˜¯å¦å­˜åœ¨
2. å¦‚æœä¸å­˜åœ¨ï¼Œè½¬æ¢mmengineæ¨¡å‹
3. ä½¿ç”¨è½¬æ¢åçš„æ¨¡å‹è¿›è¡Œæ¨ç†
4. ç”Ÿæˆå¯è§†åŒ–ç»“æœ

---

## ğŸ¯ å½“å‰å¯è§†åŒ–è¯´æ˜

å½“å‰`inference_results/predictions/`ä¸­çš„å›¾ç‰‡æ˜¯**Ground Truthå¯è§†åŒ–**ï¼Œå› ä¸ºï¼š

1. **ç¯å¢ƒé™åˆ¶**: ç³»ç»ŸPythonç¯å¢ƒæ²¡æœ‰mmengine
2. **ä¸´æ—¶æ–¹æ¡ˆ**: å…ˆå¯è§†åŒ–GTä½œä¸ºå‚è€ƒ
3. **ä¸‹ä¸€æ­¥**: éœ€è¦è½¬æ¢æ¨¡å‹æˆ–åœ¨æ­£ç¡®ç¯å¢ƒä¸­è¿è¡Œ

### å¯è§†åŒ–æ ¼å¼

æ¯å¼ å›¾ç‰‡åŒ…å«4ä¸ªé¢æ¿ï¼š
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Original   â”‚ Ground Truthâ”‚ Prediction  â”‚   Overlay   â”‚
â”‚             â”‚   (çº¢è‰²)     â”‚  (ç»¿è‰²)      â”‚  (çº¢+ç»¿)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

å½“å‰Predictioné¢æ¿æ˜¾ç¤ºçš„æ˜¯Ground Truthï¼ˆå› ä¸ºæ¨¡å‹æœªåŠ è½½ï¼‰ã€‚

---

## ğŸ” éªŒè¯æ¨¡å‹æƒé‡

### æ£€æŸ¥checkpointå†…å®¹

```python
import torch

ckpt = torch.load(
    'work_dirs/merged_vessel_segmentation/iter_3672.pth',
    map_location='cpu',
    weights_only=False
)

print("Checkpoint keys:", ckpt.keys())
print("Meta info:", ckpt.get('meta', {}))
print("State dict keys:", list(ckpt['state_dict'].keys())[:10])
```

### é¢„æœŸè¾“å‡º
```
Checkpoint keys: dict_keys(['state_dict', 'meta', 'optimizer', ...])
Meta info: {'iter': 3672, 'epoch': 3, ...}
State dict keys: ['mllm.model.embed_tokens.weight', ...]
```

---

## ğŸ“Š æ¨ç†æ€§èƒ½é¢„ä¼°

åŸºäºè®­ç»ƒé…ç½®ï¼š

| æŒ‡æ ‡ | å€¼ |
|------|-----|
| æ¨¡å‹å¤§å° | 2.5GB |
| æ¨èæ˜¾å­˜ | â‰¥24GB (å•GPU) |
| FP16æ¨ç† | ~12GBæ˜¾å­˜ |
| æ¨ç†é€Ÿåº¦ | ~1-2ç§’/å›¾ (512Ã—512) |
| Batchæ¨ç† | æ”¯æŒ (æ ¹æ®æ˜¾å­˜) |

---

## âš ï¸ å¸¸è§é—®é¢˜

### Q1: ä¸ºä»€ä¹ˆå½“å‰å¯è§†åŒ–æ˜¾ç¤ºçš„æ˜¯Ground Truthï¼Ÿ

**A**: å› ä¸ºç³»ç»ŸPythonç¯å¢ƒç¼ºå°‘mmengineï¼Œæ¨¡å‹æ— æ³•åŠ è½½ã€‚éœ€è¦ï¼š
- åœ¨`topo-sarl`ç¯å¢ƒä¸­è¿è¡Œï¼Œæˆ–
- è½¬æ¢ä¸ºHuggingFaceæ ¼å¼åæ¨ç†

### Q2: å¦‚ä½•åœ¨topo-sarlç¯å¢ƒä¸­è¿è¡Œï¼Ÿ

**A**: 
```bash
# æ–¹æ³•1: ä½¿ç”¨micromamba
~/micromamba/bin/micromamba run -n topo-sarl python inference_with_trained_model.py

# æ–¹æ³•2: æ¿€æ´»ç¯å¢ƒåè¿è¡Œ
eval "$(~/micromamba/bin/micromamba shell hook --shell bash)"
micromamba activate topo-sarl
python inference_with_trained_model.py
```

### Q3: è½¬æ¢å¤±è´¥æ€ä¹ˆåŠï¼Ÿ

**A**: æ£€æŸ¥ï¼š
1. æ˜¯å¦åœ¨topo-sarlç¯å¢ƒä¸­
2. mmengineæ˜¯å¦æ­£ç¡®å®‰è£…
3. checkpointè·¯å¾„æ˜¯å¦æ­£ç¡®
4. é…ç½®æ–‡ä»¶æ˜¯å¦å­˜åœ¨

### Q4: å¦‚ä½•è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Ÿ

**A**: 
```python
from sklearn.metrics import jaccard_score, f1_score

# è®¡ç®—IoU
iou = jaccard_score(gt_mask.flatten(), pred_mask.flatten())

# è®¡ç®—Dice Score
dice = f1_score(gt_mask.flatten(), pred_mask.flatten())

print(f"IoU: {iou:.4f}")
print(f"Dice: {dice:.4f}")
```

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- `TRAINING_ANALYSIS_REPORT.md` - è®­ç»ƒè¯¦ç»†åˆ†æ
- `TRAINING_COMPLETE_SUMMARY.md` - è®­ç»ƒå®Œæ•´æ€»ç»“
- `inference_results/README.md` - æ¨ç†ç»“æœè¯´æ˜
- `tools/convert_to_hf.py` - æ¨¡å‹è½¬æ¢è„šæœ¬

---

## ğŸ“ æ¨èå·¥ä½œæµç¨‹

### å®Œæ•´æ¨ç†æµç¨‹

```bash
# 1. è½¬æ¢æ¨¡å‹ (åœ¨topo-sarlç¯å¢ƒ)
python tools/convert_to_hf.py \
    --model_path projects/sa2va/configs/sa2va_merged_vessel_finetune.py \
    --ckpt_path work_dirs/merged_vessel_segmentation/iter_3672.pth \
    --save_path models/sa2va_vessel_hf

# 2. æ¨ç† (å¯ä»¥åœ¨ä»»ä½•ç¯å¢ƒ)
python hf_inference_script.py

# 3. è¯„ä¼°
python evaluate_predictions.py

# 4. å¯è§†åŒ–
python visualize_results.py
```

---

## ğŸ’¡ ä¸‹ä¸€æ­¥å»ºè®®

1. **ç«‹å³å¯åš**:
   - æŸ¥çœ‹å½“å‰GTå¯è§†åŒ–: `ls -lh inference_results/predictions/`
   - é˜…è¯»è®­ç»ƒåˆ†ææŠ¥å‘Š

2. **éœ€è¦ç¯å¢ƒ**:
   - è½¬æ¢æ¨¡å‹ä¸ºHuggingFaceæ ¼å¼
   - è¿›è¡Œå®é™…æ¨¡å‹æ¨ç†

3. **è¿›é˜¶ä»»åŠ¡**:
   - åœ¨æµ‹è¯•é›†ä¸Šå…¨é¢è¯„ä¼°
   - è®¡ç®—IoUã€Diceç­‰æŒ‡æ ‡
   - ä¸baselineæ¨¡å‹å¯¹æ¯”

---

**æ–‡æ¡£æ›´æ–°æ—¶é—´**: 2025-11-25  
**æ¨¡å‹ç‰ˆæœ¬**: iter_3672.pth  
**è®­ç»ƒLoss**: 13.76 â†’ 1.08 (â†“92.2%)
