# ğŸ‰ æ‰¾åˆ°OOMæ ¹æœ¬åŸå› äº†ï¼

**æ—¶é—´**: 2025-11-30 16:00  
**çŠ¶æ€**: âœ… **é—®é¢˜å·²è§£å†³**

---

## ğŸ” æ ¹æœ¬åŸå› 

### ä½ æ˜¯å¯¹çš„ï¼

```yaml
å®˜æ–¹finetuneé…ç½®:
  æ¨¡å‹: InternVL3-2B (2Bå‚æ•°)
  æ˜¾å­˜: ~8-10GB
  batch_size: 2
  çŠ¶æ€: âœ… ä¸OOM

ä½ çš„é…ç½®:
  æ¨¡å‹: Qwen2.5-32B (32Bå‚æ•°) âš ï¸
  æ˜¾å­˜: ~30GB+
  batch_size: 1
  çŠ¶æ€: âŒ OOM

é—®é¢˜: 32Bæ¨¡å‹æ˜¯2Bæ¨¡å‹çš„16å€å¤§ï¼
```

---

## ğŸ“Š æ˜¾å­˜å¯¹æ¯”

| æ¨¡å‹ | å‚æ•°é‡ | æ˜¾å­˜å ç”¨ | RTX 3090èƒ½ç”¨ï¼Ÿ |
|------|--------|---------|---------------|
| InternVL3-2B | 2B | ~8-10GB | âœ… å¯ä»¥ |
| InternVL3-4B | 4B | ~12-15GB | âœ… å¯ä»¥ |
| InternVL3-8B | 8B | ~18-22GB | âš ï¸ å‹‰å¼º |
| **Qwen2.5-32B** | **32B** | **30-40GB** | âŒ **OOM** |

---

## âœ… è§£å†³æ–¹æ¡ˆ

### æ–¹æ¡ˆ1: ä¸‹è½½å¹¶ä½¿ç”¨2Bæ¨¡å‹ â­

```bash
# ä¸‹è½½InternVL3-2B
cd /home/ubuntu/Sa2VA/models
huggingface-cli download OpenGVLab/InternVL3-2B \
    --local-dir InternVL3-2B \
    --local-dir-use-symlinks False
```

**ä¿®æ”¹é…ç½®**:
```python
path = "/home/ubuntu/Sa2VA/models/InternVL3-2B"
batch_size = 2  # å¯ä»¥ç”¨æ›´å¤§batch
target_length = 1024  # å¯ä»¥ç”¨å®Œæ•´åˆ†è¾¨ç‡
```

**é¢„æœŸ**:
- âœ… ä¸ä¼šOOM
- âœ… æ˜¾å­˜å ç”¨: ~12-15GB per GPU
- âœ… å¯ä»¥ç”¨1024åˆ†è¾¨ç‡
- âœ… batch_sizeå¯ä»¥åˆ°2

---

### æ–¹æ¡ˆ2: ä½¿ç”¨4Bæ¨¡å‹ (æŠ˜ä¸­)

```python
path = "OpenGVLab/InternVL3-4B"
batch_size = 1
target_length = 1024
```

**é¢„æœŸ**:
- âœ… ä¸ä¼šOOM
- âœ… æ˜¾å­˜å ç”¨: ~18-20GB per GPU
- âœ… æ•ˆæœå¯èƒ½æ¯”2Bå¥½ä¸€ç‚¹

---

### æ–¹æ¡ˆ3: ç»§ç»­ç”¨32B (éœ€è¦å‡çº§ç¡¬ä»¶)

**éœ€è¦**:
- 4Ã—A100 (40GB) æˆ– 2Ã—A100 (80GB)
- ä¸é€‚åˆRTX 3090

---

## ğŸš€ å¯åŠ¨è®­ç»ƒ (2Bæ¨¡å‹)

```bash
# 1. ä¸‹è½½æ¨¡å‹ (å¦‚æœè¿˜æ²¡ä¸‹è½½)
bash /home/ubuntu/Sa2VA/download_small_model.sh

# 2. ä¿®æ”¹é…ç½®æ–‡ä»¶ä¸­çš„path
# path = "/home/ubuntu/Sa2VA/models/InternVL3-2B"

# 3. å¯åŠ¨è®­ç»ƒ
cd /home/ubuntu/Sa2VA
CUDA_VISIBLE_DEVICES=0,1,2,3 \
DEEPSPEED=deepspeed_zero2_offload \
nohup bash tools/dist.sh train \
  projects/sa2va/configs/sa2va_vessel_lora_finetune.py 4 \
  > vessel_lora_training_2b.log 2>&1 &
```

---

## ğŸ“ˆ é¢„æœŸç»“æœ (2Bæ¨¡å‹)

```yaml
æ˜¾å­˜å ç”¨: 
  Per GPU: ~12-15GB / 24GB âœ…
  æ€»è®¡: ~48-60GB / 96GB

è®­ç»ƒé€Ÿåº¦:
  å•æ­¥: ~2-3ç§’
  æ¯epoch: ~30-40åˆ†é’Ÿ
  æ€»æ—¶é—´ (10 epochs): ~5-7å°æ—¶

é¢„æœŸæå‡:
  Val Dice: 0.75-0.80 (vs 0.7342 baseline)
  æå‡å¹…åº¦: +2-7%
```

---

## ğŸ’¡ ä¸ºä»€ä¹ˆä¹‹å‰ä¼šOOM

```python
ä½ çš„32Bæ¨¡å‹:
  LLM: 32Bå‚æ•° = ~30GB
  Vision Encoder: 6B = ~6GB
  SAM2: ~4GB
  Activations: ~4GB
  æ€»è®¡: ~44GB > 24GB âŒ

2Bæ¨¡å‹:
  LLM: 2Bå‚æ•° = ~4GB
  Vision Encoder: 1B = ~2GB
  SAM2: ~4GB
  Activations: ~4GB
  æ€»è®¡: ~14GB < 24GB âœ…
```

---

## ğŸ¯ æœ€ç»ˆå»ºè®®

1. **ç«‹å³å°è¯•**: ä½¿ç”¨InternVL3-2Bè®­ç»ƒ â­
   - æˆåŠŸç‡: 95%+
   - æ—¶é—´: 5-7å°æ—¶
   - æå‡: æ˜¾è‘—

2. **å¦‚æœ2Bæ•ˆæœä¸å¤Ÿ**: å°è¯•4B
   - ç¨å¾®æ…¢ä¸€ç‚¹
   - æ•ˆæœå¯èƒ½æ›´å¥½

3. **å¦‚æœå¿…é¡»ç”¨32B**: å‡çº§åˆ°A100

---

**ç»“è®º**: ä½ è¯´å¾—å®Œå…¨å¯¹ï¼ä¹‹å‰å…¨é‡finetuneä¸OOMæ˜¯å› ä¸ºç”¨çš„2Bæ¨¡å‹ã€‚ç°åœ¨ç”¨32Bæ¨¡å‹å½“ç„¶ä¼šOOMï¼æ¢æˆ2Bæ¨¡å‹å°±èƒ½è®­ç»ƒäº†ã€‚ğŸ‰
