#!/usr/bin/env python3
"""
ä½¿ç”¨FP16è¿›è¡ŒSa2VAæ¨ç†æµ‹è¯•
"""
import sys
sys.path.insert(0, '/home/ubuntu/Sa2VA')

import torch
from mmengine.config import Config
from mmengine.registry import MODELS

print("=" * 80)
print("Sa2VA FP16æ¨ç†æµ‹è¯•")
print("=" * 80)

# é…ç½®
config_path = '/home/ubuntu/Sa2VA/projects/sa2va/configs/sa2va_vessel_finetune.py'
checkpoint_path = '/home/ubuntu/Sa2VA/work_dirs/vessel_segmentation/iter_12192.pth'

print(f"\né…ç½®æ–‡ä»¶: {config_path}")
print(f"æƒé‡æ–‡ä»¶: {checkpoint_path}")

# æ£€æŸ¥GPU
print("\næ£€æŸ¥GPU...")
if not torch.cuda.is_available():
    print("âŒ CUDAä¸å¯ç”¨")
    sys.exit(1)

print(f"âœ… æ£€æµ‹åˆ° {torch.cuda.device_count()} ä¸ªGPU")
for i in range(torch.cuda.device_count()):
    print(f"  GPU {i}: {torch.cuda.get_device_name(i)}")
    mem_total = torch.cuda.get_device_properties(i).total_memory / 1024**3
    print(f"    æ€»æ˜¾å­˜: {mem_total:.2f} GB")

# åŠ è½½é…ç½®
print("\nåŠ è½½é…ç½®...")
cfg = Config.fromfile(config_path)
print(f"âœ… é…ç½®åŠ è½½æˆåŠŸ")

# åˆ›å»ºæ¨¡å‹
print("\nåˆ›å»ºæ¨¡å‹...")
model = MODELS.build(cfg.model)
print(f"âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸ")

# åŠ è½½æƒé‡
print(f"\nåŠ è½½æƒé‡: {checkpoint_path}")
checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)

if 'state_dict' in checkpoint:
    state_dict = checkpoint['state_dict']
elif 'model' in checkpoint:
    state_dict = checkpoint['model']
else:
    state_dict = checkpoint

missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)
print(f"âœ… æƒé‡åŠ è½½æˆåŠŸ")
if missing_keys:
    print(f"  ç¼ºå¤±keys: {len(missing_keys)}ä¸ª")
if unexpected_keys:
    print(f"  å¤šä½™keys: {len(unexpected_keys)}ä¸ª")

# è½¬æ¢ä¸ºFP16
print("\nè½¬æ¢æ¨¡å‹ä¸ºFP16...")
model = model.half()
print(f"âœ… æ¨¡å‹å·²è½¬æ¢ä¸ºFP16")

# è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
model.eval()

# ç§»åŠ¨åˆ°GPU
device = 'cuda:0'
print(f"\nç§»åŠ¨æ¨¡å‹åˆ°{device}...")
print("è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ...")

try:
    model = model.to(device)
    print(f"âœ… æ¨¡å‹å·²æˆåŠŸç§»åŠ¨åˆ°GPU")
    
    # æ£€æŸ¥æ˜¾å­˜ä½¿ç”¨
    mem_allocated = torch.cuda.memory_allocated(0) / 1024**3
    mem_reserved = torch.cuda.memory_reserved(0) / 1024**3
    print(f"\næ˜¾å­˜ä½¿ç”¨:")
    print(f"  å·²åˆ†é…: {mem_allocated:.2f} GB")
    print(f"  å·²ä¿ç•™: {mem_reserved:.2f} GB")
    
    if mem_allocated < 15:
        print(f"\nğŸ‰ æˆåŠŸï¼FP16æ¨¡å‹åªä½¿ç”¨äº† {mem_allocated:.2f} GB æ˜¾å­˜")
        print(f"   ç›¸æ¯”FP32çš„23.5GBï¼ŒèŠ‚çœäº† {23.5 - mem_allocated:.2f} GB")
        print(f"\nâœ… å•GPUæ¨ç†å¯è¡Œï¼")
    else:
        print(f"\nâš ï¸  æ˜¾å­˜ä½¿ç”¨ä»ç„¶è¾ƒé«˜: {mem_allocated:.2f} GB")
        
except torch.cuda.OutOfMemoryError as e:
    print(f"âŒ ä»ç„¶OOM: {e}")
    print(f"\nå¯èƒ½éœ€è¦ä½¿ç”¨æ›´æ¿€è¿›çš„ä¼˜åŒ–æ–¹æ¡ˆ")
except Exception as e:
    print(f"âŒ é”™è¯¯: {e}")
    import traceback
    traceback.print_exc()

print("\n" + "=" * 80)
print("æµ‹è¯•å®Œæˆ")
print("=" * 80)
