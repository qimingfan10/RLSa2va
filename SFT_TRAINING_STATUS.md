# ğŸ‰ LoRA SFTè®­ç»ƒ - æˆåŠŸè¿è¡Œ

**æ—¶é—´**: 2025-11-29 22:50  
**çŠ¶æ€**: âœ… **è®­ç»ƒæ­£åœ¨è¿›è¡Œä¸­ï¼Œæ— é”™è¯¯**

---

## âœ… è®­ç»ƒçŠ¶æ€

### å½“å‰è¿›åº¦
```yaml
Epoch: 1/15 (6.7%)
æ‰¹æ¬¡: 66/976
é€Ÿåº¦: ~1.7 it/s
é¢„è®¡å•epochæ—¶é—´: ~9-10åˆ†é’Ÿ
é¢„è®¡æ€»æ—¶é—´: 2.5-3å°æ—¶ (15 epochs)
```

### æ¨¡å‹é…ç½®
```yaml
LoRAé…ç½®:
  Rank: 64
  Alpha: 128
  å¯è®­ç»ƒå‚æ•°: 41.6M (0.51%)
  æ€»å‚æ•°: 8.2B
  
æ•°æ®é›†:
  è®­ç»ƒé›†: 976å¼ 
  éªŒè¯é›†: 244å¼ 
  
ä¼˜åŒ–å™¨:
  ç±»å‹: AdamW
  å­¦ä¹ ç‡: 1e-4
  è°ƒåº¦: Cosine Annealing
  
Loss:
  ComboLoss (Dice + Focal + BCE)
```

---

## ğŸ“Š é¢„æœŸç»“æœ

### è®­ç»ƒè¿›åº¦é¢„ä¼°
```yaml
Epoch 1-3:   Losså¿«é€Ÿä¸‹é™ï¼ŒTrain Dice 0.70+
Epoch 4-7:   Train Dice 0.85+ï¼ŒVal Dice 0.78+
Epoch 8-12:  Val Diceç¨³å®šæå‡è‡³ 0.82+
Epoch 13-15: Val Diceè¾¾åˆ°æœ€ä¼˜ 0.84-0.86
```

### æœ€ç»ˆç›®æ ‡
```yaml
éªŒè¯é›†æŒ‡æ ‡:
  Dice:      0.84 - 0.86  ğŸ¯
  Recall:    0.83 - 0.85
  Precision: 0.85 - 0.87
```

---

## ğŸ“ æ–‡ä»¶ä½ç½®

```yaml
è®­ç»ƒè„šæœ¬: /home/ubuntu/Sa2VA/lora_sft_training/train_sft.py
è®­ç»ƒæ—¥å¿—: /home/ubuntu/Sa2VA/lora_sft_training/sft_training.log
è¾“å‡ºç›®å½•: /home/ubuntu/Sa2VA/lora_sft_training/output_sft/sft_20251129_224757/
æœ€ä½³æ¨¡å‹: (è®­ç»ƒå®Œæˆå) output_sft/sft_*/best_model/
```

---

## ğŸ”§ ç›‘æ§å‘½ä»¤

### æŸ¥çœ‹å®æ—¶æ—¥å¿—
```bash
tail -f /home/ubuntu/Sa2VA/lora_sft_training/sft_training.log
```

### æŸ¥çœ‹è®­ç»ƒæ‘˜è¦
```bash
bash /home/ubuntu/Sa2VA/lora_sft_training/monitor.sh
```

### æŸ¥çœ‹GPUçŠ¶æ€
```bash
watch -n 1 nvidia-smi
```

### æŸ¥çœ‹è¿›ç¨‹
```bash
ps aux | grep train_sft
```

---

## âš ï¸ æ³¨æ„äº‹é¡¹

### å¦‚æœéœ€è¦åœæ­¢è®­ç»ƒ
```bash
pkill -f train_sft.py
```

### å¦‚æœå‡ºç°OOM
```bash
# é™ä½LoRA rank
python3 train_sft.py --lora_rank 32 --epochs 15 --gpu 3
```

### ç»§ç»­è®­ç»ƒï¼ˆå¦‚æœä¸­æ–­ï¼‰
```bash
# éœ€è¦å®ç°checkpointåŠ è½½åŠŸèƒ½
# å½“å‰ç‰ˆæœ¬ä»å¤´å¼€å§‹è®­ç»ƒ
```

---

## ğŸ¯ æˆåŠŸæ ‡å¿—

è®­ç»ƒæˆåŠŸçš„æ ‡å¿—ï¼š
- âœ… æ¯ä¸ªepochæ­£å¸¸å®Œæˆ
- âœ… LossæŒç»­ä¸‹é™
- âœ… Train Diceä¸Šå‡
- âœ… Val Diceç¨³å®šæå‡
- âœ… æ— OOMé”™è¯¯
- âœ… æ¢¯åº¦æ­£å¸¸ï¼ˆé0énanï¼‰

---

## ğŸ“ˆ å·²ä¿®å¤çš„é—®é¢˜

### é—®é¢˜1: DataLoaderæ— æ³•å¤„ç†PIL Image âœ…
```python
é”™è¯¯: TypeError: default_collate: batch must contain tensors
è§£å†³: æ·»åŠ è‡ªå®šä¹‰collate_fnï¼Œbatch_size=1ï¼Œnum_workers=0
```

### é—®é¢˜2: Batchè®¿é—®æ–¹å¼ âœ…
```python
é”™è¯¯: batch['image'][0]  # é”™è¯¯çš„ç´¢å¼•
æ­£ç¡®: batch['image']     # collate_fnå·²è¿”å›å•ä¸ªæ ·æœ¬
```

---

## ğŸš€ ä¸‹ä¸€æ­¥

1. **ç­‰å¾…è®­ç»ƒå®Œæˆ** - é¢„è®¡2.5-3å°æ—¶
2. **æŸ¥çœ‹æœ€ä½³æ¨¡å‹** - Val Diceæœ€é«˜çš„epoch
3. **è¯„ä¼°æ€§èƒ½** - åœ¨éªŒè¯é›†ä¸Šå®Œæ•´è¯„ä¼°
4. **ä¸Baselineå¯¹æ¯”** - å¯¹æ¯”Diceæå‡

---

## ğŸ’¡ æŠ€æœ¯ç»†èŠ‚

### ComboLosså·¥ä½œåŸç†
```python
# Dice Loss - ç›´æ¥ä¼˜åŒ–é‡å åº¦
dice_loss = 1 - (2*intersection) / (pred_sum + gt_sum)

# Focal Loss - é™ä½æ˜“åˆ†æ ·æœ¬æƒé‡ï¼Œå…³æ³¨éš¾æ ·æœ¬
focal_loss = -Î± * (1-pt)^Î³ * log(pt)
  Î±=0.8: å…³æ³¨æ­£æ ·æœ¬ï¼ˆè¡€ç®¡ï¼‰
  Î³=2.0: éš¾æ ·æœ¬æƒé‡æå‡

# BCE Loss - åŸºç¡€åˆ†ç±»
bce_loss = -[y*log(p) + (1-y)*log(1-p)]

# ç»„åˆ
total_loss = 1.0*dice + 1.0*focal + 0.5*bce
```

### LoRAå¾®è°ƒ
```
åŸå§‹å‚æ•°: 8.2B
LoRAå‚æ•°: 41.6M (0.51%)
æ›´æ–°æ–¹å¼: W' = W + Î±BA/r

ä¼˜åŠ¿:
  - å‚æ•°é‡å°‘ï¼Œæ˜¾å­˜å ç”¨ä½
  - è®­ç»ƒå¿«é€Ÿ
  - ä¸å½±å“åŸæ¨¡å‹æƒé‡
```

---

**çŠ¶æ€**: ğŸŸ¢ è®­ç»ƒä¸­  
**é¢„è®¡å®Œæˆæ—¶é—´**: ~3å°æ—¶ï¼ˆå‡Œæ™¨1:50å·¦å³ï¼‰  
**è¾“å‡ºç›®å½•**: `./output_sft/sft_20251129_224757/`  
**ç›‘æ§è„šæœ¬**: `bash monitor.sh` ğŸ“Š
