"""
Sa2VA DPO (Direct Preference Optimization) æ¨¡åž‹

DPO Losså…¬å¼ï¼š
L_DPO = -E[log Ïƒ(Î² * (log Ï€(chosen) - log Ï€_ref(chosen) - log Ï€(rejected) + log Ï€_ref(rejected)))]

ç®€åŒ–ç‰ˆï¼ˆLoRAæ¨¡å¼ï¼Œæ— éœ€reference modelï¼‰ï¼š
L_DPO = -E[log Ïƒ(Î² * (log Ï€(chosen) - log Ï€(rejected)))]
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Optional, Tuple
from mmengine.model import BaseModel
from xtuner.registry import BUILDER


class Sa2VADPOModel(BaseModel):
    """Sa2VA DPOè®­ç»ƒæ¨¡åž‹"""
    
    def __init__(
        self,
        mllm: dict,
        grounding_encoder: dict,
        tokenizer: dict,
        special_tokens: List[str],
        pretrained_pth: Optional[str] = None,
        beta: float = 0.1,
        label_smoothing: float = 0.0,
        use_reference_model: bool = False,
        training_bs: int = 1,
        **kwargs
    ):
        super().__init__()
        
        self.beta = beta
        self.label_smoothing = label_smoothing
        self.use_reference_model = use_reference_model
        
        # æž„å»ºåŸºç¡€Sa2VAæ¨¡åž‹
        from projects.sa2va.models import Sa2VAModel
        self.model = Sa2VAModel(
            mllm=mllm,
            grounding_encoder=grounding_encoder,
            tokenizer=tokenizer,
            special_tokens=special_tokens,
            pretrained_pth=pretrained_pth,
            training_bs=training_bs,
            **kwargs
        )
        
        # å¦‚æžœéœ€è¦reference modelï¼ˆéžLoRAæ¨¡å¼ï¼‰
        if use_reference_model:
            import copy
            self.ref_model = copy.deepcopy(self.model)
            for param in self.ref_model.parameters():
                param.requires_grad = False
            self.ref_model.eval()
        else:
            self.ref_model = None
        
        print(f"\n{'='*60}")
        print("ðŸŽ¯ Sa2VA DPO Model åˆå§‹åŒ–")
        print(f"{'='*60}")
        print(f"  - beta: {beta}")
        print(f"  - label_smoothing: {label_smoothing}")
        print(f"  - use_reference_model: {use_reference_model}")
        print(f"{'='*60}\n")
    
    def compute_log_probs(
        self,
        model: nn.Module,
        images: List,
        masks: torch.Tensor,
        prompts: List[str]
    ) -> torch.Tensor:
        """
        è®¡ç®—ç»™å®šmaskçš„logæ¦‚çŽ‡
        
        å¯¹äºŽåˆ†å‰²ä»»åŠ¡ï¼Œæˆ‘ä»¬è®¡ç®—çš„æ˜¯æ¯ä¸ªåƒç´ çš„logæ¦‚çŽ‡
        ç„¶åŽå¯¹æ•´ä¸ªmaskè¿›è¡Œå¹³å‡
        """
        batch_size = len(images)
        log_probs = []
        
        for i in range(batch_size):
            image = images[i]
            mask = masks[i]  # [H, W] or [1, H, W]
            prompt = prompts[i]
            
            # èŽ·å–æ¨¡åž‹çš„åˆ†å‰²è¾“å‡ºï¼ˆlogitsï¼‰
            with torch.set_grad_enabled(model.training):
                outputs = model.forward_segmentation(
                    image=image,
                    prompt=prompt,
                    return_logits=True
                )
                
                if outputs is None or 'logits' not in outputs:
                    # å¦‚æžœæ²¡æœ‰logitsï¼Œè¿”å›ždummyå€¼
                    log_probs.append(torch.tensor(0.0, device=mask.device))
                    continue
                
                logits = outputs['logits']  # [1, 1, H, W]
                
                # ç¡®ä¿maskå’Œlogitså°ºå¯¸ä¸€è‡´
                if logits.shape[-2:] != mask.shape[-2:]:
                    mask = F.interpolate(
                        mask.unsqueeze(0).unsqueeze(0).float(),
                        size=logits.shape[-2:],
                        mode='nearest'
                    ).squeeze()
                
                # è®¡ç®—æ¯ä¸ªåƒç´ çš„logæ¦‚çŽ‡
                # ä½¿ç”¨Binary Cross Entropyçš„è´Ÿå€¼ä½œä¸ºlog probability
                # log p(y|x) = y * log(Ïƒ(logits)) + (1-y) * log(1-Ïƒ(logits))
                probs = torch.sigmoid(logits)
                mask_flat = mask.flatten().float()
                probs_flat = probs.flatten()
                
                # é¿å…log(0)
                eps = 1e-7
                probs_flat = probs_flat.clamp(eps, 1 - eps)
                
                # è®¡ç®—logæ¦‚çŽ‡
                log_p = mask_flat * torch.log(probs_flat) + (1 - mask_flat) * torch.log(1 - probs_flat)
                
                # å¯¹æ•´ä¸ªmaskå¹³å‡
                log_prob = log_p.mean()
                log_probs.append(log_prob)
        
        return torch.stack(log_probs)
    
    def compute_segmentation_log_probs(
        self,
        images: List,
        masks: torch.Tensor,
        prompts: List[str],
        use_reference: bool = False
    ) -> torch.Tensor:
        """è®¡ç®—åˆ†å‰²maskçš„logæ¦‚çŽ‡"""
        model = self.ref_model if (use_reference and self.ref_model is not None) else self.model
        
        if use_reference and self.ref_model is not None:
            with torch.no_grad():
                return self.compute_log_probs(model, images, masks, prompts)
        else:
            return self.compute_log_probs(model, images, masks, prompts)
    
    def dpo_loss(
        self,
        chosen_log_probs: torch.Tensor,
        rejected_log_probs: torch.Tensor,
        ref_chosen_log_probs: Optional[torch.Tensor] = None,
        ref_rejected_log_probs: Optional[torch.Tensor] = None
    ) -> Tuple[torch.Tensor, Dict]:
        """
        è®¡ç®—DPO Loss
        
        Args:
            chosen_log_probs: log Ï€(chosen)
            rejected_log_probs: log Ï€(rejected)
            ref_chosen_log_probs: log Ï€_ref(chosen) (å¯é€‰)
            ref_rejected_log_probs: log Ï€_ref(rejected) (å¯é€‰)
        
        Returns:
            loss: DPO loss
            metrics: è®­ç»ƒæŒ‡æ ‡
        """
        if ref_chosen_log_probs is not None and ref_rejected_log_probs is not None:
            # å®Œæ•´DPOå…¬å¼
            chosen_rewards = self.beta * (chosen_log_probs - ref_chosen_log_probs)
            rejected_rewards = self.beta * (rejected_log_probs - ref_rejected_log_probs)
        else:
            # ç®€åŒ–ç‰ˆï¼ˆæ— reference modelï¼‰
            chosen_rewards = self.beta * chosen_log_probs
            rejected_rewards = self.beta * rejected_log_probs
        
        # DPO loss: -log Ïƒ(chosen_reward - rejected_reward)
        logits = chosen_rewards - rejected_rewards
        
        if self.label_smoothing > 0:
            # Label smoothing
            loss = (
                -F.logsigmoid(logits) * (1 - self.label_smoothing)
                - F.logsigmoid(-logits) * self.label_smoothing
            )
        else:
            loss = -F.logsigmoid(logits)
        
        loss = loss.mean()
        
        # è®¡ç®—æŒ‡æ ‡
        with torch.no_grad():
            chosen_probs = torch.sigmoid(chosen_rewards)
            rejected_probs = torch.sigmoid(rejected_rewards)
            accuracy = (logits > 0).float().mean()
            margin = (chosen_rewards - rejected_rewards).mean()
        
        metrics = {
            'dpo_loss': loss.item(),
            'chosen_rewards': chosen_rewards.mean().item(),
            'rejected_rewards': rejected_rewards.mean().item(),
            'accuracy': accuracy.item(),
            'margin': margin.item(),
        }
        
        return loss, metrics
    
    def forward(self, data: Dict, mode: str = 'loss') -> Dict:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            data: åŒ…å«ä»¥ä¸‹é”®çš„å­—å…¸
                - images: å›¾åƒåˆ—è¡¨
                - chosen_masks: èƒœè€…masks [B, H, W]
                - rejected_masks: è´¥è€…masks [B, H, W]
                - prompts: promptåˆ—è¡¨
            mode: 'loss' æˆ– 'predict'
        """
        if mode == 'predict':
            return self.predict(data)
        
        images = data['images']
        chosen_masks = data['chosen_masks']
        rejected_masks = data['rejected_masks']
        prompts = data['prompts']
        
        # è®¡ç®—chosençš„logæ¦‚çŽ‡
        chosen_log_probs = self.compute_segmentation_log_probs(
            images, chosen_masks, prompts, use_reference=False
        )
        
        # è®¡ç®—rejectedçš„logæ¦‚çŽ‡
        rejected_log_probs = self.compute_segmentation_log_probs(
            images, rejected_masks, prompts, use_reference=False
        )
        
        # å¦‚æžœä½¿ç”¨reference model
        ref_chosen_log_probs = None
        ref_rejected_log_probs = None
        
        if self.use_reference_model and self.ref_model is not None:
            ref_chosen_log_probs = self.compute_segmentation_log_probs(
                images, chosen_masks, prompts, use_reference=True
            )
            ref_rejected_log_probs = self.compute_segmentation_log_probs(
                images, rejected_masks, prompts, use_reference=True
            )
        
        # è®¡ç®—DPO loss
        loss, metrics = self.dpo_loss(
            chosen_log_probs,
            rejected_log_probs,
            ref_chosen_log_probs,
            ref_rejected_log_probs
        )
        
        # è¿”å›žlosså­—å…¸
        return {
            'loss': loss,
            **{f'train/{k}': v for k, v in metrics.items()}
        }
    
    def predict(self, data: Dict) -> Dict:
        """æŽ¨ç†æ¨¡å¼"""
        return self.model.predict(data)


# ç®€åŒ–ç‰ˆDPO Losså‡½æ•°ï¼ˆå¯ç‹¬ç«‹ä½¿ç”¨ï¼‰
def dpo_loss_simple(
    policy_chosen_logps: torch.Tensor,
    policy_rejected_logps: torch.Tensor,
    reference_chosen_logps: torch.Tensor,
    reference_rejected_logps: torch.Tensor,
    beta: float = 0.1,
) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    ç®€åŒ–ç‰ˆDPO Lossè®¡ç®—
    
    Returns:
        losses: per-sample losses
        chosen_rewards: implicit rewards for chosen
        rejected_rewards: implicit rewards for rejected
    """
    pi_logratios = policy_chosen_logps - policy_rejected_logps
    ref_logratios = reference_chosen_logps - reference_rejected_logps
    
    logits = pi_logratios - ref_logratios
    
    losses = -F.logsigmoid(beta * logits)
    
    chosen_rewards = beta * (policy_chosen_logps - reference_chosen_logps).detach()
    rejected_rewards = beta * (policy_rejected_logps - reference_rejected_logps).detach()
    
    return losses, chosen_rewards, rejected_rewards
