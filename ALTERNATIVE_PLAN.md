# ğŸ¯ å¯å°è¯•çš„æœ€åæ–¹æ¡ˆï¼šæœ€å°åŒ–è®­ç»ƒ

## é…ç½®ä¿®æ”¹

### 1. å†»ç»“SAM2 decoder
```python
frozen_sam2_decoder=True  # åªè®­ç»ƒLoRA
```

### 2. å‡å°‘LoRA rank
```python
llm_lora=dict(
    r=32,  # ä»64é™åˆ°32
    lora_alpha=64,
)
```

### 3. å‡å°‘batchå’Œsequence
```python
batch_size = 1
accumulative_counts = 1
max_length = 2048  # ä»8192é™åˆ°2048
```

### 4. å¯ç”¨gradient checkpointing
éœ€è¦ä¿®æ”¹InternVLæ¨¡å‹ä»£ç 

## æ‰§è¡Œå‘½ä»¤

```bash
# 1. ä¿®æ”¹é…ç½®
vim /home/ubuntu/Sa2VA/projects/sa2va/configs/sa2va_vessel_lora_finetune.py

# 2. è¿è¡Œ
cd /home/ubuntu/Sa2VA
CUDA_VISIBLE_DEVICES=0,1,2,3 \
PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True \
DEEPSPEED=deepspeed_zero2_offload \
bash tools/dist.sh train \
  projects/sa2va/configs/sa2va_vessel_lora_finetune.py 4
```

## é¢„æœŸ

- æˆåŠŸç‡: 50-70%
- æ˜¾å­˜å ç”¨: 18-20GB per GPU
- è®­ç»ƒæ—¶é—´: 10-15å°æ—¶
- æå‡å¹…åº¦: Dice +0.01-0.03 (å¾ˆå°)

## æ˜¯å¦å€¼å¾—

**å¦**, å› ä¸ºï¼š
- æŠ•å…¥: 10-15å°æ—¶
- æå‡: æœ€å¤š+3%
- é£é™©: 50%å¤±è´¥

å¯¹æ¯”é˜ˆå€¼ä¼˜åŒ–ï¼š
- æŠ•å…¥: 0ç§’
- æå‡: +7%
- é£é™©: 0%

**ç»“è®º**: é™¤éæœ‰ç‰¹æ®Šéœ€æ±‚ï¼ˆå¦‚å‘è®ºæ–‡å¿…é¡»æœ‰trainingç»“æœï¼‰ï¼Œå¦åˆ™ä¸å»ºè®®ç»§ç»­ã€‚
