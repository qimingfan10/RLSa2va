"""
Sa2VAè§†é¢‘é¢„æµ‹ - ä½¿ç”¨å®˜æ–¹predict_forwardæ–¹æ³•
å¯¹è§†é¢‘åºåˆ—è¿›è¡Œé¢„æµ‹å¹¶ç”Ÿæˆå¯¹æ¯”MP4è§†é¢‘
"""
import os
import sys
import json
import numpy as np
import torch
from PIL import Image
import cv2
from collections import defaultdict
from sklearn.metrics import jaccard_score, f1_score
from transformers import AutoModelForCausalLM, AutoTokenizer
from tqdm import tqdm

sys.path.insert(0, '/home/ubuntu/Sa2VA')

print("=" * 80)
print("Sa2VAè§†é¢‘é¢„æµ‹ - ä½¿ç”¨å®˜æ–¹predict_forwardæ–¹æ³•")
print("=" * 80)

# é…ç½®
HF_MODEL_PATH = "/home/ubuntu/Sa2VA/models/sa2va_vessel_hf"
DATA_ROOT = "/home/ubuntu/Sa2VA/data/merged_vessel_data/"
OUTPUT_BASE_DIR = "/home/ubuntu/Sa2VA/video_prediction_results"
NUM_VIDEOS = 5  # é¢„æµ‹å‰5ä¸ªè§†é¢‘åºåˆ—
START_VIDEO_INDEX = 0  # ä»ç¬¬å‡ ä¸ªè§†é¢‘å¼€å§‹ï¼ˆ0-basedï¼‰

print(f"HFæ¨¡å‹è·¯å¾„: {HF_MODEL_PATH}")
print(f"æ•°æ®è·¯å¾„: {DATA_ROOT}")
print(f"é¢„æµ‹è§†é¢‘æ•°é‡: {NUM_VIDEOS}")
print(f"èµ·å§‹è§†é¢‘ç´¢å¼•: {START_VIDEO_INDEX}")
print()

os.makedirs(OUTPUT_BASE_DIR, exist_ok=True)

# è¯„ä»·æŒ‡æ ‡è®¡ç®—
def calculate_metrics(pred_mask, gt_mask):
    """è®¡ç®—åˆ†å‰²è¯„ä»·æŒ‡æ ‡"""
    pred_flat = (pred_mask > 127).flatten().astype(int)
    gt_flat = (gt_mask > 127).flatten().astype(int)
    
    if len(np.unique(gt_flat)) == 1 and len(np.unique(pred_flat)) == 1:
        if gt_flat[0] == pred_flat[0]:
            return {'IoU': 1.0, 'Dice': 1.0}
        else:
            return {'IoU': 0.0, 'Dice': 0.0}
    
    iou = jaccard_score(gt_flat, pred_flat, zero_division=0)
    dice = f1_score(gt_flat, pred_flat, zero_division=0)
    
    return {'IoU': float(iou), 'Dice': float(dice)}

# æ­¥éª¤1: åˆ†ææ•°æ®é›†ï¼Œæ‰¾å‡ºè§†é¢‘åºåˆ—
print("=" * 80)
print("æ­¥éª¤1: åˆ†ææ•°æ®é›†")
print("=" * 80)

with open(os.path.join(DATA_ROOT, "annotations.json")) as f:
    dataset = json.load(f)

print(f"æ•°æ®é›†æ€»æ•°: {len(dataset)}")

# æŒ‰å‰ç¼€åˆ†ç»„
video_groups = defaultdict(list)
for item in dataset:
    img_name = item['image']
    if '_frame_' in img_name:
        prefix = img_name.split('_frame_')[0]
        frame_num = img_name.split('_frame_')[1].split('.')[0]
        video_groups[prefix].append((frame_num, item))
    else:
        video_groups[img_name].append(('000000', item))

# ç­›é€‰è§†é¢‘åºåˆ—ï¼ˆè‡³å°‘3å¸§ï¼‰
video_sequences = []
for prefix, frames in sorted(video_groups.items()):
    if len(frames) >= 3:
        sorted_frames = sorted(frames, key=lambda x: x[0])
        video_sequences.append((prefix, sorted_frames))

print(f"\næ‰¾åˆ° {len(video_sequences)} ä¸ªè§†é¢‘åºåˆ—ï¼ˆ>=3å¸§ï¼‰")

# æ˜¾ç¤ºæ‰€æœ‰è§†é¢‘
print("\nå¯ç”¨çš„è§†é¢‘åºåˆ—:")
for i, (prefix, frames) in enumerate(video_sequences[:20]):
    print(f"  {i}. {prefix}: {len(frames)}å¸§ (å¸§{frames[0][0]}-{frames[-1][0]})")
if len(video_sequences) > 20:
    print(f"  ... è¿˜æœ‰ {len(video_sequences) - 20} ä¸ªè§†é¢‘")

# é€‰æ‹©è¦é¢„æµ‹çš„è§†é¢‘èŒƒå›´
end_index = min(START_VIDEO_INDEX + NUM_VIDEOS, len(video_sequences))
selected_videos = video_sequences[START_VIDEO_INDEX:end_index]

print(f"\nâœ… å°†é¢„æµ‹ {len(selected_videos)} ä¸ªè§†é¢‘ (ç´¢å¼• {START_VIDEO_INDEX} åˆ° {end_index-1})")
for i, (prefix, frames) in enumerate(selected_videos):
    print(f"  #{START_VIDEO_INDEX + i}. {prefix}: {len(frames)}å¸§")
print()

# æ­¥éª¤2: åŠ è½½HuggingFaceæ¨¡å‹
print("=" * 80)
print("æ­¥éª¤2: åŠ è½½HuggingFaceæ¨¡å‹")
print("=" * 80)

try:
    print("åŠ è½½tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(
        HF_MODEL_PATH,
        trust_remote_code=True
    )
    print("âœ… TokenizeråŠ è½½æˆåŠŸ")
    
    print("\nåŠ è½½æ¨¡å‹...")
    model = AutoModelForCausalLM.from_pretrained(
        HF_MODEL_PATH,
        torch_dtype="auto",
        device_map="auto",
        trust_remote_code=True,
        low_cpu_mem_usage=True
    )
    print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    
    model.eval()
    
except Exception as e:
    print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    exit(1)

# æ­¥éª¤3: å¯¹è§†é¢‘åºåˆ—è¿›è¡Œæ¨ç†
print("\n" + "=" * 80)
print("æ­¥éª¤3: è§†é¢‘åºåˆ—æ¨ç†")
print("=" * 80)

all_metrics = []
frame_results = []
successful_frames = 0

for idx, (frame_num, sample) in enumerate(tqdm(selected_frames, desc="æ¨ç†è¿›åº¦")):
    img_path = os.path.join(DATA_ROOT, "images", sample['image'])
    
    if not os.path.exists(img_path):
        print(f"âš ï¸  è·³è¿‡ä¸å­˜åœ¨çš„å›¾ç‰‡: {sample['image']}")
        continue
    
    # åŠ è½½å›¾ç‰‡
    image = Image.open(img_path).convert('RGB')
    image_np = np.array(image)
    h, w = image_np.shape[:2]
    
    # åˆ›å»ºGround Truth mask
    gt_mask = np.zeros((h, w), dtype=np.uint8)
    for mask_coords in sample['mask']:
        if len(mask_coords) >= 6:
            points = np.array(mask_coords).reshape(-1, 2).astype(np.int32)
            cv2.fillPoly(gt_mask, [points], 255)
    
    # ä½¿ç”¨predict_forwardè¿›è¡Œæ¨ç†
    try:
        text = "<image>Please segment the blood vessel."
        
        result = model.predict_forward(
            image=image,
            text=text,
            tokenizer=tokenizer,
            processor=None,
        )
        
        prediction_text = result.get('prediction', '')
        
        # æå–é¢„æµ‹mask
        if '[SEG]' in prediction_text and 'prediction_masks' in result:
            pred_masks = result['prediction_masks']
            
            if len(pred_masks) > 0:
                pred_mask = pred_masks[0][0]
                
                if isinstance(pred_mask, torch.Tensor):
                    pred_mask = pred_mask.cpu().numpy()
                
                if pred_mask.shape != (h, w):
                    pred_mask = cv2.resize(pred_mask, (w, h), interpolation=cv2.INTER_NEAREST)
                
                if pred_mask.max() <= 1.0:
                    pred_mask = (pred_mask > 0.5).astype(np.uint8) * 255
                else:
                    pred_mask = (pred_mask > 127).astype(np.uint8) * 255
                
                successful_frames += 1
            else:
                pred_mask = np.zeros((h, w), dtype=np.uint8)
        else:
            pred_mask = np.zeros((h, w), dtype=np.uint8)
    
    except Exception as e:
        print(f"\nâš ï¸  å¸§ {frame_num} æ¨ç†å¤±è´¥: {e}")
        pred_mask = np.zeros((h, w), dtype=np.uint8)
    
    # è®¡ç®—æŒ‡æ ‡
    metrics = calculate_metrics(pred_mask, gt_mask)
    all_metrics.append(metrics)
    
    # ä¿å­˜ç»“æœ
    frame_results.append({
        'frame_num': frame_num,
        'image': sample['image'],
        'image_np': image_np,
        'gt_mask': gt_mask,
        'pred_mask': pred_mask,
        'metrics': metrics
    })

print(f"\nâœ… æ¨ç†å®Œæˆ!")
print(f"   æˆåŠŸ: {successful_frames}/{len(selected_frames)}")
print(f"   æˆåŠŸç‡: {successful_frames/len(selected_frames)*100:.1f}%")

# è®¡ç®—å¹³å‡æŒ‡æ ‡
avg_metrics = {
    key: np.mean([m[key] for m in all_metrics])
    for key in all_metrics[0].keys()
}

print(f"\nè§†é¢‘å¹³å‡æŒ‡æ ‡:")
print(f"   IoU (Jaccard): {avg_metrics['IoU']:.4f}")
print(f"   Dice Score:    {avg_metrics['Dice']:.4f}")

# æ­¥éª¤4: ç”Ÿæˆå¯¹æ¯”è§†é¢‘
print("\n" + "=" * 80)
print("æ­¥éª¤4: ç”Ÿæˆå¯¹æ¯”MP4è§†é¢‘")
print("=" * 80)

# ç¡®å®šè§†é¢‘å‚æ•°
h, w = frame_results[0]['image_np'].shape[:2]
fps = 10  # 10 FPS

# åˆ›å»ºä¸‰ç§è§†é¢‘ï¼šåŸå›¾+GTã€åŸå›¾+é¢„æµ‹ã€GT vs é¢„æµ‹å¯¹æ¯”
video_configs = [
    {
        'name': 'original_with_gt.mp4',
        'description': 'åŸå›¾ + Ground Truthå åŠ ',
        'function': lambda frame: create_overlay(frame['image_np'], frame['gt_mask'], (0, 0, 255))
    },
    {
        'name': 'original_with_pred.mp4',
        'description': 'åŸå›¾ + Sa2VAé¢„æµ‹å åŠ ',
        'function': lambda frame: create_overlay(frame['image_np'], frame['pred_mask'], (0, 255, 0))
    },
    {
        'name': 'comparison.mp4',
        'description': 'GT vs é¢„æµ‹å¯¹æ¯”',
        'function': lambda frame: create_comparison(frame)
    }
]

def create_overlay(image, mask, color):
    """åˆ›å»ºmaskå åŠ å›¾"""
    result = image.copy()
    mask_colored = np.zeros_like(result)
    mask_colored[mask > 127] = color
    result = cv2.addWeighted(result, 0.7, mask_colored, 0.3, 0)
    return result

def create_comparison(frame):
    """åˆ›å»ºGT vs é¢„æµ‹å¯¹æ¯”å›¾"""
    image = frame['image_np']
    gt_mask = frame['gt_mask']
    pred_mask = frame['pred_mask']
    metrics = frame['metrics']
    
    # åˆ›å»º2x2ç½‘æ ¼
    # å·¦ä¸Š: åŸå›¾, å³ä¸Š: GT
    # å·¦ä¸‹: é¢„æµ‹, å³ä¸‹: GT vs é¢„æµ‹å åŠ 
    
    h, w = image.shape[:2]
    
    # åŸå›¾
    img1 = image.copy()
    cv2.putText(img1, 'Original', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # GT mask (å½©è‰²)
    img2 = cv2.cvtColor(gt_mask, cv2.COLOR_GRAY2BGR)
    cv2.putText(img2, 'Ground Truth', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # é¢„æµ‹mask (å½©è‰²)
    img3 = cv2.cvtColor(pred_mask, cv2.COLOR_GRAY2BGR)
    cv2.putText(img3, 'Sa2VA Prediction', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    
    # GT vs é¢„æµ‹å åŠ 
    img4 = image.copy()
    # GTç”¨çº¢è‰²
    gt_overlay = np.zeros_like(img4)
    gt_overlay[gt_mask > 127] = [0, 0, 255]
    # é¢„æµ‹ç”¨ç»¿è‰²
    pred_overlay = np.zeros_like(img4)
    pred_overlay[pred_mask > 127] = [0, 255, 0]
    
    img4 = cv2.addWeighted(img4, 0.5, gt_overlay, 0.3, 0)
    img4 = cv2.addWeighted(img4, 1.0, pred_overlay, 0.3, 0)
    
    # æ·»åŠ æŒ‡æ ‡æ–‡æœ¬
    metric_text = f"IoU:{metrics['IoU']:.3f} Dice:{metrics['Dice']:.3f}"
    cv2.putText(img4, 'GT(Red) vs Pred(Green)', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
    cv2.putText(img4, metric_text, (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2)
    
    # æ‹¼æ¥æˆ2x2ç½‘æ ¼
    top_row = np.hstack([img1, img2])
    bottom_row = np.hstack([img3, img4])
    result = np.vstack([top_row, bottom_row])
    
    return result

# ç”Ÿæˆè§†é¢‘
for video_config in video_configs:
    video_name = video_config['name']
    video_path = os.path.join(OUTPUT_DIR, video_name)
    description = video_config['description']
    func = video_config['function']
    
    print(f"\nç”Ÿæˆè§†é¢‘: {description}")
    print(f"   æ–‡ä»¶: {video_name}")
    
    # è·å–ç¬¬ä¸€å¸§çš„å°ºå¯¸
    first_frame = func(frame_results[0])
    video_h, video_w = first_frame.shape[:2]
    
    # åˆ›å»ºè§†é¢‘å†™å…¥å™¨
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    video_writer = cv2.VideoWriter(video_path, fourcc, fps, (video_w, video_h))
    
    # å†™å…¥æ‰€æœ‰å¸§
    for frame in tqdm(frame_results, desc=f"  ç”Ÿæˆ{video_name}"):
        frame_img = func(frame)
        video_writer.write(frame_img)
    
    video_writer.release()
    
    # æ£€æŸ¥æ–‡ä»¶å¤§å°
    file_size = os.path.getsize(video_path) / (1024 * 1024)
    print(f"   âœ… å®Œæˆ! å¤§å°: {file_size:.2f}MB")

# æ­¥éª¤5: ç”ŸæˆæŠ¥å‘Š
print("\n" + "=" * 80)
print("æ­¥éª¤5: ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š")
print("=" * 80)

report_path = os.path.join(OUTPUT_DIR, "video_evaluation_report.md")
with open(report_path, 'w', encoding='utf-8') as f:
    f.write(f"# Sa2VAè§†é¢‘é¢„æµ‹è¯„ä¼°æŠ¥å‘Š\n\n")
    f.write(f"## è§†é¢‘ä¿¡æ¯\n\n")
    f.write(f"- **è§†é¢‘ID**: {selected_prefix}\n")
    f.write(f"- **æ€»å¸§æ•°**: {len(selected_frames)}\n")
    f.write(f"- **å¸§èŒƒå›´**: {selected_frames[0][0]} - {selected_frames[-1][0]}\n")
    f.write(f"- **åˆ†è¾¨ç‡**: {w}x{h}\n")
    f.write(f"- **å¸§ç‡**: {fps} FPS\n\n")
    
    f.write(f"## æ¨ç†ç»Ÿè®¡\n\n")
    f.write(f"- **æˆåŠŸå¸§æ•°**: {successful_frames}/{len(selected_frames)}\n")
    f.write(f"- **æˆåŠŸç‡**: {successful_frames/len(selected_frames)*100:.1f}%\n\n")
    
    f.write(f"## å¹³å‡æ€§èƒ½æŒ‡æ ‡\n\n")
    f.write(f"| æŒ‡æ ‡ | æ•°å€¼ |\n")
    f.write(f"|------|------|\n")
    f.write(f"| IoU (Jaccard) | {avg_metrics['IoU']:.4f} |\n")
    f.write(f"| Dice Score | {avg_metrics['Dice']:.4f} |\n\n")
    
    f.write(f"## è¾“å‡ºè§†é¢‘\n\n")
    f.write(f"1. **original_with_gt.mp4** - åŸå›¾ + Ground Truthå åŠ ï¼ˆçº¢è‰²ï¼‰\n")
    f.write(f"2. **original_with_pred.mp4** - åŸå›¾ + Sa2VAé¢„æµ‹å åŠ ï¼ˆç»¿è‰²ï¼‰\n")
    f.write(f"3. **comparison.mp4** - å››å®«æ ¼å¯¹æ¯”è§†é¢‘\n")
    f.write(f"   - å·¦ä¸Š: åŸå›¾\n")
    f.write(f"   - å³ä¸Š: Ground Truth\n")
    f.write(f"   - å·¦ä¸‹: Sa2VAé¢„æµ‹\n")
    f.write(f"   - å³ä¸‹: GT(çº¢) vs é¢„æµ‹(ç»¿) å åŠ \n\n")
    
    f.write(f"## é€å¸§æŒ‡æ ‡\n\n")
    f.write(f"| å¸§å· | æ–‡ä»¶å | IoU | Dice |\n")
    f.write(f"|------|--------|-----|------|\n")
    for frame in frame_results:
        f.write(f"| {frame['frame_num']} | {frame['image']} | {frame['metrics']['IoU']:.4f} | {frame['metrics']['Dice']:.4f} |\n")

print(f"âœ… æŠ¥å‘Šå·²ä¿å­˜: {report_path}")

# ä¿å­˜JSONç»“æœ
json_path = os.path.join(OUTPUT_DIR, "video_evaluation_results.json")
results_data = {
    'video_id': selected_prefix,
    'total_frames': len(selected_frames),
    'successful_frames': successful_frames,
    'success_rate': successful_frames / len(selected_frames),
    'average_metrics': avg_metrics,
    'frame_metrics': [
        {
            'frame_num': f['frame_num'],
            'image': f['image'],
            'metrics': f['metrics']
        }
        for f in frame_results
    ],
    'output_videos': [
        'original_with_gt.mp4',
        'original_with_pred.mp4',
        'comparison.mp4'
    ]
}

with open(json_path, 'w', encoding='utf-8') as f:
    json.dump(results_data, f, indent=2, ensure_ascii=False)

print(f"âœ… JSONç»“æœå·²ä¿å­˜: {json_path}")

# æ€»ç»“
print("\n" + "=" * 80)
print("ğŸ‰ è§†é¢‘é¢„æµ‹å®Œæˆï¼")
print("=" * 80)
print(f"\nè§†é¢‘ä¿¡æ¯:")
print(f"  è§†é¢‘ID: {selected_prefix}")
print(f"  æ€»å¸§æ•°: {len(selected_frames)}")
print(f"  æˆåŠŸç‡: {successful_frames/len(selected_frames)*100:.1f}%")
print(f"\nå¹³å‡æ€§èƒ½:")
print(f"  IoU:  {avg_metrics['IoU']:.4f}")
print(f"  Dice: {avg_metrics['Dice']:.4f}")
print(f"\nè¾“å‡ºæ–‡ä»¶:")
print(f"  ğŸ“ {OUTPUT_DIR}/")
print(f"     ğŸ¬ original_with_gt.mp4 - åŸå›¾+GT")
print(f"     ğŸ¬ original_with_pred.mp4 - åŸå›¾+é¢„æµ‹")
print(f"     ğŸ¬ comparison.mp4 - å››å®«æ ¼å¯¹æ¯”")
print(f"     ğŸ“„ video_evaluation_report.md")
print(f"     ğŸ“„ video_evaluation_results.json")
print()
print("=" * 80)
