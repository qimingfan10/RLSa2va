# ğŸ“ Sa2VAå®˜æ–¹LoRAå¾®è°ƒæ–¹æ³•åˆ†æ

**å‘ç°æ—¶é—´**: 2025-11-30 13:15  
**çŠ¶æ€**: âœ… **æ‰¾åˆ°æ­£ç¡®çš„è®­ç»ƒæ–¹æ³•ï¼**

---

## ğŸ” å…³é”®å‘ç°

### 1. è®­ç»ƒè„šæœ¬ç»“æ„

```bash
# å®˜æ–¹è®­ç»ƒå‘½ä»¤
bash tools/dist.sh train projects/sa2va/configs/sa2va_finetune.py 8
```

**å·¥ä½œåŸç†**ï¼š
- ä½¿ç”¨`tools/train.py` (å®é™…è°ƒç”¨`xtuner.tools.train`)
- é…ç½®æ–‡ä»¶ï¼š`projects/sa2va/configs/sa2va_finetune.py`
- æ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒï¼ˆ8 GPUï¼‰

---

## ğŸ—ï¸ æ¨¡å‹æ¶æ„ï¼ˆSa2VAModelï¼‰

### å…³é”®ç»„ä»¶

```python
class Sa2VAModel(BaseModel):
    def __init__(self):
        # 1. MLLM (å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹)
        self.mllm = InternVLMLLM(
            freeze_llm=True,           # â„ï¸ å†»ç»“LLM
            freeze_visual_encoder=True, # â„ï¸ å†»ç»“è§†è§‰ç¼–ç å™¨
            llm_lora=LoraConfig(        # âœ… åªè®­ç»ƒLoRA
                r=128,
                lora_alpha=256,
                lora_dropout=0.05,
                task_type='CAUSAL_LM',
                modules_to_save=["embed_tokens", "lm_head"]
            )
        )
        
        # 2. SAM2 Grounding Encoder
        self.grounding_encoder = SAM2TrainRunner()
        self.grounding_encoder.requires_grad_(False)  # â„ï¸ é»˜è®¤å†»ç»“
        
        # 3. SAM2 Mask Decoderï¼ˆå¯é€‰è®­ç»ƒï¼‰
        if not frozen_sam2_decoder:
            self.grounding_encoder.sam2_model.sam_mask_decoder.requires_grad_(True)
        
        # 4. æ–‡æœ¬åˆ°è§†è§‰æ˜ å°„
        self.text_hidden_fcs = nn.Sequential(
            nn.Linear(in_dim, in_dim), nn.ReLU(),
            nn.Linear(in_dim, out_dim), nn.Dropout(0.0)
        )  # âœ… å¯è®­ç»ƒ
        
        # 5. Losså‡½æ•°
        self.loss_mask = CrossEntropyLoss(loss_weight=2.0)
        self.loss_dice = DiceLoss(loss_weight=0.5)
```

---

## ğŸ¯ è®­ç»ƒé…ç½®ï¼ˆsa2va_finetune.pyï¼‰

### LoRAå‚æ•°

```python
llm_lora=dict(
    type=LoraConfig,
    r=128,              # LoRA rank
    lora_alpha=256,     # LoRA alpha
    lora_dropout=0.05,
    bias='none',
    task_type='CAUSAL_LM',
    modules_to_save=["embed_tokens", "lm_head"]  # é¢å¤–è®­ç»ƒçš„æ¨¡å—
)
```

### è®­ç»ƒè¶…å‚æ•°

```python
batch_size = 2              # per device
accumulative_counts = 16    # 8 GPUs Ã— 2 = å®é™…batch=32
max_epochs = 1
lr = 4e-5
weight_decay = 0.05
warmup_ratio = 0.05
max_length = 8192
```

### Lossé…ç½®

```python
loss_mask = dict(
    type=CrossEntropyLoss,
    use_sigmoid=True,
    reduction='mean',
    loss_weight=2.0
)

loss_dice = dict(
    type=DiceLoss,
    use_sigmoid=True,
    activate=True,
    reduction='mean',
    naive_dice=True,
    eps=1.0,
    loss_weight=0.5
)
```

---

## ğŸ“Š æ•°æ®æ ¼å¼ï¼ˆannotations.jsonï¼‰

```json
[
    {
        "image": "image001.jpg",
        "text": ["blood vessel", "artery"],
        "mask": [
            [[x1,y1,x2,y2,...], [...]],  // polygon for object 1
            [[x1,y1,x2,y2,...]]           // polygon for object 2
        ]
    }
]
```

**å¤„ç†æµç¨‹**ï¼š
1. è¯»å–imageå’Œmask
2. å°†polygonè½¬æ¢ä¸ºbinary mask
3. åˆ›å»ºå¯¹è¯æ ¼å¼ï¼š
   ```python
   "<image>\nPlease segment the blood vessel. [SEG]"
   "Sure, [SEG]."
   ```
4. Tokenizeå¹¶ç¼–ç 

---

## ğŸ”„ è®­ç»ƒæµç¨‹ï¼ˆforwardå‡½æ•°ï¼‰

### å…³é”®ä»£ç ç‰‡æ®µ

```python
def forward(self, data_samples):
    # 1. å‰å‘ä¼ æ’­MLLM
    llm_output = self.mllm(
        input_ids=data_samples['input_ids'],
        pixel_values=data_samples['pixel_values'],
        labels=data_samples['labels'],  # ç”¨äºè®¡ç®—language loss
    )
    
    # 2. æå–[SEG] tokençš„hidden states
    seg_hidden_states = extract_seg_hidden_states(
        llm_output.hidden_states,
        output_ids,
        seg_token_idx
    )
    
    # 3. é€šè¿‡text_hidden_fcsæ˜ å°„
    seg_embeddings = self.text_hidden_fcs(seg_hidden_states)
    
    # 4. SAM2ç¼–ç å™¨ç”Ÿæˆç‰¹å¾
    sam_states = self.grounding_encoder.get_sam2_embeddings(
        data_samples['extra_pixel_values']
    )
    
    # 5. æ³¨å…¥language embeddingå¹¶ç”Ÿæˆmask
    pred_masks = self.grounding_encoder.inject_language_embd(
        sam_states, seg_embeddings
    )
    
    # 6. è®¡ç®—mask loss
    loss_mask = self.loss_mask(pred_masks, gt_masks)
    loss_dice = self.loss_dice(pred_masks, gt_masks)
    
    # 7. æ€»loss = language_loss + mask_loss + dice_loss
    total_loss = llm_output.loss + loss_mask + loss_dice
    
    return {'loss': total_loss}
```

---

## âš¡ å…³é”®åŒºåˆ«ï¼šè®­ç»ƒ vs æ¨ç†

| æ–¹é¢ | è®­ç»ƒï¼ˆforwardï¼‰ | æ¨ç†ï¼ˆpredict_forwardï¼‰ |
|------|----------------|----------------------|
| å‡½æ•° | `forward()` | `predict_forward()` |
| æ¨¡å¼ | `model.train()` | `model.eval()` |
| æ¢¯åº¦ | âœ… æœ‰ | âŒ æ— ï¼ˆ@torch.no_gradï¼‰ |
| è¾“å…¥ | å®Œæ•´è®­ç»ƒæ•°æ® | å•å¼ å›¾åƒ+æ–‡æœ¬ |
| è¾“å‡º | Loss | åˆ†å‰²mask |
| Tokenç”Ÿæˆ | ä½¿ç”¨labelsï¼ˆteacher forcingï¼‰ | ä½¿ç”¨generate()é‡‡æ · |
| SAM2 | ç›´æ¥è®¡ç®—loss | æ¨ç†ç”Ÿæˆmask |

---

## ğŸ†š ä¸ºä»€ä¹ˆæˆ‘ä»¬çš„è®­ç»ƒå¤±è´¥äº†

### æˆ‘ä»¬çš„æ–¹æ³• âŒ

```python
# ä½¿ç”¨predict_forwardï¼ˆæ¨ç†å‡½æ•°ï¼‰
result = model.predict_forward(image, text, tokenizer, return_tensors=True)
pred = result['probability_maps'][0][0]
loss = criterion(pred, gt_mask)
loss.backward()  # âŒ æ¢¯åº¦æ— æ³•å›ä¼ 
```

**é—®é¢˜**ï¼š
1. `predict_forward`æ˜¯æ¨ç†æµç¨‹
2. å†…éƒ¨ä½¿ç”¨`generate()`ï¼ˆå³ä½¿ç§»é™¤@no_gradï¼Œä¹Ÿæ˜¯ç¦»æ•£é‡‡æ ·ï¼‰
3. SAM2éƒ¨åˆ†æ²¡æœ‰æ¢¯åº¦
4. æ— æ³•çœŸæ­£ä¼˜åŒ–å‚æ•°

### å®˜æ–¹æ–¹æ³• âœ…

```python
# ä½¿ç”¨forwardï¼ˆè®­ç»ƒå‡½æ•°ï¼‰
data = {
    'input_ids': ...,
    'pixel_values': ...,
    'extra_pixel_values': ...,
    'labels': ...,
    'masks': ...
}
outputs = model.forward(data)
loss = outputs['loss']  # å†…éƒ¨å·²è®¡ç®—å¥½
loss.backward()  # âœ… æ¢¯åº¦æ­£ç¡®å›ä¼ 
optimizer.step()
```

**ä¼˜åŠ¿**ï¼š
1. ä½¿ç”¨teacher forcingï¼ˆlabelsç›´æ¥æŒ‡å¯¼ï¼‰
2. SAM2 decoderå¯ä»¥è®­ç»ƒ
3. text_hidden_fcså¯ä»¥è®­ç»ƒ
4. å®Œæ•´çš„æ¢¯åº¦è·¯å¾„

---

## ğŸ“ å¯è®­ç»ƒå‚æ•°

```python
âœ… LoRAå‚æ•°ï¼ˆLLMï¼‰         ~41M
âœ… embed_tokensï¼ˆtokenåµŒå…¥ï¼‰
âœ… lm_headï¼ˆè¾“å‡ºå±‚ï¼‰
âœ… text_hidden_fcs          ~2M
âœ… SAM2 mask decoder       ~4Mï¼ˆå¯é€‰ï¼‰

â„ï¸ LLM backboneï¼ˆå†»ç»“ï¼‰
â„ï¸ Vision encoderï¼ˆå†»ç»“ï¼‰
â„ï¸ SAM2 encoderï¼ˆå†»ç»“ï¼‰

æ€»å¯è®­ç»ƒï¼š~45-50Må‚æ•°ï¼ˆçº¦å æ€»å‚æ•°çš„1-2%ï¼‰
```

---

## ğŸ¨ æ•°æ®å‡†å¤‡ç¤ºä¾‹

### è½¬æ¢æˆ‘ä»¬çš„æ•°æ®æ ¼å¼

```python
import json
import glob
import numpy as np
from PIL import Image

annotations = []
for img_path in glob.glob('Segment_DATA_Merged_512/images/*.jpg'):
    img_name = os.path.basename(img_path)
    mask_path = img_path.replace('images', 'masks').replace('.jpg', '_mask.png')
    
    # è¯»å–mask
    mask = np.array(Image.open(mask_path).convert('L'))
    
    # è½¬æ¢ä¸ºpolygonï¼ˆç®€åŒ–ç‰ˆï¼‰
    contours = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[0]
    polygon = contours[0].flatten().tolist()
    
    annotations.append({
        'image': img_name,
        'text': ['blood vessel'],
        'mask': [[polygon]]
    })

with open('annotations.json', 'w') as f:
    json.dump(annotations, f)
```

---

## ğŸš€ ä½¿ç”¨å®˜æ–¹æ–¹æ³•è®­ç»ƒçš„æ­¥éª¤

### 1. å‡†å¤‡æ•°æ®

```bash
data/my_data/
  â”œâ”€â”€ images/
  â”‚   â”œâ”€â”€ image001.jpg
  â”‚   â””â”€â”€ ...
  â””â”€â”€ annotations.json
```

### 2. ä¿®æ”¹é…ç½®

ç¼–è¾‘`projects/sa2va/configs/sa2va_finetune.py`ï¼š
```python
path = "/home/ubuntu/Sa2VA/models/sa2va_vessel_hf"  # ä½ çš„æ¨¡å‹è·¯å¾„
pretrained_pth = None  # æˆ–æŒ‡å‘é¢„è®­ç»ƒæƒé‡
RES_ROOT = '/home/ubuntu/Sa2VA/Segment_DATA_Merged_512/'
```

### 3. è¿è¡Œè®­ç»ƒ

```bash
cd /home/ubuntu/Sa2VA
bash tools/dist.sh train projects/sa2va/configs/sa2va_finetune.py 1  # å•GPU
# æˆ–
bash tools/dist.sh train projects/sa2va/configs/sa2va_finetune.py 4  # 4 GPU
```

---

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **å†…å­˜è¦æ±‚**ï¼š
   - å•GPU batch_size=2 éœ€è¦~24GBæ˜¾å­˜
   - å»ºè®®ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯ï¼ˆaccumulative_countsï¼‰

2. **æ•°æ®æ ¼å¼**ï¼š
   - annotations.jsonå¿…é¡»ä¸¥æ ¼æŒ‰ç…§æ ¼å¼
   - polygonåæ ‡å¿…é¡»æ˜¯validçš„

3. **é¢„è®­ç»ƒæƒé‡**ï¼š
   - æœ€å¥½ä»å®˜æ–¹é¢„è®­ç»ƒæ¨¡å‹å¼€å§‹
   - æˆ–è®¾ç½®`pretrained_pth=None`ä»å¤´è®­ç»ƒ

4. **è®­ç»ƒæ—¶é—´**ï¼š
   - 1000æ ·æœ¬ Ã— 100 repeats = 100k steps
   - å•GPUçº¦éœ€2-3å¤©

---

## ğŸ¯ æ€»ç»“

| æ–¹é¢ | æˆ‘ä»¬çš„æ–¹æ³• | å®˜æ–¹æ–¹æ³• |
|------|----------|---------|
| å‡½æ•° | predict_forward | forward |
| é…ç½® | æ‰‹åŠ¨è„šæœ¬ | mmengineé…ç½® |
| æ•°æ® | ç®€å•åŠ è½½ | å®Œæ•´pipeline |
| Loss | æ‰‹åŠ¨è®¡ç®— | å†…ç½®è®¡ç®— |
| LoRA | PEFTåº“ | XTuneré›†æˆ |
| è®­ç»ƒå™¨ | PyTorchåŸç”Ÿ | XTuner Trainer |
| ç»“æœ | âŒ å¤±è´¥ | âœ… å¯è¡Œ |

---

## ğŸ’¡ ä¸‹ä¸€æ­¥å»ºè®®

### é€‰é¡¹A: ä½¿ç”¨å®˜æ–¹è®­ç»ƒæ–¹æ³• â­

**æ—¶é—´**: 1-2å¤©  
**éš¾åº¦**: ä¸­ç­‰  
**æ”¶ç›Š**: å¯èƒ½è¾¾åˆ°ç›®æ ‡æ€§èƒ½

**æ­¥éª¤**ï¼š
1. å‡†å¤‡annotations.json
2. ä¿®æ”¹é…ç½®æ–‡ä»¶
3. è¿è¡Œå®˜æ–¹è®­ç»ƒè„šæœ¬

### é€‰é¡¹B: ä½¿ç”¨é˜ˆå€¼ä¼˜åŒ–

**æ—¶é—´**: ç«‹å³  
**éš¾åº¦**: ç®€å•  
**æ”¶ç›Š**: Val Dice 0.7849ï¼ˆå·²éªŒè¯ï¼‰

---

**æ¨è**: å¦‚æœæ—¶é—´ç´§ï¼Œä½¿ç”¨**é˜ˆå€¼ä¼˜åŒ–**ã€‚å¦‚æœè¦çœŸæ­£è®­ç»ƒï¼Œä½¿ç”¨**å®˜æ–¹è®­ç»ƒæ–¹æ³•**ã€‚
