#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
è¡€ç®¡åˆ†å‰²æ¨ç†è„šæœ¬
ä½¿ç”¨è®­ç»ƒå¥½çš„æƒé‡å¯¹æ•°æ®é›†å›¾ç‰‡è¿›è¡Œé¢„æµ‹
"""

import os
import sys
import torch
import argparse
from PIL import Image
import numpy as np
import cv2
from pathlib import Path
from transformers import AutoModel, AutoTokenizer
import json

def parse_args():
    parser = argparse.ArgumentParser(description='Sa2VA Vessel Segmentation Inference')
    parser.add_argument('--checkpoint', type=str, required=True, help='è®­ç»ƒå¥½çš„checkpointè·¯å¾„')
    parser.add_argument('--image-dir', type=str, default='/home/ubuntu/Sa2VA/data/vessel_data/images', 
                        help='å›¾ç‰‡ç›®å½•')
    parser.add_argument('--output-dir', type=str, default='./inference_results', 
                        help='è¾“å‡ºç›®å½•')
    parser.add_argument('--num-images', type=int, default=10, 
                        help='è¦æ¨ç†çš„å›¾ç‰‡æ•°é‡')
    parser.add_argument('--base-model', type=str, 
                        default='/home/ubuntu/huggingface_cache/models--OpenGVLab--InternVL3-8B/snapshots/853e3a797a661694b1b8ece0cb72dc2b23e3dac9',
                        help='åŸºç¡€æ¨¡å‹è·¯å¾„')
    parser.add_argument('--device', type=str, default='cpu', choices=['cpu', 'cuda'],
                        help='æ¨ç†è®¾å¤‡ (cpu æˆ– cuda)')
    return parser.parse_args()

def show_mask_on_image(image, masks, alpha=0.5):
    """
    åœ¨å›¾åƒä¸Šå åŠ æ©ç 
    """
    if isinstance(image, Image.Image):
        image = np.array(image)
    
    # ç¡®ä¿å›¾åƒæ˜¯RGBæ ¼å¼
    if len(image.shape) == 2:
        image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)
    elif image.shape[2] == 4:
        image = cv2.cvtColor(image, cv2.COLOR_RGBA2RGB)
    
    overlay = image.copy()
    
    # ä¸ºæ¯ä¸ªæ©ç ä½¿ç”¨ä¸åŒçš„é¢œè‰²
    colors = [
        (255, 0, 0),    # çº¢è‰²
        (0, 255, 0),    # ç»¿è‰²
        (0, 0, 255),    # è“è‰²
        (255, 255, 0),  # é»„è‰²
        (255, 0, 255),  # å“çº¢
        (0, 255, 255),  # é’è‰²
    ]
    
    if masks is not None and len(masks) > 0:
        for idx, mask in enumerate(masks):
            if isinstance(mask, torch.Tensor):
                mask = mask.cpu().numpy()
            
            # ç¡®ä¿maskæ˜¯2Dçš„
            if len(mask.shape) > 2:
                mask = mask.squeeze()
            
            # è°ƒæ•´maskå¤§å°ä»¥åŒ¹é…å›¾åƒ
            if mask.shape != image.shape[:2]:
                mask = cv2.resize(mask.astype(np.uint8), 
                                (image.shape[1], image.shape[0]), 
                                interpolation=cv2.INTER_NEAREST)
            
            color = colors[idx % len(colors)]
            mask_bool = mask > 0.5
            
            # åœ¨æ©ç åŒºåŸŸåº”ç”¨é¢œè‰²
            for c in range(3):
                overlay[:, :, c] = np.where(mask_bool, 
                                           color[c], 
                                           overlay[:, :, c])
    
    # æ··åˆåŸå›¾å’Œæ©ç 
    result = cv2.addWeighted(image, 1-alpha, overlay, alpha, 0)
    return result

def load_model(checkpoint_path, base_model_path, device='cpu'):
    """
    åŠ è½½æ¨¡å‹å’Œcheckpoint
    """
    print(f"æ­£åœ¨åŠ è½½åŸºç¡€æ¨¡å‹: {base_model_path}")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½tokenizer
    tokenizer = AutoTokenizer.from_pretrained(
        base_model_path,
        trust_remote_code=True,
        padding_side='right'
    )
    
    # æ ¹æ®è®¾å¤‡é€‰æ‹©dtype
    if device == 'cpu':
        torch_dtype = torch.float32  # CPUä½¿ç”¨float32
        print("âš ï¸  ä½¿ç”¨CPUæ¨ç†,é€Ÿåº¦ä¼šæ¯”è¾ƒæ…¢,è¯·è€å¿ƒç­‰å¾…...")
    else:
        torch_dtype = torch.bfloat16  # GPUä½¿ç”¨bfloat16
    
    # åŠ è½½æ¨¡å‹
    model = AutoModel.from_pretrained(
        base_model_path,
        torch_dtype=torch_dtype,
        low_cpu_mem_usage=True,
        trust_remote_code=True,
    ).eval()
    
    # åŠ è½½è®­ç»ƒå¥½çš„æƒé‡
    print(f"æ­£åœ¨åŠ è½½checkpoint: {checkpoint_path}")
    state_dict = torch.load(checkpoint_path, map_location='cpu', weights_only=False)
    
    # å¤„ç†å¯èƒ½çš„state_dictæ ¼å¼
    if 'state_dict' in state_dict:
        state_dict = state_dict['state_dict']
    
    # ç§»é™¤å¯èƒ½çš„'module.'å‰ç¼€
    new_state_dict = {}
    for k, v in state_dict.items():
        if k.startswith('module.'):
            new_state_dict[k[7:]] = v
        else:
            new_state_dict[k] = v
    
    # åŠ è½½æƒé‡
    model.load_state_dict(new_state_dict, strict=False)
    print("æƒé‡åŠ è½½å®Œæˆ!")
    
    # ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
    if device == 'cuda' and torch.cuda.is_available():
        model = model.cuda()
        print("æ¨¡å‹å·²ç§»è‡³GPU")
    else:
        print("æ¨¡å‹åœ¨CPUä¸Šè¿è¡Œ")
    
    return model, tokenizer

def inference_image(model, tokenizer, image_path, prompt="è¯·åˆ†å‰²å›¾åƒä¸­çš„è¡€ç®¡ã€‚"):
    """
    å¯¹å•å¼ å›¾ç‰‡è¿›è¡Œæ¨ç†
    """
    # åŠ è½½å›¾ç‰‡
    image = Image.open(image_path).convert('RGB')
    
    # å‡†å¤‡è¾“å…¥
    text = f"<image>{prompt}"
    
    input_dict = {
        'image': image,
        'text': text,
        'past_text': '',
        'mask_prompts': None,
        'tokenizer': tokenizer,
    }
    
    # æ¨ç†
    with torch.no_grad():
        try:
            return_dict = model.predict_forward(**input_dict)
            return return_dict
        except Exception as e:
            print(f"æ¨ç†å‡ºé”™: {e}")
            return None

def main():
    args = parse_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # åŠ è½½æ¨¡å‹
    model, tokenizer = load_model(args.checkpoint, args.base_model, args.device)
    
    # è·å–å›¾ç‰‡åˆ—è¡¨
    image_dir = Path(args.image_dir)
    image_files = sorted(list(image_dir.glob('*.png')) + list(image_dir.glob('*.jpg')))[:args.num_images]
    
    print(f"\næ‰¾åˆ° {len(image_files)} å¼ å›¾ç‰‡è¿›è¡Œæ¨ç†")
    print(f"è¾“å‡ºç›®å½•: {output_dir}\n")
    
    results = []
    
    for idx, image_path in enumerate(image_files):
        print(f"[{idx+1}/{len(image_files)}] æ­£åœ¨å¤„ç†: {image_path.name}")
        
        # æ¨ç†
        result = inference_image(model, tokenizer, str(image_path))
        
        if result is None:
            print(f"  âš ï¸  æ¨ç†å¤±è´¥")
            continue
        
        # è·å–é¢„æµ‹ç»“æœ
        prediction_text = result.get('prediction', '').strip()
        prediction_masks = result.get('prediction_masks', [])
        
        print(f"  ğŸ“ é¢„æµ‹æ–‡æœ¬: {prediction_text}")
        print(f"  ğŸ­ æ©ç æ•°é‡: {len(prediction_masks) if prediction_masks else 0}")
        
        # ä¿å­˜ç»“æœ
        original_image = Image.open(image_path).convert('RGB')
        
        # ç”Ÿæˆå¯è§†åŒ–ç»“æœ
        if prediction_masks and len(prediction_masks) > 0:
            vis_image = show_mask_on_image(original_image, prediction_masks)
            vis_image = Image.fromarray(vis_image)
        else:
            vis_image = original_image
        
        # ä¿å­˜å›¾ç‰‡
        output_path = output_dir / f"{image_path.stem}_result.jpg"
        vis_image.save(output_path)
        print(f"  âœ… å·²ä¿å­˜: {output_path}\n")
        
        # è®°å½•ç»“æœ
        results.append({
            'image': image_path.name,
            'prediction': prediction_text,
            'num_masks': len(prediction_masks) if prediction_masks else 0,
            'output': str(output_path)
        })
    
    # ä¿å­˜JSONç»“æœ
    json_path = output_dir / 'results.json'
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… æ¨ç†å®Œæˆ! å…±å¤„ç† {len(results)} å¼ å›¾ç‰‡")
    print(f"ğŸ“Š ç»“æœå·²ä¿å­˜è‡³: {json_path}")

if __name__ == '__main__':
    main()
