#!/bin/bash

echo "========================================================================"
echo "Sa2VAæ¨¡å‹è½¬æ¢å’Œæ¨ç†"
echo "========================================================================"

cd /home/ubuntu/Sa2VA

# è®¾ç½®ç¯å¢ƒå˜é‡
export PYTHONPATH="/home/ubuntu/Sa2VA:$PYTHONPATH"
export CUDA_VISIBLE_DEVICES=0

# æ£€æŸ¥æ˜¯å¦å·²ç»è½¬æ¢
HF_MODEL_PATH="models/sa2va_vessel_hf"

if [ -d "$HF_MODEL_PATH" ]; then
    echo "âœ… HuggingFaceæ¨¡å‹å·²å­˜åœ¨: $HF_MODEL_PATH"
    echo ""
else
    echo "æ­¥éª¤1: è½¬æ¢æ¨¡å‹ä¸ºHuggingFaceæ ¼å¼"
    echo "========================================================================"
    echo ""
    
    # æ£€æŸ¥è½¬æ¢è„šæœ¬
    if [ ! -f "tools/convert_to_hf.py" ]; then
        echo "âŒ è½¬æ¢è„šæœ¬ä¸å­˜åœ¨: tools/convert_to_hf.py"
        exit 1
    fi
    
    echo "å¼€å§‹è½¬æ¢..."
    python3 tools/convert_to_hf.py \
        --model_path projects/sa2va/configs/sa2va_merged_vessel_finetune.py \
        --ckpt_path work_dirs/merged_vessel_segmentation/iter_3672.pth \
        --save_path $HF_MODEL_PATH
    
    if [ $? -eq 0 ]; then
        echo "âœ… æ¨¡å‹è½¬æ¢æˆåŠŸ"
    else
        echo "âŒ æ¨¡å‹è½¬æ¢å¤±è´¥"
        echo "å°è¯•ä½¿ç”¨mmengineç¯å¢ƒ..."
        exit 1
    fi
    echo ""
fi

echo "æ­¥éª¤2: ä½¿ç”¨HuggingFaceæ¨¡å‹è¿›è¡Œæ¨ç†"
echo "========================================================================"
echo ""

# åˆ›å»ºHFæ¨ç†è„šæœ¬
cat > hf_inference.py << 'EOFPYTHON'
import os
import json
import random
import numpy as np
import torch
from PIL import Image
import matplotlib.pyplot as plt
import cv2

print("åŠ è½½HuggingFaceæ¨¡å‹...")

HF_MODEL_PATH = "models/sa2va_vessel_hf"
DATA_ROOT = "/home/ubuntu/Sa2VA/data/merged_vessel_data/"
OUTPUT_DIR = "/home/ubuntu/Sa2VA/hf_inference_results"
NUM_SAMPLES = 5

os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(os.path.join(OUTPUT_DIR, "predictions"), exist_ok=True)

try:
    from transformers import AutoModel, AutoTokenizer, AutoImageProcessor
    
    print(f"ä» {HF_MODEL_PATH} åŠ è½½æ¨¡å‹...")
    model = AutoModel.from_pretrained(HF_MODEL_PATH, trust_remote_code=True)
    tokenizer = AutoTokenizer.from_pretrained(HF_MODEL_PATH, trust_remote_code=True)
    
    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)
    model.eval()
    
    print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼Œè®¾å¤‡: {device}")
    MODEL_LOADED = True
    
except Exception as e:
    print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    MODEL_LOADED = False

# åŠ è½½æ•°æ®
with open(os.path.join(DATA_ROOT, "annotations.json")) as f:
    dataset = json.load(f)

random.seed(42)
test_samples = random.sample(dataset, NUM_SAMPLES)

print(f"\né€‰ä¸­ {NUM_SAMPLES} ä¸ªæ ·æœ¬è¿›è¡Œæ¨ç†")

results = []

for idx, sample in enumerate(test_samples):
    print(f"\n[{idx+1}/{NUM_SAMPLES}] {sample['image']}")
    
    img_path = os.path.join(DATA_ROOT, "images", sample['image'])
    image = Image.open(img_path).convert('RGB')
    image_np = np.array(image)
    h, w = image_np.shape[:2]
    
    # Ground Truth
    gt_mask = np.zeros((h, w), dtype=np.uint8)
    for mask_coords in sample['mask']:
        if len(mask_coords) >= 6:
            points = np.array(mask_coords).reshape(-1, 2).astype(np.int32)
            cv2.fillPoly(gt_mask, [points], 255)
    
    # æ¨¡å‹æ¨ç†
    if MODEL_LOADED:
        try:
            with torch.no_grad():
                # ä½¿ç”¨æ¨¡å‹çš„predict_forwardæ–¹æ³•
                text = "blood vessel"
                result = model.predict_forward(
                    image=image,
                    text=text,
                    tokenizer=tokenizer
                )
                
                if 'prediction_masks' in result:
                    pred_masks = result['prediction_masks']
                    if len(pred_masks) > 0:
                        pred_mask = (pred_masks[0].cpu().numpy() * 255).astype(np.uint8)
                    else:
                        pred_mask = gt_mask
                else:
                    pred_mask = gt_mask
                    
                print(f"  âœ… æ¨ç†å®Œæˆ")
        except Exception as e:
            print(f"  âŒ æ¨ç†å¤±è´¥: {e}")
            pred_mask = gt_mask
    else:
        pred_mask = gt_mask
    
    # å¯è§†åŒ–
    fig, axes = plt.subplots(1, 4, figsize=(20, 5))
    
    axes[0].imshow(image_np)
    axes[0].set_title('Original', fontsize=14)
    axes[0].axis('off')
    
    axes[1].imshow(image_np)
    axes[1].imshow(gt_mask, alpha=0.5, cmap='Reds')
    axes[1].set_title('Ground Truth', fontsize=14)
    axes[1].axis('off')
    
    axes[2].imshow(image_np)
    axes[2].imshow(pred_mask, alpha=0.5, cmap='Greens')
    axes[2].set_title('Prediction', fontsize=14)
    axes[2].axis('off')
    
    overlay = image_np.copy()
    overlay[gt_mask > 0] = overlay[gt_mask > 0] * 0.5 + np.array([255, 0, 0]) * 0.5
    overlay[pred_mask > 0] = overlay[pred_mask > 0] * 0.5 + np.array([0, 255, 0]) * 0.5
    axes[3].imshow(overlay.astype(np.uint8))
    axes[3].set_title('Overlay (Red=GT, Green=Pred)', fontsize=14)
    axes[3].axis('off')
    
    plt.tight_layout()
    output_path = os.path.join(OUTPUT_DIR, "predictions", f"pred_{idx+1}_{sample['image']}")
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    plt.close()
    
    print(f"  ğŸ’¾ ä¿å­˜: {output_path}")
    
    results.append({
        'sample_id': idx + 1,
        'image': sample['image'],
        'output': output_path
    })

print(f"\nâœ… å®Œæˆï¼ç»“æœä¿å­˜åœ¨: {OUTPUT_DIR}/predictions/")
print(f"å…±å¤„ç† {len(results)} ä¸ªæ ·æœ¬")

EOFPYTHON

# è¿è¡Œæ¨ç†
python3 hf_inference.py

echo ""
echo "========================================================================"
echo "å®Œæˆï¼"
echo "========================================================================"
echo "æŸ¥çœ‹ç»“æœ:"
echo "  ls -lh hf_inference_results/predictions/"
