"""
Sa2VA Êé®ÁêÜ + TTA (Test Time Augmentation)
ÂØπÂêå‰∏ÄÂº†ÂõæËøõË°åÂ§öÊ¨°ÂèòÊç¢ÔºåÈ¢ÑÊµãÂêéÂèñÂπ≥Âùá
"""
import os
import sys
import json
import random
import numpy as np
import torch
from PIL import Image
import cv2
from sklearn.metrics import jaccard_score, f1_score, precision_score, recall_score, accuracy_score
from transformers import AutoModelForCausalLM, AutoTokenizer

sys.path.insert(0, '/home/ubuntu/Sa2VA')

print("=" * 80)
print("Sa2VA + TTA (Test Time Augmentation)")
print("=" * 80)

# ÈÖçÁΩÆ
HF_MODEL_PATH = "/home/ubuntu/Sa2VA/models/sa2va_vessel_hf"
DATA_ROOT = "/home/ubuntu/Sa2VA/data/merged_vessel_data/"
OUTPUT_DIR = "/home/ubuntu/Sa2VA/evaluation_tta_results"
NUM_SAMPLES = 10

print(f"HFÊ®°ÂûãË∑ØÂæÑ: {HF_MODEL_PATH}")
print(f"Êï∞ÊçÆË∑ØÂæÑ: {DATA_ROOT}")
print(f"ËØÑ‰º∞Ê†∑Êú¨Êï∞: {NUM_SAMPLES}")
print()

os.makedirs(OUTPUT_DIR, exist_ok=True)

# ËØÑ‰ª∑ÊåáÊ†áËÆ°ÁÆó
def calculate_metrics(pred_mask, gt_mask):
    """ËÆ°ÁÆóÂàÜÂâ≤ËØÑ‰ª∑ÊåáÊ†á"""
    pred_flat = (pred_mask > 127).flatten().astype(int)
    gt_flat = (gt_mask > 127).flatten().astype(int)
    
    if len(np.unique(gt_flat)) == 1 and len(np.unique(pred_flat)) == 1:
        if gt_flat[0] == pred_flat[0]:
            return {'IoU': 1.0, 'Dice': 1.0, 'Precision': 1.0, 'Recall': 1.0, 'Accuracy': 1.0}
        else:
            return {'IoU': 0.0, 'Dice': 0.0, 'Precision': 0.0, 'Recall': 0.0, 'Accuracy': 0.0}
    
    iou = jaccard_score(gt_flat, pred_flat, zero_division=0)
    dice = f1_score(gt_flat, pred_flat, zero_division=0)
    precision = precision_score(gt_flat, pred_flat, zero_division=0)
    recall = recall_score(gt_flat, pred_flat, zero_division=0)
    accuracy = accuracy_score(gt_flat, pred_flat)
    
    return {
        'IoU': float(iou),
        'Dice': float(dice),
        'Precision': float(precision),
        'Recall': float(recall),
        'Accuracy': float(accuracy),
    }


def predict_single(model, tokenizer, image):
    """ÂçïÊ¨°È¢ÑÊµã"""
    text = "<image>Please segment the blood vessel."
    
    with torch.no_grad():
        result = model.predict_forward(
            image=image,
            text=text,
            tokenizer=tokenizer,
            processor=None,
        )
    
    prediction_text = result.get('prediction', '')
    
    if '[SEG]' in prediction_text and 'probability_maps' in result:
        prob_maps = result['probability_maps']
        if len(prob_maps) > 0:
            prob_map = prob_maps[0][0]
            if isinstance(prob_map, torch.Tensor):
                prob_map = prob_map.cpu().numpy()
            return prob_map
    
    return None


def predict_with_tta(model, tokenizer, image):
    """
    TTAÈ¢ÑÊµãÔºöÂØπÂõæÂÉèËøõË°åÂ§öÁßçÂèòÊç¢ÔºåÈ¢ÑÊµãÂêéÂèñÂπ≥Âùá
    ÂèòÊç¢ÔºöÂéüÂõæ„ÄÅÊ∞¥Âπ≥ÁøªËΩ¨„ÄÅÂûÇÁõ¥ÁøªËΩ¨„ÄÅÊ∞¥Âπ≥+ÂûÇÁõ¥ÁøªËΩ¨
    """
    h, w = image.size[1], image.size[0]  # PIL: (w, h)
    
    # Êî∂ÈõÜÊâÄÊúâÊ¶ÇÁéáÂõæ
    prob_maps = []
    
    # 1. ÂéüÂõæ
    prob = predict_single(model, tokenizer, image)
    if prob is not None:
        if prob.shape != (h, w):
            prob = cv2.resize(prob, (w, h), interpolation=cv2.INTER_LINEAR)
        prob_maps.append(prob)
    
    # 2. Ê∞¥Âπ≥ÁøªËΩ¨
    image_hflip = image.transpose(Image.FLIP_LEFT_RIGHT)
    prob = predict_single(model, tokenizer, image_hflip)
    if prob is not None:
        if prob.shape != (h, w):
            prob = cv2.resize(prob, (w, h), interpolation=cv2.INTER_LINEAR)
        # ÁøªËΩ¨ÂõûÊù•
        prob = np.fliplr(prob)
        prob_maps.append(prob)
    
    # 3. ÂûÇÁõ¥ÁøªËΩ¨
    image_vflip = image.transpose(Image.FLIP_TOP_BOTTOM)
    prob = predict_single(model, tokenizer, image_vflip)
    if prob is not None:
        if prob.shape != (h, w):
            prob = cv2.resize(prob, (w, h), interpolation=cv2.INTER_LINEAR)
        # ÁøªËΩ¨ÂõûÊù•
        prob = np.flipud(prob)
        prob_maps.append(prob)
    
    # 4. Ê∞¥Âπ≥+ÂûÇÁõ¥ÁøªËΩ¨ (180Â∫¶ÊóãËΩ¨)
    image_hvflip = image.transpose(Image.FLIP_LEFT_RIGHT).transpose(Image.FLIP_TOP_BOTTOM)
    prob = predict_single(model, tokenizer, image_hvflip)
    if prob is not None:
        if prob.shape != (h, w):
            prob = cv2.resize(prob, (w, h), interpolation=cv2.INTER_LINEAR)
        # ÁøªËΩ¨ÂõûÊù•
        prob = np.flipud(np.fliplr(prob))
        prob_maps.append(prob)
    
    if len(prob_maps) == 0:
        return None
    
    # Âπ≥ÂùáÊâÄÊúâÊ¶ÇÁéáÂõæ
    avg_prob = np.mean(prob_maps, axis=0)
    
    # ‰∫åÂÄºÂåñ
    pred_mask = (avg_prob > 0.5).astype(np.uint8) * 255
    
    return pred_mask, len(prob_maps)


# Âä†ËΩΩÊ®°Âûã
print("üì• Loading model...")
tokenizer = AutoTokenizer.from_pretrained(HF_MODEL_PATH, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    HF_MODEL_PATH,
    trust_remote_code=True,
    torch_dtype=torch.bfloat16,
    device_map='auto',
)
model.eval()
print("‚úÖ Model loaded")

# Âä†ËΩΩÊï∞ÊçÆ
ann_file = os.path.join(DATA_ROOT, "annotations.json")
with open(ann_file, 'r') as f:
    annotations = json.load(f)

# Á≠õÈÄâÊúâÊïàÊ†∑Êú¨
valid_samples = []
for item in annotations:
    if 'image' in item and 'mask' in item and len(item['mask']) > 0:
        img_path = os.path.join(DATA_ROOT, "images", item['image'])
        if os.path.exists(img_path):
            valid_samples.append(item)

print(f"ÊúâÊïàÊ†∑Êú¨: {len(valid_samples)}")

# ÈöèÊú∫ÈÄâÊã©Ê†∑Êú¨
random.seed(42)
if len(valid_samples) > NUM_SAMPLES:
    test_samples = random.sample(valid_samples, NUM_SAMPLES)
else:
    test_samples = valid_samples

print(f"ÊµãËØïÊ†∑Êú¨: {len(test_samples)}")
print()

# ËØÑ‰º∞
all_metrics = []
all_metrics_no_tta = []

for idx, sample in enumerate(test_samples):
    print(f"\n[{idx+1}/{len(test_samples)}] {sample['image']}")
    
    # Âä†ËΩΩÂõæÂÉè
    img_path = os.path.join(DATA_ROOT, "images", sample['image'])
    image = Image.open(img_path).convert('RGB')
    w, h = image.size
    
    # ÂàõÂª∫GT mask
    gt_mask = np.zeros((h, w), dtype=np.uint8)
    for mask_coords in sample['mask']:
        if len(mask_coords) >= 6:
            points = np.array(mask_coords).reshape(-1, 2).astype(np.int32)
            cv2.fillPoly(gt_mask, [points], 255)
    
    # Êó†TTAÈ¢ÑÊµã
    prob_no_tta = predict_single(model, tokenizer, image)
    if prob_no_tta is not None:
        if prob_no_tta.shape != (h, w):
            prob_no_tta = cv2.resize(prob_no_tta, (w, h), interpolation=cv2.INTER_LINEAR)
        pred_mask_no_tta = (prob_no_tta > 0.5).astype(np.uint8) * 255
        metrics_no_tta = calculate_metrics(pred_mask_no_tta, gt_mask)
    else:
        pred_mask_no_tta = np.zeros((h, w), dtype=np.uint8)
        metrics_no_tta = {'IoU': 0, 'Dice': 0, 'Precision': 0, 'Recall': 0, 'Accuracy': 0}
    
    all_metrics_no_tta.append(metrics_no_tta)
    
    # TTAÈ¢ÑÊµã
    result = predict_with_tta(model, tokenizer, image)
    if result is not None:
        pred_mask_tta, num_augs = result
        metrics_tta = calculate_metrics(pred_mask_tta, gt_mask)
    else:
        pred_mask_tta = np.zeros((h, w), dtype=np.uint8)
        num_augs = 0
        metrics_tta = {'IoU': 0, 'Dice': 0, 'Precision': 0, 'Recall': 0, 'Accuracy': 0}
    
    all_metrics.append(metrics_tta)
    
    print(f"  Êó†TTA: Dice={metrics_no_tta['Dice']:.4f}")
    print(f"  ÊúâTTA ({num_augs}x): Dice={metrics_tta['Dice']:.4f} (Œî={metrics_tta['Dice']-metrics_no_tta['Dice']:+.4f})")

# Ê±áÊÄª
print("\n" + "=" * 80)
print("üìä ÁªìÊûúÊ±áÊÄª")
print("=" * 80)

mean_no_tta = {k: np.mean([m[k] for m in all_metrics_no_tta]) for k in all_metrics_no_tta[0].keys()}
mean_tta = {k: np.mean([m[k] for m in all_metrics]) for k in all_metrics[0].keys()}

print(f"\nÊó†TTA (Baseline):")
print(f"  IoU:       {mean_no_tta['IoU']:.4f}")
print(f"  Dice:      {mean_no_tta['Dice']:.4f}")
print(f"  Precision: {mean_no_tta['Precision']:.4f}")
print(f"  Recall:    {mean_no_tta['Recall']:.4f}")

print(f"\nÊúâTTA (4x Augmentation):")
print(f"  IoU:       {mean_tta['IoU']:.4f} (Œî={mean_tta['IoU']-mean_no_tta['IoU']:+.4f})")
print(f"  Dice:      {mean_tta['Dice']:.4f} (Œî={mean_tta['Dice']-mean_no_tta['Dice']:+.4f})")
print(f"  Precision: {mean_tta['Precision']:.4f} (Œî={mean_tta['Precision']-mean_no_tta['Precision']:+.4f})")
print(f"  Recall:    {mean_tta['Recall']:.4f} (Œî={mean_tta['Recall']-mean_no_tta['Recall']:+.4f})")

print("\n" + "=" * 80)
