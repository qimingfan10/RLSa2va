# Sa2VA è®­ç»ƒä¸é¢„æµ‹å®Œæ•´æŒ‡å—

## ğŸ“š ç›®å½•

- [1. æ¦‚è¿°](#1-æ¦‚è¿°)
- [2. ä½¿ç”¨çš„æ•°æ®é›†](#2-ä½¿ç”¨çš„æ•°æ®é›†)
- [3. ä½¿ç”¨çš„æƒé‡](#3-ä½¿ç”¨çš„æƒé‡)
- [4. ç¯å¢ƒå‡†å¤‡](#4-ç¯å¢ƒå‡†å¤‡)
- [5. å¦‚ä½•å¼€å§‹è®­ç»ƒ](#5-å¦‚ä½•å¼€å§‹è®­ç»ƒ)
- [6. å¦‚ä½•å¼€å§‹é¢„æµ‹](#6-å¦‚ä½•å¼€å§‹é¢„æµ‹)
- [7. æ¨¡å‹è½¬æ¢](#7-æ¨¡å‹è½¬æ¢)
- [8. å¸¸è§é—®é¢˜](#8-å¸¸è§é—®é¢˜)

---

## 1. æ¦‚è¿°

### Sa2VAæ˜¯ä»€ä¹ˆï¼Ÿ

**Sa2VA (SAM2 + Vision-Language Assistant)** æ˜¯ä¸€ä¸ªç»“åˆäº†SAM2åˆ†å‰²æ¨¡å‹å’Œå¤§å‹å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„ç»Ÿä¸€æ¡†æ¶ï¼Œç”¨äºå›¾åƒå’Œè§†é¢‘çš„å¯†é›†ç†è§£ä¸åˆ†å‰²ä»»åŠ¡ã€‚

### æœ¬é¡¹ç›®çš„åº”ç”¨åœºæ™¯

æœ¬é¡¹ç›®ä¸“æ³¨äº **OCTè¡€ç®¡åˆ†å‰²ä»»åŠ¡**ï¼Œä½¿ç”¨Sa2VA-InternVL3-8Bæ¨¡å‹åœ¨åŒ»å­¦OCTï¼ˆå…‰å­¦ç›¸å¹²æ–­å±‚æ‰«æï¼‰è¡€ç®¡å›¾åƒä¸Šè¿›è¡Œfine-tuneã€‚

### æŠ€æœ¯æ¶æ„

```
Sa2VA-8B æ¨¡å‹æ¶æ„
â”œâ”€â”€ Vision Encoder (InternVL3-8B)    â† å†»ç»“ï¼Œæå–å›¾åƒç‰¹å¾
â”œâ”€â”€ Language Model (8B)              â† LoRAå¾®è°ƒï¼Œç†è§£æŒ‡ä»¤
â”œâ”€â”€ Projector                        â† è§†è§‰-è¯­è¨€ç‰¹å¾å¯¹é½
â””â”€â”€ SAM2 Decoder                     â† å¯è®­ç»ƒï¼Œç”Ÿæˆåˆ†å‰²mask
```

---

## 2. ä½¿ç”¨çš„æ•°æ®é›†

### ğŸ“Š OCTè¡€ç®¡åˆ†å‰²æ•°æ®é›†

#### æ•°æ®é›†ä½ç½®

```
/home/ubuntu/Sa2VA/Segment_DATA_Merged_512/
â”œâ”€â”€ images/          â† OCTè¡€ç®¡å›¾åƒï¼ˆ512x512ï¼‰
â”œâ”€â”€ masks/           â† åˆ†å‰²æ ‡æ³¨mask
â”œâ”€â”€ json/            â† æ ‡æ³¨ä¿¡æ¯ï¼ˆå¤šè¾¹å½¢ï¼‰
â”œâ”€â”€ annotations.json â† æ•°æ®é›†ç´¢å¼•æ–‡ä»¶
â””â”€â”€ README.md        â† æ•°æ®é›†è¯´æ˜
```

#### æ•°æ®é›†ç»Ÿè®¡

| æŒ‡æ ‡ | æ•°å€¼ |
|------|------|
| **å›¾åƒæ•°é‡** | 1,220å¼  |
| **å›¾åƒå°ºå¯¸** | 512 Ã— 512 åƒç´  |
| **å›¾åƒæ ¼å¼** | JPG |
| **æ ‡æ³¨æ ¼å¼** | JSON (å¤šè¾¹å½¢åæ ‡) |
| **æ€»å¤§å°** | ~194MB |

#### æ•°æ®é›†ç»“æ„

`annotations.json` æ–‡ä»¶æ ¼å¼ï¼š

```json
[
  {
    "image": "images/patient_name_frame_000001.jpg",
    "mask": [
      {
        "segmentation": [[x1, y1, x2, y2, ...]], // å¤šè¾¹å½¢åæ ‡
        "area": 12345,
        "bbox": [x, y, width, height],
        "category_id": 1
      }
    ],
    "conversations": [
      {
        "from": "human",
        "value": "<image>\nPlease segment the blood vessels."
      },
      {
        "from": "gpt",
        "value": "<p>blood vessels</p><vp>[[x1,y1,x2,y2,...]]</vp>[SEG]"
      }
    ]
  }
]
```

#### æ•°æ®å‡†å¤‡

æ•°æ®é›†å·²å­˜æ”¾åœ¨HuggingFaceï¼š

```bash
# ä¸‹è½½æ•°æ®é›†
huggingface-cli download \
    ly17/sa2va-vessel-dataset \
    --local-dir Segment_DATA_Merged_512 \
    --repo-type dataset
```

#### æ•°æ®é¢„å¤„ç†

æ•°æ®é›†å·²ç»è¿‡ä»¥ä¸‹é¢„å¤„ç†ï¼š
- âœ… å›¾åƒresizeåˆ°512Ã—512
- âœ… æ ‡æ³¨è½¬æ¢ä¸ºå¤šè¾¹å½¢æ ¼å¼
- âœ… åˆ›å»ºannotations.jsonç´¢å¼•
- âœ… æ•°æ®æ¸…æ´—å’ŒéªŒè¯

---

## 3. ä½¿ç”¨çš„æƒé‡

### ğŸ¯ é¢„è®­ç»ƒæƒé‡

#### 3.1 åŸºç¡€è§†è§‰-è¯­è¨€æ¨¡å‹

**InternVL3-8B**

```bash
# HuggingFaceåœ°å€
https://huggingface.co/OpenGVLab/InternVL3-8B

# æœ¬åœ°ç¼“å­˜è·¯å¾„
/home/ubuntu/huggingface_cache/models--OpenGVLab--InternVL3-8B/snapshots/853e3a797a661694b1b8ece0cb72dc2b23e3dac9/
```

**ç»„æˆéƒ¨åˆ†**ï¼š
- Vision Encoder: InternViT-6B (å†»ç»“)
- Language Model: Qwen2.5-7B (LoRAå¾®è°ƒ)

#### 3.2 Sa2VAé¢„è®­ç»ƒæƒé‡

**Sa2VA-26B.pth**

```bash
# æœ¬åœ°è·¯å¾„
/home/ubuntu/Sa2VA-26B.pth

# å¤§å°
~60GB

# è¯´æ˜
Sa2VA-26Bæ¨¡å‹çš„é¢„è®­ç»ƒæƒé‡ï¼Œç”¨äºåˆå§‹åŒ–8Bæ¨¡å‹
åªåŠ è½½å½¢çŠ¶åŒ¹é…çš„æƒé‡ï¼ˆçŸ¥è¯†è’¸é¦ï¼‰
```

**æ¥æº**: ByteDanceå®˜æ–¹å‘å¸ƒçš„Sa2VA-26Bæ¨¡å‹

#### 3.3 SAM2 Decoderæƒé‡

**sam2_hiera_large.pt**

```bash
# HuggingFaceåœ°å€
https://huggingface.co/facebook/sam2-hiera-large

# è¯´æ˜
SAM2åˆ†å‰²æ¨¡å‹çš„decoderéƒ¨åˆ†
åœ¨Sa2VAä¸­é›†æˆç”¨äºmaskç”Ÿæˆ
```

### ğŸ“¦ æƒé‡åŠ è½½ç­–ç•¥

#### è®­ç»ƒæ—¶çš„æƒé‡åŠ è½½æµç¨‹

```python
# 1. åŠ è½½InternVL3-8BåŸºç¡€æ¨¡å‹
model_path = "/home/ubuntu/huggingface_cache/.../InternVL3-8B/"

# 2. åŠ è½½Sa2VA-26Bé¢„è®­ç»ƒæƒé‡
pretrained_pth = "/home/ubuntu/Sa2VA-26B.pth"

# 3. æƒé‡åŒ¹é…ä¸åŠ è½½
# - Vision Encoder: ä»InternVL3-8BåŠ è½½ï¼ˆå†»ç»“ï¼‰
# - Language Model: ä»InternVL3-8BåŠ è½½ + LoRA
# - Projector: ä»Sa2VA-26BåŠ è½½ï¼ˆå½¢çŠ¶åŒ¹é…çš„éƒ¨åˆ†ï¼‰
# - SAM2 Decoder: ä»Sa2VA-26BåŠ è½½ï¼ˆå¯è®­ç»ƒï¼‰
```

#### æƒé‡åˆå§‹åŒ–è¯´æ˜

| æ¨¡å— | åˆå§‹åŒ–æ¥æº | è®­ç»ƒç­–ç•¥ |
|------|-----------|----------|
| **Vision Encoder** | InternVL3-8B | â„ï¸ å†»ç»“ |
| **Language Model** | InternVL3-8B | ğŸ”¥ LoRA (r=64) |
| **Projector** | Sa2VA-26B (åŒ¹é…éƒ¨åˆ†) | ğŸ”¥ å¯è®­ç»ƒ |
| **SAM2 Decoder** | Sa2VA-26B | ğŸ”¥ å¯è®­ç»ƒ |
| **Embed/LM Head** | InternVL3-8B | ğŸ”¥ å¯è®­ç»ƒ |

### ğŸ“ ä¸ºä»€ä¹ˆä½¿ç”¨26Bæƒé‡åˆå§‹åŒ–8Bæ¨¡å‹ï¼Ÿ

**çŸ¥è¯†è’¸é¦ç­–ç•¥**ï¼š
1. âœ… åˆ©ç”¨å¤§æ¨¡å‹çš„è§†è§‰-è¯­è¨€ç†è§£èƒ½åŠ›
2. âœ… è½¬ç§»åˆ†å‰²ä»»åŠ¡çš„å…ˆéªŒçŸ¥è¯†
3. âœ… åŠ é€Ÿå°æ¨¡å‹æ”¶æ•›
4. âœ… æå‡æœ€ç»ˆæ€§èƒ½

**å®é™…æ•ˆæœ**ï¼š
- ä½¿ç”¨26Båˆå§‹åŒ–ï¼šæ›´å¿«æ”¶æ•›ï¼Œæ›´å¥½æ€§èƒ½
- ä»å¤´è®­ç»ƒï¼šéœ€è¦æ›´å¤šæ•°æ®å’Œæ—¶é—´

---

## 4. ç¯å¢ƒå‡†å¤‡

### 4.1 ç¡¬ä»¶è¦æ±‚

#### è®­ç»ƒ

| é…ç½® | æ¨è | æœ€ä½ |
|------|------|------|
| **GPU** | 4Ã—RTX 3090 (24GB) | 2Ã—RTX 3090 |
| **å†…å­˜** | 128GB+ | 64GB |
| **å­˜å‚¨** | 500GB+ SSD | 200GB |
| **æ˜¾å­˜** | 24GBÃ—4 | 24GBÃ—2 |

**æ³¨æ„**ï¼šä½¿ç”¨DeepSpeed ZeRO-3å¯ä»¥åœ¨æœ‰é™æ˜¾å­˜ä¸‹è®­ç»ƒå¤§æ¨¡å‹

#### æ¨ç†

| é…ç½® | æ¨è | æœ€ä½ |
|------|------|------|
| **GPU** | 1Ã—RTX 3090 (24GB) | 1Ã—RTX 3080 (10GB) |
| **å†…å­˜** | 32GB+ | 16GB |
| **æ˜¾å­˜** | 24GB | 12GB |

### 4.2 è½¯ä»¶ç¯å¢ƒ

#### åˆ›å»ºCondaç¯å¢ƒ

```bash
# ä½¿ç”¨micromambaï¼ˆæ¨èï¼‰
micromamba create -n sa2va python=3.10 -y
micromamba activate sa2va

# æˆ–ä½¿ç”¨conda
conda create -n sa2va python=3.10 -y
conda activate sa2va
```

#### å®‰è£…ä¾èµ–

```bash
cd /home/ubuntu/Sa2VA

# å®‰è£…PyTorch
pip install torch==2.1.0 torchvision==0.16.0 --index-url https://download.pytorch.org/whl/cu118

# å®‰è£…XTunerå’Œä¾èµ–
pip install -e '.[all]'

# æˆ–å®‰è£…requirements.txt
pip install -r requirements.txt
```

#### æ ¸å¿ƒä¾èµ–ç‰ˆæœ¬

```
torch==2.1.0
transformers==4.37.2
xtuner>=0.1.17
deepspeed==0.12.6
peft==0.7.1
mmengine==0.10.1
opencv-python==4.9.0.80
pillow==10.2.0
huggingface-hub==0.20.3
```

### 4.3 ä¸‹è½½é¢„è®­ç»ƒæƒé‡

#### InternVL3-8B

```bash
# æ–¹æ³•1: ä½¿ç”¨huggingface-cliï¼ˆæ¨èï¼‰
huggingface-cli download OpenGVLab/InternVL3-8B \
    --local-dir /home/ubuntu/huggingface_cache/models--OpenGVLab--InternVL3-8B/snapshots/853e3a797a661694b1b8ece0cb72dc2b23e3dac9

# æ–¹æ³•2: Pythonä»£ç 
python -c "
from transformers import AutoModel
model = AutoModel.from_pretrained('OpenGVLab/InternVL3-8B', trust_remote_code=True)
"
```

#### Sa2VA-26Bé¢„è®­ç»ƒæƒé‡

```bash
# ä¸‹è½½ï¼ˆå¦‚æœæ²¡æœ‰ï¼‰
# æ³¨æ„ï¼šè¿™æ˜¯60GBçš„å¤§æ–‡ä»¶
huggingface-cli download ByteDance/Sa2VA-26B \
    --local-dir /tmp/sa2va-26b

# è½¬æ¢ä¸º.pthæ ¼å¼ï¼ˆå¦‚éœ€è¦ï¼‰
python tools/convert_hf_to_pth.py \
    --hf-model /tmp/sa2va-26b \
    --save-path /home/ubuntu/Sa2VA-26B.pth
```

---

## 5. å¦‚ä½•å¼€å§‹è®­ç»ƒ

### 5.1 è®­ç»ƒé…ç½®æ–‡ä»¶

é…ç½®æ–‡ä»¶ä½ç½®ï¼š
```
/home/ubuntu/Sa2VA/projects/sa2va/configs/sa2va_vessel_finetune.py
```

### 5.2 å…³é”®è®­ç»ƒå‚æ•°

```python
# æ¨¡å‹
path = "/home/ubuntu/huggingface_cache/.../InternVL3-8B/"
pretrained_pth = "/home/ubuntu/Sa2VA-26B.pth"

# æ•°æ®
DATA_ROOT = '/home/ubuntu/Sa2VA/data/'
batch_size = 1              # æ¯GPUæ‰¹æ¬¡å¤§å°
accumulative_counts = 8     # æ¢¯åº¦ç´¯ç§¯ï¼ˆæœ‰æ•ˆbatch=32ï¼‰
max_length = 4096           # åºåˆ—é•¿åº¦

# ä¼˜åŒ–å™¨
lr = 2e-5                   # å­¦ä¹ ç‡
weight_decay = 0.05
max_epochs = 1              # epochæ•°
warmup_ratio = 0.1          # warmupæ¯”ä¾‹

# ä¿å­˜
save_steps = 500            # æ¯500æ­¥ä¿å­˜ä¸€æ¬¡
save_total_limit = 5        # ä¿ç•™5ä¸ªcheckpoint

# LoRAé…ç½®
r = 64                      # LoRA rank
lora_alpha = 128
lora_dropout = 0.1
```

### 5.3 è®­ç»ƒå‘½ä»¤

#### å•æœºå¤šå¡è®­ç»ƒï¼ˆæ¨èï¼‰

```bash
cd /home/ubuntu/Sa2VA

# 4å¡è®­ç»ƒï¼ˆä½¿ç”¨DeepSpeedï¼‰
CUDA_VISIBLE_DEVICES=0,1,2,3 \
xtuner train \
    projects/sa2va/configs/sa2va_vessel_finetune.py \
    --work-dir work_dirs/vessel_segmentation \
    --deepspeed deepspeed_zero3
```

#### 2å¡è®­ç»ƒ

```bash
# è°ƒæ•´é…ç½®ä¸­çš„accumulative_countsä»¥ä¿æŒæœ‰æ•ˆbatch size
CUDA_VISIBLE_DEVICES=0,1 \
xtuner train \
    projects/sa2va/configs/sa2va_vessel_finetune.py \
    --work-dir work_dirs/vessel_segmentation \
    --deepspeed deepspeed_zero3
```

#### ä½¿ç”¨è‡ªå®šä¹‰é…ç½®

```bash
# å¤åˆ¶é…ç½®æ–‡ä»¶
cp projects/sa2va/configs/sa2va_vessel_finetune.py \
   projects/sa2va/configs/my_config.py

# ç¼–è¾‘my_config.pyä¿®æ”¹å‚æ•°

# è®­ç»ƒ
CUDA_VISIBLE_DEVICES=0,1,2,3 \
xtuner train \
    projects/sa2va/configs/my_config.py \
    --work-dir work_dirs/my_experiment
```

### 5.4 å®Œæ•´è®­ç»ƒè„šæœ¬

åˆ›å»º `train_vessel.sh`:

```bash
#!/bin/bash

# Sa2VA OCTè¡€ç®¡åˆ†å‰²è®­ç»ƒè„šæœ¬

set -e

# é…ç½®
export CUDA_VISIBLE_DEVICES=0,1,2,3
export PYTHONPATH=/home/ubuntu/Sa2VA:$PYTHONPATH

# å·¥ä½œç›®å½•
WORK_DIR="work_dirs/vessel_segmentation_$(date +%Y%m%d_%H%M%S)"
CONFIG="projects/sa2va/configs/sa2va_vessel_finetune.py"

echo "========================================"
echo "Sa2VA OCTè¡€ç®¡åˆ†å‰²è®­ç»ƒ"
echo "========================================"
echo "é…ç½®æ–‡ä»¶: $CONFIG"
echo "å·¥ä½œç›®å½•: $WORK_DIR"
echo "GPU: $CUDA_VISIBLE_DEVICES"
echo ""

# æ£€æŸ¥æ•°æ®é›†
if [ ! -d "Segment_DATA_Merged_512" ]; then
    echo "âŒ æ•°æ®é›†ä¸å­˜åœ¨ï¼Œè¯·å…ˆä¸‹è½½æ•°æ®é›†"
    exit 1
fi

echo "âœ… æ•°æ®é›†å·²å°±ç»ª"

# æ£€æŸ¥æƒé‡
if [ ! -f "/home/ubuntu/Sa2VA-26B.pth" ]; then
    echo "âš ï¸  Sa2VA-26B.pthä¸å­˜åœ¨ï¼Œå°†ä½¿ç”¨é»˜è®¤åˆå§‹åŒ–"
fi

# å¼€å§‹è®­ç»ƒ
echo ""
echo "å¼€å§‹è®­ç»ƒ..."
echo ""

xtuner train \
    $CONFIG \
    --work-dir $WORK_DIR \
    --deepspeed deepspeed_zero3

echo ""
echo "========================================"
echo "âœ… è®­ç»ƒå®Œæˆï¼"
echo "========================================"
echo "æ£€æŸ¥ç‚¹ä¿å­˜åœ¨: $WORK_DIR"
echo ""
```

ä½¿ç”¨ï¼š

```bash
chmod +x train_vessel.sh
bash train_vessel.sh
```

### 5.5 è®­ç»ƒç›‘æ§

#### å®æ—¶ç›‘æ§æ—¥å¿—

```bash
# æ–¹æ³•1: æŸ¥çœ‹è®­ç»ƒæ—¥å¿—
tail -f work_dirs/vessel_segmentation/$(date +%Y%m%d)_*.log

# æ–¹æ³•2: ä½¿ç”¨tmux/screen
tmux new -s training
# åœ¨tmuxä¸­è¿è¡Œè®­ç»ƒ
# Ctrl+B, D åˆ†ç¦»ä¼šè¯
# tmux attach -t training é‡æ–°è¿æ¥
```

#### æŸ¥çœ‹GPUä½¿ç”¨

```bash
# å®æ—¶ç›‘æ§
watch -n 1 nvidia-smi

# æˆ–ä½¿ç”¨nvitop
nvitop
```

#### TensorBoardå¯è§†åŒ–ï¼ˆå¯é€‰ï¼‰

```bash
# å®‰è£…tensorboard
pip install tensorboard

# å¯åŠ¨tensorboard
tensorboard --logdir work_dirs/vessel_segmentation
```

### 5.6 è®­ç»ƒè¾“å‡º

è®­ç»ƒå®Œæˆåï¼Œå·¥ä½œç›®å½•ç»“æ„ï¼š

```
work_dirs/vessel_segmentation/
â”œâ”€â”€ iter_500.pth          â† Checkpoint (500æ­¥)
â”œâ”€â”€ iter_1000.pth         â† Checkpoint (1000æ­¥)
â”œâ”€â”€ iter_12192.pth        â† æœ€ç»ˆCheckpoint
â”œâ”€â”€ 20251128_102030.log   â† è®­ç»ƒæ—¥å¿—
â”œâ”€â”€ tf_logs/              â† TensorBoardæ—¥å¿—
â””â”€â”€ vis_data/             â† å¯è§†åŒ–æ•°æ®ï¼ˆå¯é€‰ï¼‰
```

### 5.7 è®­ç»ƒæ—¶é—´ä¼°è®¡

åŸºäº4Ã—RTX 3090 (24GB)ï¼š

| æ•°æ®é‡ | Batch Size | é¢„è®¡æ—¶é—´ |
|--------|-----------|----------|
| 1,220å¼  (Ã—10é‡å¤) | 32 (æœ‰æ•ˆ) | ~8-12å°æ—¶ |
| 1,220å¼  (Ã—5é‡å¤) | 32 (æœ‰æ•ˆ) | ~4-6å°æ—¶ |
| 1,220å¼  (Ã—1é‡å¤) | 32 (æœ‰æ•ˆ) | ~1-2å°æ—¶ |

**å®é™…è®­ç»ƒ**ï¼š
- iter_12192ï¼šçº¦12,192æ­¥
- å¹³å‡é€Ÿåº¦ï¼šçº¦3-4ç§’/æ­¥
- æ€»æ—¶é—´ï¼šçº¦10-12å°æ—¶

---

## 6. å¦‚ä½•å¼€å§‹é¢„æµ‹

### 6.1 ä½¿ç”¨è®­ç»ƒçš„Checkpointé¢„æµ‹

#### å‡†å¤‡å·¥ä½œ

```bash
cd /home/ubuntu/Sa2VA

# ç¡®ä¿æœ‰è®­ç»ƒå¥½çš„checkpoint
ls work_dirs/vessel_segmentation/iter_12192.pth
```

#### æ–¹æ³•1: ä½¿ç”¨HuggingFaceæ ¼å¼æ¨¡å‹ï¼ˆæ¨èï¼‰

é¦–å…ˆè½¬æ¢checkpointä¸ºHFæ ¼å¼ï¼š

```bash
# è½¬æ¢checkpoint
python tools/convert_to_hf.py \
    projects/sa2va/configs/sa2va_vessel_finetune.py \
    --pth-model work_dirs/vessel_segmentation/iter_12192.pth \
    --save-path models/sa2va_vessel_hf
```

ç„¶åä½¿ç”¨HFæ¨¡å‹é¢„æµ‹ï¼š

```python
# predict_hf.py
import torch
from transformers import AutoModel, AutoTokenizer
from PIL import Image
import numpy as np

# åŠ è½½æ¨¡å‹
model_path = "models/sa2va_vessel_hf"
model = AutoModel.from_pretrained(
    model_path,
    torch_dtype=torch.bfloat16,
    trust_remote_code=True
).cuda().eval()

tokenizer = AutoTokenizer.from_pretrained(
    model_path,
    trust_remote_code=True
)

# åŠ è½½å›¾åƒ
image_path = "Segment_DATA_Merged_512/images/sample.jpg"
image = Image.open(image_path).convert('RGB')

# æ„å»ºå¯¹è¯
question = "Please segment the blood vessels."
conversation = [
    {
        "role": "user",
        "content": f"<image>\n{question}"
    }
]

# é¢„æµ‹
with torch.no_grad():
    response, masks = model.chat(
        image=image,
        msgs=conversation,
        tokenizer=tokenizer,
        return_masks=True
    )

# ä¿å­˜ç»“æœ
if masks is not None:
    for i, mask in enumerate(masks):
        mask_img = Image.fromarray((mask * 255).astype(np.uint8))
        mask_img.save(f"output_mask_{i}.png")

print(f"Response: {response}")
print(f"Saved {len(masks)} masks")
```

è¿è¡Œï¼š

```bash
python predict_hf.py
```

#### æ–¹æ³•2: ä½¿ç”¨é¢„æµ‹è„šæœ¬

```bash
# å•å¼ å›¾ç‰‡é¢„æµ‹
python demo/predict-img.py \
    --model_path models/sa2va_vessel_hf \
    --image_path Segment_DATA_Merged_512/images/sample.jpg \
    --output_dir predictions \
    --text "Please segment the blood vessels."

# æ‰¹é‡é¢„æµ‹
python predict_5_videos.py  # ä½¿ç”¨ç°æœ‰è„šæœ¬
```

### 6.2 æ‰¹é‡é¢„æµ‹è„šæœ¬

åˆ›å»º `batch_predict.py`:

```python
"""
æ‰¹é‡é¢„æµ‹OCTè¡€ç®¡åˆ†å‰²
"""
import os
import json
import torch
from transformers import AutoModel, AutoTokenizer
from PIL import Image
import numpy as np
from tqdm import tqdm

# é…ç½®
MODEL_PATH = "models/sa2va_vessel_hf"
DATA_ROOT = "Segment_DATA_Merged_512"
OUTPUT_DIR = "predictions_batch"

os.makedirs(OUTPUT_DIR, exist_ok=True)

# åŠ è½½æ¨¡å‹
print("åŠ è½½æ¨¡å‹...")
model = AutoModel.from_pretrained(
    MODEL_PATH,
    torch_dtype=torch.bfloat16,
    trust_remote_code=True
).cuda().eval()

tokenizer = AutoTokenizer.from_pretrained(
    MODEL_PATH,
    trust_remote_code=True
)

# åŠ è½½æ•°æ®é›†
with open(os.path.join(DATA_ROOT, "annotations.json")) as f:
    dataset = json.load(f)

print(f"æ•°æ®é›†å¤§å°: {len(dataset)}")

# æ‰¹é‡é¢„æµ‹
results = []
for idx, sample in enumerate(tqdm(dataset)):
    image_path = os.path.join(DATA_ROOT, sample['image'])
    image = Image.open(image_path).convert('RGB')
    
    # é¢„æµ‹
    conversation = [{
        "role": "user",
        "content": "<image>\nPlease segment the blood vessels."
    }]
    
    with torch.no_grad():
        response, masks = model.chat(
            image=image,
            msgs=conversation,
            tokenizer=tokenizer,
            return_masks=True
        )
    
    # ä¿å­˜mask
    if masks is not None and len(masks) > 0:
        mask_path = os.path.join(OUTPUT_DIR, f"mask_{idx:04d}.png")
        mask_img = Image.fromarray((masks[0] * 255).astype(np.uint8))
        mask_img.save(mask_path)
        
        results.append({
            "image": sample['image'],
            "prediction": mask_path,
            "response": response
        })

# ä¿å­˜ç»“æœ
with open(os.path.join(OUTPUT_DIR, "results.json"), 'w') as f:
    json.dump(results, f, indent=2)

print(f"âœ… é¢„æµ‹å®Œæˆï¼ä¿å­˜åœ¨ {OUTPUT_DIR}")
```

è¿è¡Œï¼š

```bash
python batch_predict.py
```

### 6.3 ä»HuggingFaceä¸‹è½½æ¨¡å‹é¢„æµ‹

å¦‚æœæ¨¡å‹å·²ä¸Šä¼ åˆ°HuggingFaceï¼š

```python
from transformers import AutoModel, AutoTokenizer
from PIL import Image

# ç›´æ¥ä»HFä¸‹è½½
model = AutoModel.from_pretrained(
    "ly17/sa2va-vessel-hf",
    torch_dtype=torch.bfloat16,
    trust_remote_code=True
).cuda()

tokenizer = AutoTokenizer.from_pretrained(
    "ly17/sa2va-vessel-hf",
    trust_remote_code=True
)

# é¢„æµ‹
image = Image.open("test.jpg")
response, masks = model.chat(
    image=image,
    msgs=[{"role": "user", "content": "<image>\nSegment vessels."}],
    tokenizer=tokenizer,
    return_masks=True
)
```

### 6.4 é¢„æµ‹å‚æ•°è°ƒæ•´

#### æ¸©åº¦é‡‡æ ·

```python
# æ›´ç¡®å®šçš„è¾“å‡º
response = model.chat(
    image=image,
    msgs=conversation,
    tokenizer=tokenizer,
    temperature=0.1,  # é™ä½éšæœºæ€§
    top_p=0.9
)
```

#### Maskåå¤„ç†

```python
import cv2

# äºŒå€¼åŒ–
mask_binary = (masks[0] > 0.5).astype(np.uint8) * 255

# å½¢æ€å­¦æ“ä½œ
kernel = np.ones((3, 3), np.uint8)
mask_clean = cv2.morphologyEx(mask_binary, cv2.MORPH_CLOSE, kernel)
mask_clean = cv2.morphologyEx(mask_clean, cv2.MORPH_OPEN, kernel)
```

### 6.5 å¯è§†åŒ–é¢„æµ‹ç»“æœ

```python
import matplotlib.pyplot as plt

def visualize_prediction(image, mask, save_path):
    """å¯è§†åŒ–é¢„æµ‹ç»“æœ"""
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))
    
    # åŸå›¾
    axes[0].imshow(image)
    axes[0].set_title('Original Image')
    axes[0].axis('off')
    
    # Mask
    axes[1].imshow(mask, cmap='gray')
    axes[1].set_title('Prediction Mask')
    axes[1].axis('off')
    
    # å åŠ 
    axes[2].imshow(image)
    axes[2].imshow(mask, alpha=0.5, cmap='Reds')
    axes[2].set_title('Overlay')
    axes[2].axis('off')
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.close()

# ä½¿ç”¨
visualize_prediction(image, masks[0], "visualization.png")
```

---

## 7. æ¨¡å‹è½¬æ¢

### 7.1 Checkpointè½¬HuggingFaceæ ¼å¼

```bash
# åŸºæœ¬è½¬æ¢
python tools/convert_to_hf.py \
    projects/sa2va/configs/sa2va_vessel_finetune.py \
    --pth-model work_dirs/vessel_segmentation/iter_12192.pth \
    --save-path models/sa2va_vessel_hf

# æŒ‡å®šé…ç½®
python tools/convert_to_hf.py \
    projects/sa2va/configs/sa2va_vessel_finetune.py \
    --pth-model work_dirs/vessel_segmentation/iter_12192.pth \
    --save-path models/sa2va_vessel_hf \
    --model-name "Sa2VA-Vessel-8B"
```

### 7.2 éªŒè¯è½¬æ¢

```python
# verify_conversion.py
from transformers import AutoModel, AutoTokenizer

model_path = "models/sa2va_vessel_hf"

# åŠ è½½æ¨¡å‹
model = AutoModel.from_pretrained(
    model_path,
    trust_remote_code=True
)

tokenizer = AutoTokenizer.from_pretrained(
    model_path,
    trust_remote_code=True
)

print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
print(f"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()) / 1e9:.2f}B")
print(f"Tokenizerè¯è¡¨å¤§å°: {len(tokenizer)}")
```

### 7.3 ä¸Šä¼ åˆ°HuggingFace

```bash
# ç™»å½•
huggingface-cli login

# ä¸Šä¼ 
huggingface-cli upload \
    ly17/sa2va-vessel-hf \
    models/sa2va_vessel_hf \
    . \
    --repo-type model \
    --commit-message "Upload Sa2VA vessel segmentation model"
```

---

## 8. å¸¸è§é—®é¢˜

### 8.1 è®­ç»ƒç›¸å…³

#### Q1: æ˜¾å­˜ä¸è¶³æ€ä¹ˆåŠï¼Ÿ

**A**: 
```python
# å‡å°batch size
batch_size = 1
accumulative_counts = 16  # å¢åŠ æ¢¯åº¦ç´¯ç§¯

# å‡å°åºåˆ—é•¿åº¦
max_length = 2048

# ä½¿ç”¨DeepSpeed ZeRO-3
# å·²åœ¨é…ç½®æ–‡ä»¶ä¸­å¯ç”¨

# å‡å°‘LoRA rank
r = 32
lora_alpha = 64
```

#### Q2: è®­ç»ƒé€Ÿåº¦å¤ªæ…¢ï¼Ÿ

**A**:
```python
# å‡å°‘dataloader workers
dataloader_num_workers = 2

# å‡å°‘æ•°æ®é‡å¤
repeats = 5  # ä»10æ”¹ä¸º5

# ä½¿ç”¨æ›´å°‘çš„å¡ä½†å¢åŠ accumulation
CUDA_VISIBLE_DEVICES=0,1  # 2å¡
accumulative_counts = 16   # ä¿æŒæœ‰æ•ˆbatch=32
```

#### Q3: å¦‚ä½•ä»checkpointç»§ç»­è®­ç»ƒï¼Ÿ

**A**:
```bash
xtuner train \
    projects/sa2va/configs/sa2va_vessel_finetune.py \
    --work-dir work_dirs/vessel_segmentation \
    --resume work_dirs/vessel_segmentation/iter_5000.pth
```

#### Q4: è®­ç»ƒlossä¸ä¸‹é™ï¼Ÿ

**A**:
1. æ£€æŸ¥å­¦ä¹ ç‡æ˜¯å¦å¤ªå¤§æˆ–å¤ªå°
2. æ£€æŸ¥æ•°æ®æ˜¯å¦æ­£ç¡®åŠ è½½
3. å¢åŠ warmupæ­¥æ•°
4. æ£€æŸ¥æ¢¯åº¦è£å‰ªå‚æ•°

### 8.2 é¢„æµ‹ç›¸å…³

#### Q5: é¢„æµ‹ç»“æœä¸ç†æƒ³ï¼Ÿ

**A**:
1. æ£€æŸ¥ä½¿ç”¨çš„checkpointæ˜¯å¦æ­£ç¡®
2. å°è¯•ä¸åŒçš„temperatureå‚æ•°
3. ä½¿ç”¨æ›´å¤šè®­ç»ƒæ­¥æ•°çš„checkpoint
4. å¯¹maskè¿›è¡Œåå¤„ç†

#### Q6: å¦‚ä½•åŠ é€Ÿæ¨ç†ï¼Ÿ

**A**:
```python
# ä½¿ç”¨åŠç²¾åº¦
model = model.to(torch.bfloat16)

# ä½¿ç”¨torch.compile (PyTorch 2.0+)
model = torch.compile(model)

# æ‰¹é‡æ¨ç†
# ä¸€æ¬¡å¤„ç†å¤šå¼ å›¾ç‰‡
```

#### Q7: å†…å­˜å ç”¨å¤ªå¤§ï¼Ÿ

**A**:
```python
# æ¸…ç†GPUç¼“å­˜
torch.cuda.empty_cache()

# ä½¿ç”¨æ›´å°çš„å›¾åƒå°ºå¯¸
# åœ¨é…ç½®ä¸­ä¿®æ”¹target_length

# åŠæ—¶é‡Šæ”¾ä¸éœ€è¦çš„å˜é‡
del intermediate_results
torch.cuda.empty_cache()
```

### 8.3 æ•°æ®ç›¸å…³

#### Q8: å¦‚ä½•å‡†å¤‡è‡ªå·±çš„æ•°æ®ï¼Ÿ

**A**: å‚è€ƒ `Segment_DATA_Merged_512/annotations.json` æ ¼å¼ï¼š
```json
[
  {
    "image": "images/your_image.jpg",
    "mask": [
      {
        "segmentation": [[x1, y1, x2, y2, ...]],
        "category_id": 1
      }
    ],
    "conversations": [
      {"from": "human", "value": "<image>\næè¿°ä½ çš„ä»»åŠ¡"},
      {"from": "gpt", "value": "<p>ç›®æ ‡</p><vp>[[åæ ‡]]</vp>[SEG]"}
    ]
  }
]
```

#### Q9: æ•°æ®å¢å¼ºå¦‚ä½•é…ç½®ï¼Ÿ

**A**: åœ¨é…ç½®æ–‡ä»¶ä¸­ä¿®æ”¹ï¼š
```python
extra_image_processor = dict(
    type=DirectResize,
    target_length=1024,
    # å¯æ·»åŠ å…¶ä»–å¢å¼º
)
```

### 8.4 ç¯å¢ƒç›¸å…³

#### Q10: CUDAç‰ˆæœ¬ä¸åŒ¹é…ï¼Ÿ

**A**:
```bash
# æ£€æŸ¥CUDAç‰ˆæœ¬
nvcc --version
nvidia-smi

# å®‰è£…å¯¹åº”ç‰ˆæœ¬çš„PyTorch
# CUDA 11.8
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118

# CUDA 12.1
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121
```

#### Q11: DeepSpeedå®‰è£…å¤±è´¥ï¼Ÿ

**A**:
```bash
# æ–¹æ³•1: ä½¿ç”¨é¢„ç¼–è¯‘ç‰ˆæœ¬
pip install deepspeed --no-build-isolation

# æ–¹æ³•2: ä»æºç å®‰è£…
DS_BUILD_OPS=1 pip install deepspeed

# æ–¹æ³•3: ä½¿ç”¨conda
conda install -c conda-forge deepspeed
```

---

## ğŸ“š é™„å½•

### A. å®Œæ•´å‘½ä»¤é€ŸæŸ¥

```bash
# ç¯å¢ƒæ¿€æ´»
micromamba activate sa2va

# è®­ç»ƒï¼ˆ4å¡ï¼‰
CUDA_VISIBLE_DEVICES=0,1,2,3 xtuner train \
    projects/sa2va/configs/sa2va_vessel_finetune.py \
    --work-dir work_dirs/vessel_seg

# è½¬æ¢æ¨¡å‹
python tools/convert_to_hf.py \
    projects/sa2va/configs/sa2va_vessel_finetune.py \
    --pth-model work_dirs/vessel_seg/iter_12192.pth \
    --save-path models/sa2va_vessel_hf

# é¢„æµ‹
python predict_hf.py

# ä¸Šä¼ åˆ°HF
huggingface-cli upload ly17/sa2va-vessel-hf models/sa2va_vessel_hf .
```

### B. ç›®å½•ç»“æ„

```
Sa2VA/
â”œâ”€â”€ projects/sa2va/
â”‚   â”œâ”€â”€ configs/
â”‚   â”‚   â””â”€â”€ sa2va_vessel_finetune.py  â† è®­ç»ƒé…ç½®
â”‚   â”œâ”€â”€ models/                        â† æ¨¡å‹å®šä¹‰
â”‚   â””â”€â”€ datasets/                      â† æ•°æ®åŠ è½½
â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ train.py                       â† è®­ç»ƒå…¥å£
â”‚   â””â”€â”€ convert_to_hf.py              â† æ¨¡å‹è½¬æ¢
â”œâ”€â”€ Segment_DATA_Merged_512/           â† æ•°æ®é›†
â”‚   â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ masks/
â”‚   â””â”€â”€ annotations.json
â”œâ”€â”€ work_dirs/                         â† è®­ç»ƒè¾“å‡º
â”‚   â””â”€â”€ vessel_segmentation/
â”‚       â””â”€â”€ iter_12192.pth
â””â”€â”€ models/                            â† è½¬æ¢åçš„æ¨¡å‹
    â””â”€â”€ sa2va_vessel_hf/
```

### C. ç›¸å…³é“¾æ¥

- **Sa2VAå®˜æ–¹**: https://github.com/magic-research/Sa2VA
- **InternVL**: https://huggingface.co/OpenGVLab/InternVL3-8B
- **SAM2**: https://github.com/facebookresearch/sam2
- **XTuner**: https://github.com/InternLM/xtuner
- **DeepSpeed**: https://www.deepspeed.ai/

### D. å¼•ç”¨

å¦‚æœä½¿ç”¨æœ¬é¡¹ç›®ï¼Œè¯·å¼•ç”¨ï¼š

```bibtex
@article{sa2va2025,
  title={Sa2VA: Marrying SAM2 with LLaVA for Dense Grounded Understanding},
  author={Yuan, Haobo and Li, Xiangtai and Zhang, Tao and others},
  journal={arXiv preprint arXiv:2501.04001},
  year={2025}
}
```

---

**æ–‡æ¡£ç‰ˆæœ¬**: 1.0  
**åˆ›å»ºæ—¥æœŸ**: 2025-11-28  
**é€‚ç”¨æ¨¡å‹**: Sa2VA-InternVL3-8B  
**åº”ç”¨åœºæ™¯**: OCTè¡€ç®¡åˆ†å‰²

**è·å–å¸®åŠ©**:
- GitHub Issues: https://github.com/qimingfan10/RLSa2va/issues
- HuggingFace: https://huggingface.co/ly17/sa2va-vessel-hf

**ä¸‹ä¸€æ­¥**: 
1. âœ… å‡†å¤‡ç¯å¢ƒå’Œæ•°æ®
2. âœ… å¼€å§‹è®­ç»ƒ
3. âœ… è½¬æ¢å’Œæµ‹è¯•æ¨¡å‹
4. âœ… ä¸Šä¼ åˆ†äº«æˆæœ
