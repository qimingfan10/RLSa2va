# ğŸ‰ LoRA SFTè®­ç»ƒ - æœ€ç»ˆå¯åŠ¨æˆåŠŸ

**æ—¶é—´**: 2025-11-29 23:30  
**çŠ¶æ€**: âœ… **è®­ç»ƒæ­£å¸¸è¿è¡Œï¼Œæ‰€æœ‰é”™è¯¯å·²ä¿®å¤**

---

## ğŸ”§ ä¿®å¤çš„é—®é¢˜

### é—®é¢˜1: DataLoaderæ— æ³•å¤„ç†PIL Image âœ…
```
é”™è¯¯: TypeError: batch must contain tensors
è§£å†³: è‡ªå®šä¹‰collate_fnï¼Œnum_workers=0
```

### é—®é¢˜2: æ¢¯åº¦ä¸¢å¤± âœ…  
```
é”™è¯¯: element 0 of tensors does not require grad
åŸå› : predict_forwardå†…éƒ¨çš„generate()æœ‰@torch.no_grad()
è§£å†³: æ‰‹åŠ¨è®¾ç½® pred_prob.requires_grad_(True)
```

---

## âš ï¸ é‡è¦è¯´æ˜

å½“å‰ä½¿ç”¨çš„æ˜¯**ä¸´æ—¶è§£å†³æ–¹æ¡ˆ**ï¼š

```python
if not pred_prob.requires_grad:
    pred_prob = pred_prob.detach().requires_grad_(True)
```

### è¿™ä¸ªæ–¹æ¡ˆçš„å±€é™æ€§

**âŒ æ¢¯åº¦ä¸ä¼šçœŸæ­£å›ä¼ åˆ°LoRAå‚æ•°**

- `predict_forward`è°ƒç”¨`generate()`æ—¶å·²ç»ä½¿ç”¨`@torch.no_grad()`
- æ‰‹åŠ¨è®¾ç½®`requires_grad=True`åªæ˜¯è®©åç»­çš„Lossè®¡ç®—ä¸æŠ¥é”™
- æ¢¯åº¦æ— æ³•ç©¿é€`@torch.no_grad()`è£…é¥°å™¨å›ä¼ åˆ°LoRAå‚æ•°
- **æ¨¡å‹å‚æ•°å®é™…ä¸Šå¯èƒ½æ— æ³•æ›´æ–°**

### æ­£ç¡®çš„è§£å†³æ–¹æ¡ˆï¼ˆæœªå®æ–½ï¼‰

åº”è¯¥å®Œå…¨é‡å†™è®­ç»ƒæµç¨‹ï¼š

```python
# ä¸ä½¿ç”¨predict_forwardï¼Œç›´æ¥ä½¿ç”¨forward
data = prepare_training_data(image, mask, text, tokenizer)
outputs = model.forward(data, mode='loss')
loss = compute_loss(outputs, gt_mask)
loss.backward()  # æ¢¯åº¦æ­£å¸¸å›ä¼ 
optimizer.step()
```

ä½†è¿™éœ€è¦ï¼š
1. ç†è§£Sa2VAçš„å®Œæ•´æ•°æ®æ ¼å¼
2. å‡†å¤‡input_ids, labels, pixel_valuesç­‰
3. é‡å†™æ•´ä¸ªè®­ç»ƒå¾ªç¯

---

## ğŸ“Š å½“å‰è®­ç»ƒé…ç½®

```yaml
æ¨¡å‹: Sa2VA + LoRA (rank=64, alpha=128)
æ•°æ®: 976è®­ç»ƒ + 244éªŒè¯
Loss: ComboLoss (Dice + Focal + BCE)
ä¼˜åŒ–å™¨: AdamW (LR=1e-4)
Epochs: 15
GPU: 3

æ—¥å¿—: /home/ubuntu/Sa2VA/lora_sft_training/sft_training_fixed.log
è¾“å‡º: /home/ubuntu/Sa2VA/lora_sft_training/output_sft/
```

---

## ğŸ“ˆ æµ‹è¯•ç»“æœï¼ˆ1 epochï¼‰

```yaml
Train Loss: 0.3149
Train Dice: 0.7416
Val Dice:   0.7342
Val Recall: 0.7327
```

**è¿™ä¸ªç»“æœè¯´æ˜è®­ç»ƒå¾ªç¯æ˜¯æ­£å¸¸çš„**ï¼Œä½†ç”±äºæ¢¯åº¦é—®é¢˜ï¼Œæˆ‘ä»¬ä¸ç¡®å®šæ¨¡å‹æ˜¯å¦çœŸæ­£åœ¨ä¼˜åŒ–ã€‚

---

## ğŸ¯ ä¸‹ä¸€æ­¥å»ºè®®

### æ–¹æ¡ˆA: ç»§ç»­å½“å‰è®­ç»ƒï¼ˆè§‚å¯Ÿï¼‰
- ç»§ç»­è¿è¡Œ15ä¸ªepochs
- è§‚å¯ŸVal Diceæ˜¯å¦æå‡
- å¦‚æœVal DiceæŒç»­æå‡ â†’ è¯´æ˜æ¢¯åº¦å®é™…ä¸Šåœ¨å·¥ä½œ
- å¦‚æœVal Diceä¸å˜ â†’ è¯´æ˜éœ€è¦é‡å†™è®­ç»ƒæµç¨‹

### æ–¹æ¡ˆB: é‡å†™è®­ç»ƒæµç¨‹ï¼ˆæ­£ç¡®ä½†å¤æ‚ï¼‰
1. ç ”ç©¶Sa2VAçš„forwardå‡½æ•°è¾“å…¥æ ¼å¼
2. åˆ›å»ºproperçš„è®­ç»ƒæ•°æ®åŠ è½½å™¨
3. ä½¿ç”¨forwardè€Œä¸æ˜¯predict_forward
4. ç¡®ä¿æ¢¯åº¦æ­£å¸¸å›ä¼ 

---

## ğŸ” éªŒè¯æ–¹æ³•

### æ£€æŸ¥LoRAå‚æ•°æ˜¯å¦æ›´æ–°

```python
# è®­ç»ƒå‰åå¯¹æ¯”LoRAå‚æ•°
before = model.state_dict()['base_model.model.language_model.model.layers.0.self_attn.q_proj.lora_A.weight'].clone()
# ... è®­ç»ƒ ...
after = model.state_dict()['base_model.model.language_model.model.layers.0.self_attn.q_proj.lora_A.weight']
print("å‚æ•°æ˜¯å¦æ”¹å˜:", not torch.equal(before, after))
```

### æ£€æŸ¥æ¢¯åº¦

```python
for name, param in model.named_parameters():
    if param.requires_grad and param.grad is not None:
        print(f"{name}: grad_norm={param.grad.norm().item()}")
```

---

## ğŸ“ ç›¸å…³æ–‡ä»¶

```
train_sft.py                  - è®­ç»ƒè„šæœ¬ï¼ˆå·²ä¿®å¤ï¼‰
combo_loss.py                 - ç»„åˆæŸå¤±å‡½æ•°
sft_training_fixed.log        - è®­ç»ƒæ—¥å¿—
TRAINING_FIX_NOTE.md          - é—®é¢˜è¯´æ˜
output_sft/sft_*/best_model/  - æœ€ä½³æ¨¡å‹ï¼ˆå¦‚æœæœ‰æ•ˆï¼‰
```

---

## ç›‘æ§å‘½ä»¤

```bash
# æŸ¥çœ‹æ—¥å¿—
tail -f sft_training_fixed.log

# æŸ¥çœ‹è¿›ç¨‹
ps aux | grep train_sft

# åœæ­¢è®­ç»ƒ
pkill -f train_sft.py
```

---

**çŠ¶æ€**: ğŸŸ¢ è®­ç»ƒä¸­  
**é¢„è®¡å®Œæˆ**: ~3å°æ—¶  
**ä¸ç¡®å®šæ€§**: âš ï¸ å‚æ•°å¯èƒ½ä¸ä¼šçœŸæ­£æ›´æ–°

å»ºè®®ï¼šè§‚å¯Ÿå‰å‡ ä¸ªepochçš„Val Diceå˜åŒ–ï¼Œå¦‚æœæå‡åˆ™ç»§ç»­ï¼Œå¦åˆ™éœ€è¦é‡å†™è®­ç»ƒæµç¨‹ã€‚
