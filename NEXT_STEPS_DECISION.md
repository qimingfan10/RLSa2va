# ğŸ¯ ä¸‹ä¸€æ­¥å†³ç­–æŒ‡å—

**å½“å‰çŠ¶æ€**: æ‰€æœ‰å®éªŒå·²å®Œæˆè®­ç»ƒ  
**æœ€ä½³ç»“æœ**: LoRA+PPO Fullæ¨¡å¼ - Dice 0.7889, Recall 0.7617  
**ä¸ç›®æ ‡å·®è·**: Dice -7.2%, Recall -10.4%

---

## ğŸš¦ ç«‹å³è¡ŒåŠ¨ï¼šè¯„ä¼°å®éªŒä¸€

**å®éªŒä¸€ç›®å‰çŠ¶æ€**:
- âœ… è®­ç»ƒå®Œæˆï¼ˆ5000 timestepsï¼‰
- â“ æœªè¯„ä¼°Dice/RecallæŒ‡æ ‡
- ğŸ’¡ å¯èƒ½å·²ç»æœ‰ä¸é”™çš„æ•ˆæœ

**è¯„ä¼°å‘½ä»¤**:
```bash
cd /home/ubuntu/Sa2VA/rl_prompt_optimization

# è¯„ä¼°æœ€ä¼˜ç­–ç•¥
python evaluate_rl_prompt.py \
    --model_path outputs/rl_prompt_20251129_154906/final_model \
    --sa2va_model /home/ubuntu/Sa2VA/models/sa2va_vessel_hf \
    --test_data /home/ubuntu/Sa2VA/data/merged_vessel_data \
    --num_samples 100 \
    --output_dir evaluation_results
```

**é¢„æœŸç»“æœ**:
- å¦‚æœDice â‰¥ 0.82: å®éªŒä¸€æœ‰æ•ˆï¼Œå¯èƒ½ç»“åˆä½¿ç”¨
- å¦‚æœDice < 0.80: æ•ˆæœä¸ä½³ï¼Œä¸“æ³¨LoRA+PPOä¼˜åŒ–

---

## ğŸ”„ æ–¹æ¡ˆA: ä¼˜åŒ–LoRA+PPOï¼ˆæ¨èï¼‰ â­â­â­â­â­

### é…ç½®æ”¹è¿›

åˆ›å»ºä¼˜åŒ–é…ç½®æ–‡ä»¶ `/home/ubuntu/Sa2VA/lora_ppo_training/run_lora_ppo_v2.sh`:

```bash
#!/bin/bash

echo "========================================"
echo "LoRA + PPO ä¼˜åŒ–ç‰ˆè®­ç»ƒ"
echo "ç‰ˆæœ¬2: æ›´å¤šæ•°æ® + æ›´é«˜å­¦ä¹ ç‡ + æ›´å¤§rank"
echo "========================================"

MODEL_PATH="/home/ubuntu/Sa2VA/models/sa2va_vessel_hf"
DATA_ROOT="/home/ubuntu/Sa2VA/data/merged_vessel_data"
OUTPUT_DIR="/home/ubuntu/Sa2VA/lora_ppo_training/output_v2"
GPU=1

# ä¼˜åŒ–åçš„å‚æ•°
LORA_RANK=64          # 32 â†’ 64
LORA_ALPHA=128        # 64 â†’ 128
MAX_TRAIN_SAMPLES=1220  # 1000 â†’ 1220ï¼ˆå…¨éƒ¨æ•°æ®ï¼‰
MAX_VAL_SAMPLES=100
NUM_EPOCHS=10         # 3 â†’ 10
LEARNING_RATE=1e-4    # 5e-5 â†’ 1e-4

# è°ƒæ•´å¥–åŠ±æƒé‡ï¼ˆå¼ºåŒ–Recallï¼‰
REWARD_TYPE="multi_objective"
DICE_WEIGHT=0.4       # 0.5 â†’ 0.4
RECALL_WEIGHT=0.4     # 0.2 â†’ 0.4
TOPOLOGY_WEIGHT=0.15  # 0.2 â†’ 0.15
LENGTH_WEIGHT=0.05    # 0.1 â†’ 0.05
RECALL_TARGET=0.85

echo "ä¼˜åŒ–é…ç½®:"
echo "  LoRA Rank: $LORA_RANK (æå‡)"
echo "  å­¦ä¹ ç‡: $LEARNING_RATE (æå‡)"
echo "  è®­ç»ƒæ ·æœ¬: $MAX_TRAIN_SAMPLES (å¢åŠ )"
echo "  è®­ç»ƒè½®æ•°: $NUM_EPOCHS (å¢åŠ )"
echo "  Recallæƒé‡: $RECALL_WEIGHT (å¼ºåŒ–)"
echo ""

mkdir -p $OUTPUT_DIR
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
LOG_FILE="$OUTPUT_DIR/train_v2_${TIMESTAMP}.log"

echo "å¼€å§‹è®­ç»ƒ..."
echo "æ—¥å¿—æ–‡ä»¶: $LOG_FILE"

CUDA_VISIBLE_DEVICES=$GPU python3 /home/ubuntu/Sa2VA/lora_ppo_training/train_lora_ppo.py \
    --model_path $MODEL_PATH \
    --data_root $DATA_ROOT \
    --output_dir $OUTPUT_DIR \
    --lora_rank $LORA_RANK \
    --lora_alpha $LORA_ALPHA \
    --num_epochs $NUM_EPOCHS \
    --learning_rate $LEARNING_RATE \
    --max_train_samples $MAX_TRAIN_SAMPLES \
    --max_val_samples $MAX_VAL_SAMPLES \
    --reward_type $REWARD_TYPE \
    --dice_weight $DICE_WEIGHT \
    --recall_weight $RECALL_WEIGHT \
    --topology_weight $TOPOLOGY_WEIGHT \
    --length_weight $LENGTH_WEIGHT \
    --recall_target $RECALL_TARGET \
    --prompt "Please segment the blood vessel." \
    --gpu 0 \
    --num_workers 4 \
    --val_freq 100 \
    --save_freq 2 \
    --log_freq 10 \
    2>&1 | tee $LOG_FILE

echo ""
echo "è®­ç»ƒå®Œæˆï¼"
echo "æ—¥å¿—æ–‡ä»¶: $LOG_FILE"
```

### å¯åŠ¨å‘½ä»¤

```bash
cd /home/ubuntu/Sa2VA/lora_ppo_training

# åˆ›å»ºv2è„šæœ¬
cat > run_lora_ppo_v2.sh << 'EOF'
# (ä¸Šé¢çš„è„šæœ¬å†…å®¹)
EOF

chmod +x run_lora_ppo_v2.sh

# åå°è¿è¡Œ
nohup bash run_lora_ppo_v2.sh > lora_ppo_v2.log 2>&1 &

# æŸ¥çœ‹PID
echo $! > lora_ppo_v2.pid
```

### ç›‘æ§å‘½ä»¤

```bash
# å®æ—¶æ—¥å¿—
tail -f lora_ppo_v2.log

# GPUçŠ¶æ€
watch -n 2 nvidia-smi

# è¿›ç¨‹çŠ¶æ€
ps aux | grep train_lora_ppo

# åœæ­¢è®­ç»ƒï¼ˆå¦‚æœéœ€è¦ï¼‰
kill $(cat lora_ppo_v2.pid)
```

### é¢„æœŸæ•ˆæœ

```yaml
è®­ç»ƒæ—¶é—´: 10-15å°æ—¶
é¢„æœŸDice: 0.84-0.86
é¢„æœŸRecall: 0.82-0.84
æˆåŠŸæ¦‚ç‡: 85%
```

---

## ğŸ”„ æ–¹æ¡ˆB: Curriculum Learning

### å®ç°æ­¥éª¤

1. **æ ·æœ¬æ’åº**
```python
# åˆ›å»º prepare_curriculum_data.py
import json
import numpy as np
from PIL import Image

# æŒ‰è¡€ç®¡é¢ç§¯æ’åº
samples_with_area = []
for sample in annotations:
    mask = generate_mask(sample)
    area = mask.sum()
    samples_with_area.append((sample, area))

# ä»å¤§åˆ°å°æ’åºï¼ˆç®€å•åˆ°å›°éš¾ï¼‰
samples_with_area.sort(key=lambda x: x[1], reverse=True)

# åˆ†ä¸‰ä¸ªé˜¶æ®µ
easy = samples_with_area[:400]      # å¤§è¡€ç®¡
medium = samples_with_area[400:800] # ä¸­ç­‰
hard = samples_with_area[800:]      # ç»†å°è¡€ç®¡
```

2. **åˆ†é˜¶æ®µè®­ç»ƒ**
```bash
# Stage 1: å¤§è¡€ç®¡ï¼ˆ400å¼ ï¼Œ3 epochsï¼‰
python train_lora_ppo.py --max_train_samples 400 --num_epochs 3

# Stage 2: ä¸­ç­‰è¡€ç®¡ï¼ˆ800å¼ ï¼Œ3 epochsï¼‰
python train_lora_ppo.py --max_train_samples 800 --num_epochs 3 \
    --resume_from stage1_best_model

# Stage 3: æ‰€æœ‰è¡€ç®¡ï¼ˆ1220å¼ ï¼Œ4 epochsï¼‰
python train_lora_ppo.py --max_train_samples 1220 --num_epochs 4 \
    --resume_from stage2_best_model
```

### é¢„æœŸæ•ˆæœ

```yaml
ä¼˜åŠ¿: é€æ­¥å¢åŠ éš¾åº¦ï¼Œæ›´ç¨³å®šæ”¶æ•›
é¢„æœŸDice: 0.85-0.87
é¢„æœŸRecall: 0.83-0.85
æˆåŠŸæ¦‚ç‡: 70%
è®­ç»ƒæ—¶é—´: 15-20å°æ—¶
```

---

## ğŸ”„ æ–¹æ¡ˆC: åŠ¨æ€å¥–åŠ±æƒé‡

### å®ç°æ–¹å¼

ä¿®æ”¹`reward_functions.py`ï¼Œæ ¹æ®å½“å‰æ€§èƒ½åŠ¨æ€è°ƒæ•´æƒé‡ï¼š

```python
class AdaptiveMultiObjectiveReward:
    def __init__(self, target_recall=0.85):
        self.target_recall = target_recall
        self.current_avg_recall = 0.75  # åˆå§‹å€¼
        
    def __call__(self, pred_mask, gt_mask):
        # ... è®¡ç®—å„é¡¹æŒ‡æ ‡
        
        # åŠ¨æ€è°ƒæ•´æƒé‡
        if self.current_avg_recall < self.target_recall - 0.05:
            # Recallå¤ªä½ï¼Œå¤§å¹…å¢åŠ æƒé‡
            recall_weight = 0.5
            dice_weight = 0.3
        elif self.current_avg_recall < self.target_recall:
            # æ¥è¿‘ç›®æ ‡ï¼Œé€‚åº¦å¢åŠ 
            recall_weight = 0.4
            dice_weight = 0.4
        else:
            # å·²è¾¾æ ‡ï¼Œæ¢å¤å¹³è¡¡
            recall_weight = 0.2
            dice_weight = 0.5
        
        # è®¡ç®—æ€»å¥–åŠ±
        total_reward = (
            dice_weight * dice_reward +
            recall_weight * recall_reward +
            0.2 * topology_reward +
            0.1 * length_penalty
        )
        
        return total_reward, reward_dict
```

---

## ğŸ“Š å†³ç­–æ ‘

```
å¼€å§‹
  â”‚
  â”œâ”€â†’ è¯„ä¼°å®éªŒä¸€
  â”‚     â”‚
  â”‚     â”œâ”€â†’ Dice â‰¥ 0.82? 
  â”‚     â”‚     â”œâ”€â†’ YES: ç»“åˆPrompt+LoRA
  â”‚     â”‚     â””â”€â†’ NO: ç»§ç»­LoRAä¼˜åŒ–
  â”‚     â”‚
  â”‚     â””â”€â†’ æ—¶é—´: 1å°æ—¶
  â”‚
  â”œâ”€â†’ æ–¹æ¡ˆA: ä¼˜åŒ–LoRA+PPO â­â­â­â­â­
  â”‚     â”œâ”€â†’ è°ƒæ•´è¶…å‚æ•°
  â”‚     â”œâ”€â†’ å¢åŠ æ•°æ®å’Œè½®æ•°
  â”‚     â”œâ”€â†’ å¼ºåŒ–Recallæƒé‡
  â”‚     â””â”€â†’ æ—¶é—´: 10-15å°æ—¶
  â”‚          â”‚
  â”‚          â”œâ”€â†’ æˆåŠŸï¼ˆDice 0.84+ï¼‰: å®Œæˆ
  â”‚          â””â”€â†’ æœªè¾¾æ ‡: æ–¹æ¡ˆBæˆ–C
  â”‚
  â”œâ”€â†’ æ–¹æ¡ˆB: Curriculum Learning
  â”‚     â”œâ”€â†’ å®ç°åˆ†é˜¶æ®µè®­ç»ƒ
  â”‚     â”œâ”€â†’ é€æ­¥å¢åŠ éš¾åº¦
  â”‚     â””â”€â†’ æ—¶é—´: 15-20å°æ—¶
  â”‚
  â””â”€â†’ æ–¹æ¡ˆC: åŠ¨æ€å¥–åŠ±æƒé‡
        â”œâ”€â†’ ä¿®æ”¹å¥–åŠ±å‡½æ•°
        â”œâ”€â†’ è‡ªé€‚åº”æƒé‡è°ƒæ•´
        â””â”€â†’ æ—¶é—´: 12-18å°æ—¶
```

---

## ğŸ¯ æ¨èæ‰§è¡Œé¡ºåº

### ä»Šå¤©ï¼ˆ2025-11-29æ™šä¸Šï¼‰

#### 1. è¯„ä¼°å®éªŒä¸€ï¼ˆ1å°æ—¶ï¼‰
```bash
cd /home/ubuntu/Sa2VA/rl_prompt_optimization
# è¿è¡Œè¯„ä¼°è„šæœ¬ï¼ˆéœ€è¦å…ˆåˆ›å»ºï¼‰
```

#### 2. å¯åŠ¨æ–¹æ¡ˆAï¼ˆç«‹å³åå°è¿è¡Œï¼‰
```bash
cd /home/ubuntu/Sa2VA/lora_ppo_training
nohup bash run_lora_ppo_v2.sh > lora_ppo_v2.log 2>&1 &
```

### æ˜å¤©ï¼ˆ2025-11-30ï¼‰

#### ä¸Šåˆ
- æ£€æŸ¥æ–¹æ¡ˆAè®­ç»ƒè¿›åº¦
- æŸ¥çœ‹TensorBoardæ›²çº¿
- è¯„ä¼°å‰å‡ ä¸ªepochçš„æ•ˆæœ

#### ä¸‹åˆ
- å¦‚æœæ•ˆæœå¥½ï¼Œç­‰å¾…å®Œæˆ
- å¦‚æœæ•ˆæœä¸ä½³ï¼Œå‡†å¤‡æ–¹æ¡ˆBæˆ–C

### åå¤©ï¼ˆ2025-12-01ï¼‰

- è¯„ä¼°æœ€ç»ˆç»“æœ
- æ’°å†™å®Œæ•´æŠ¥å‘Š
- éƒ¨ç½²æœ€ä¼˜æ¨¡å‹

---

## ğŸ’¡ å…³é”®æŠ€å·§

### 1. å¦‚ä½•åˆ¤æ–­è®­ç»ƒæ˜¯å¦æœ‰æ•ˆï¼Ÿ

è§‚å¯Ÿå‰3ä¸ªepochï¼š
```yaml
æœ‰æ•ˆçš„ä¿¡å·:
  - DiceæŒç»­ä¸Šå‡ï¼ˆè‡³å°‘+0.01/epochï¼‰
  - Recallä¸Šå‡å¹…åº¦ > Dice
  - Lossç¨³å®šä¸‹é™

æ— æ•ˆçš„ä¿¡å·:
  - Diceå‡ ä¹ä¸å˜ï¼ˆ<0.005/epochï¼‰
  - Recallä¸ä¸Šå‡æˆ–ä¸‹é™
  - Losséœ‡è¡æˆ–ä¸ä¸‹é™

å¦‚æœæ— æ•ˆ:
  - åœæ­¢è®­ç»ƒ
  - è°ƒæ•´å­¦ä¹ ç‡æˆ–å¥–åŠ±æƒé‡
  - é‡æ–°å¯åŠ¨
```

### 2. å¦‚ä½•å¿«é€Ÿæµ‹è¯•è¶…å‚æ•°ï¼Ÿ

ä½¿ç”¨MiniéªŒè¯ï¼š
```bash
# 100å¼ å›¾åƒï¼Œ1 epochï¼Œå¿«é€ŸéªŒè¯è¶…å‚æ•°
python train_lora_ppo.py \
    --max_train_samples 100 \
    --num_epochs 1 \
    --learning_rate 1e-4  # æµ‹è¯•æ–°å­¦ä¹ ç‡
```

å¦‚æœ1 epochåDiceæå‡æ˜æ˜¾ â†’ è¶…å‚æ•°æœ‰æ•ˆ â†’ å®Œæ•´è®­ç»ƒ

### 3. å¦‚ä½•é¿å…è¿‡æ‹Ÿåˆï¼Ÿ

```yaml
ç­–ç•¥:
  - ä½¿ç”¨æ•°æ®å¢å¼º
  - Early stoppingï¼ˆéªŒè¯Diceä¸å†æå‡æ—¶åœæ­¢ï¼‰
  - å®šæœŸåœ¨éªŒè¯é›†ä¸Šè¯„ä¼°
  - ä¿å­˜å¤šä¸ªcheckpointå¯¹æ¯”
```

---

## ğŸ“ æœ€ç»ˆå»ºè®®

### ç«‹å³æ‰§è¡Œ âœ…

**Option 1: ä¿å®ˆç­–ç•¥ï¼ˆæ¨èç»™æ—¶é—´ç´§çš„æƒ…å†µï¼‰**
```bash
1. è¯„ä¼°å®éªŒä¸€ï¼ˆ1å°æ—¶ï¼‰
2. å¦‚æœè¾¾æ ‡ï¼Œå®Œæˆé¡¹ç›®
3. å¦‚æœæœªè¾¾æ ‡ï¼Œæ‰§è¡Œæ–¹æ¡ˆA
```

**Option 2: æ¿€è¿›ç­–ç•¥ï¼ˆæ¨èç»™è¿½æ±‚æè‡´çš„æƒ…å†µï¼‰**
```bash
1. ç«‹å³å¯åŠ¨æ–¹æ¡ˆAï¼ˆåå°è¿è¡Œ10-15å°æ—¶ï¼‰
2. åŒæ—¶è¯„ä¼°å®éªŒä¸€
3. å¯¹æ¯”ä¸¤è€…ç»“æœï¼Œé€‰æ‹©æœ€ä¼˜
```

### æˆ‘çš„æ¨è â­â­â­â­â­

**ç«‹å³æ‰§è¡ŒOption 2ï¼ˆæ¿€è¿›ç­–ç•¥ï¼‰**

åŸå› ï¼š
1. æ–¹æ¡ˆAè®­ç»ƒæ—¶é—´é•¿ï¼Œè¶Šæ—©å¼€å§‹è¶Šå¥½
2. è¯„ä¼°å®éªŒä¸€åªéœ€1å°æ—¶ï¼Œå¯å¹¶è¡Œè¿›è¡Œ
3. ä¸¤ä¸ªç»“æœéƒ½æœ‰ï¼Œé€‰æ‹©ä½™åœ°å¤§
4. æˆåŠŸæ¦‚ç‡æœ€é«˜

å…·ä½“å‘½ä»¤ï¼š
```bash
# ç»ˆç«¯1: å¯åŠ¨æ–¹æ¡ˆA
cd /home/ubuntu/Sa2VA/lora_ppo_training
nohup bash run_lora_ppo_v2.sh > lora_ppo_v2.log 2>&1 &

# ç»ˆç«¯2: è¯„ä¼°å®éªŒä¸€ï¼ˆéœ€è¦å…ˆåˆ›å»ºè¯„ä¼°è„šæœ¬ï¼‰
cd /home/ubuntu/Sa2VA/rl_prompt_optimization
# python evaluate_rl_prompt.py ...

# ç›‘æ§
tail -f /home/ubuntu/Sa2VA/lora_ppo_training/lora_ppo_v2.log
```

---

**å†³ç­–æŒ‡å—ç”Ÿæˆæ—¶é—´**: 2025-11-29 17:48  
**å½“å‰çŠ¶æ€**: ç­‰å¾…ä¸‹ä¸€æ­¥å†³ç­–  
**æ¨èè¡ŒåŠ¨**: ç«‹å³å¯åŠ¨æ–¹æ¡ˆAï¼ˆä¼˜åŒ–LoRA+PPOï¼‰ ğŸš€
