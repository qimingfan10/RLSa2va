# âš ï¸ LoRA SFTè®­ç»ƒç»“æœåˆ†æ

**è®­ç»ƒæ—¶é—´**: 2025-11-29 23:27 - 2025-11-30 10:56  
**æ€»è€—æ—¶**: ~11.5å°æ—¶  
**çŠ¶æ€**: âŒ **è®­ç»ƒæ— æ•ˆ - æ¨¡å‹å‚æ•°æœªæ›´æ–°**

---

## ğŸ“Š è®­ç»ƒç»“æœ

### å„EpochæŒ‡æ ‡

```
Epoch 1/15:  Train Loss: 0.3149, Train Dice: 0.7416, Val Dice: 0.7342
Epoch 2/15:  Train Loss: 0.3148, Train Dice: 0.7417, Val Dice: 0.7342
Epoch 3/15:  Train Loss: 0.3148, Train Dice: 0.7415, Val Dice: 0.7342
Epoch 4/15:  Train Loss: 0.3147, Train Dice: 0.7416, Val Dice: 0.7342
Epoch 5/15:  Train Loss: 0.3147, Train Dice: 0.7417, Val Dice: 0.7342
Epoch 6/15:  Train Loss: 0.3144, Train Dice: 0.7419, Val Dice: 0.7342
Epoch 7/15:  Train Loss: 0.3146, Train Dice: 0.7418, Val Dice: 0.7342
Epoch 8/15:  Train Loss: 0.3146, Train Dice: 0.7417, Val Dice: 0.7342
Epoch 9/15:  Train Loss: 0.3146, Train Dice: 0.7418, Val Dice: 0.7342
Epoch 10/15: Train Loss: 0.3148, Train Dice: 0.7416, Val Dice: 0.7342
Epoch 11/15: Train Loss: 0.3147, Train Dice: 0.7416, Val Dice: 0.7342
Epoch 12/15: Train Loss: 0.3146, Train Dice: 0.7417, Val Dice: 0.7342
Epoch 13/15: Train Loss: 0.3145, Train Dice: 0.7418, Val Dice: 0.7342
Epoch 14/15: Train Loss: 0.3147, Train Dice: 0.7418, Val Dice: 0.7342
Epoch 15/15: Train Loss: 0.3148, Train Dice: 0.7416, Val Dice: 0.7342

Best Val Dice: 0.7342 (æ²¡æœ‰æå‡)
```

---

## âŒ å…³é”®é—®é¢˜

### 1. Val Diceå®Œå…¨æ²¡æœ‰å˜åŒ–

**æ‰€æœ‰15ä¸ªepochçš„Val Diceéƒ½æ˜¯0.7342**

è¿™æ˜ç¡®è¯æ˜ï¼š
- âœ… è®­ç»ƒå¾ªç¯å¯ä»¥æ­£å¸¸è¿è¡Œ
- âŒ **LoRAå‚æ•°æ²¡æœ‰çœŸæ­£æ›´æ–°**
- âŒ æ¨¡å‹è¾“å‡ºå®Œå…¨æ²¡æœ‰æ”¹å˜

### 2. æ ¹æœ¬åŸå› 

æ­£å¦‚ä¹‹å‰åˆ†æçš„ï¼Œé—®é¢˜åœ¨äºï¼š

```python
# Sa2VAçš„predict_forwardè°ƒç”¨é“¾
predict_forward() 
  â†’ generate()  # æœ‰ @torch.no_grad() è£…é¥°å™¨
    â†’ æ•´ä¸ªè®¡ç®—å›¾è¢«ç¦ç”¨
      â†’ æ¢¯åº¦æ— æ³•å›ä¼ 
```

å³ä½¿æˆ‘ä»¬æ‰‹åŠ¨è®¾ç½®ï¼š
```python
pred_prob = pred_prob.detach().requires_grad_(True)
```

è¿™åªæ˜¯è®©åç»­çš„Lossè®¡ç®—ä¸æŠ¥é”™ï¼Œä½†**æ¢¯åº¦æ— æ³•ç©¿é€`@torch.no_grad()`å›ä¼ åˆ°LoRAå‚æ•°**ã€‚

---

## ğŸ¯ æ­£ç¡®çš„è§£å†³æ–¹æ¡ˆ

### å¿…é¡»é‡å†™è®­ç»ƒæµç¨‹

ä¸èƒ½ä½¿ç”¨`predict_forward`ï¼Œéœ€è¦ç›´æ¥ä½¿ç”¨`forward`å‡½æ•°ï¼š

```python
# æ­£ç¡®çš„è®­ç»ƒæ–¹å¼
def train_step(model, image, mask, text, tokenizer):
    # 1. å‡†å¤‡è®­ç»ƒæ•°æ®æ ¼å¼
    data = prepare_training_data(
        image=image,
        mask=mask, 
        text=text,
        tokenizer=tokenizer
    )
    # data = {
    #     'pixel_values': ...,
    #     'input_ids': ...,
    #     'labels': ...,
    #     'attention_mask': ...,
    #     'position_ids': ...,
    # }
    
    # 2. ç›´æ¥è°ƒç”¨forwardï¼ˆæœ‰æ¢¯åº¦ï¼‰
    outputs = model.forward(data, mode='loss')
    
    # 3. è®¡ç®—loss
    loss = compute_segmentation_loss(outputs, mask)
    
    # 4. åå‘ä¼ æ’­ï¼ˆæ¢¯åº¦ä¼šå›ä¼ åˆ°LoRAå‚æ•°ï¼‰
    loss.backward()
    optimizer.step()
```

---

## ğŸ“‰ å½“å‰ç»“æœåˆ†æ

### Val Dice 0.7342 vs ä¹‹å‰çš„ç»“æœ

```yaml
å½“å‰SFTè®­ç»ƒ:     Val Dice 0.7342  (æœªä¼˜åŒ–çš„åŸºç¡€æ¨¡å‹)
é˜ˆå€¼æ‰«ææœ€ä¼˜:    Val Dice 0.7849  (threshold=0.35)
LoRA PPO (æ—§):  Val Dice 0.7889  (ä½†ä¹Ÿå¯èƒ½æœ‰é—®é¢˜)
```

**ç»“è®º**: å½“å‰çš„0.7342å°±æ˜¯**æœªç»å¾®è°ƒçš„Sa2VAåŸºç¡€æ¨¡å‹**åœ¨è¯¥æ•°æ®é›†ä¸Šçš„è¡¨ç°ã€‚

---

## ğŸ”§ éœ€è¦åšçš„å·¥ä½œ

### 1. ç ”ç©¶Sa2VAçš„forwardå‡½æ•°

æŸ¥çœ‹`modeling_sa2va_chat.py`çš„`forward()`å‡½æ•°ï¼Œç†è§£å…¶è¾“å…¥æ ¼å¼ï¼š

```python
def forward(self, data, data_samples=None, mode='loss'):
    # éœ€è¦çš„è¾“å…¥
    pixel_values = data['pixel_values']
    input_ids = data['input_ids']
    position_ids = data['position_ids']
    attention_mask = data['attention_mask']
    labels = data['labels']
    # ...
```

### 2. åˆ›å»ºproperçš„æ•°æ®åŠ è½½å™¨

```python
class ProperVesselDataset(Dataset):
    def __getitem__(self, idx):
        # è¿”å›å®Œæ•´çš„è®­ç»ƒæ ¼å¼æ•°æ®
        return {
            'pixel_values': ...,
            'input_ids': ...,
            'labels': ...,
            'attention_mask': ...,
            'position_ids': ...,
        }
```

### 3. é‡å†™è®­ç»ƒå¾ªç¯

```python
def train_epoch(model, dataloader):
    for batch in dataloader:
        # ç›´æ¥ä½¿ç”¨forward
        outputs = model.forward(batch, mode='loss')
        loss = outputs.loss  # æˆ–è€…è‡ªå®šä¹‰loss
        loss.backward()
        optimizer.step()
```

---

## ğŸ“ å½“å‰è®­ç»ƒäº§å‡º

```
è¾“å‡ºç›®å½•: /home/ubuntu/Sa2VA/lora_sft_training/output_sft/sft_20251129_232726/
æ¨¡å‹: best_model/  (å®é™…ä¸Šå°±æ˜¯æœªä¼˜åŒ–çš„LoRA adapter)

è¿™ä¸ªæ¨¡å‹æ²¡æœ‰ä»»ä½•ä»·å€¼ï¼Œå› ä¸ºå‚æ•°æ²¡æœ‰æ›´æ–°ã€‚
```

---

## ğŸ’¡ æ›¿ä»£æ–¹æ¡ˆ

å¦‚æœé‡å†™è®­ç»ƒå¤ªå¤æ‚ï¼Œå¯ä»¥è€ƒè™‘ï¼š

### æ–¹æ¡ˆA: ä½¿ç”¨ç°æœ‰é˜ˆå€¼ä¼˜åŒ–
```yaml
æ–¹æ³•: å›ºå®šthreshold=0.35
ç»“æœ: Val Dice 0.7849
ä¼˜åŠ¿: ç®€å•ç›´æ¥ï¼Œå·²éªŒè¯æœ‰æ•ˆ
```

### æ–¹æ¡ˆB: å°è¯•å…¶ä»–è®­ç»ƒæ¡†æ¶
- ä½¿ç”¨Hugging Face Trainer
- ä½¿ç”¨PEFTåº“çš„å®˜æ–¹è®­ç»ƒç¤ºä¾‹
- æŸ¥æ‰¾Sa2VAçš„å®˜æ–¹è®­ç»ƒä»£ç 

### æ–¹æ¡ˆC: è”ç³»Sa2VAä½œè€…
- è¯¢é—®å¦‚ä½•æ­£ç¡®è®­ç»ƒ
- è·å–å®˜æ–¹è®­ç»ƒè„šæœ¬

---

## ğŸ“Š æ—¶é—´æŠ•å…¥ vs æ”¶ç›Š

```
å·²æŠ•å…¥æ—¶é—´: ~11.5å°æ—¶è®­ç»ƒ + è°ƒè¯•æ—¶é—´
å®é™…æ”¶ç›Š:   0 (æ¨¡å‹æœªä¼˜åŒ–)
å­¦åˆ°çš„:     LoRAè®­ç»ƒçš„æ­£ç¡®æ–¹å¼å¾ˆé‡è¦

å»ºè®®: 
1. å¦‚æœæ€¥éœ€ç»“æœ â†’ ä½¿ç”¨é˜ˆå€¼ä¼˜åŒ–(0.35) â†’ Dice 0.7849
2. å¦‚æœè¦çœŸæ­£ä¼˜åŒ– â†’ é‡å†™è®­ç»ƒæµç¨‹ â†’ å¯èƒ½éœ€è¦1-2å¤©
3. å¦‚æœåªæ˜¯å®éªŒ â†’ å·²ç»å®Œæˆç›®æ ‡ï¼ˆéªŒè¯äº†æ–¹æ³•çš„å¯è¡Œæ€§å’Œå±€é™æ€§ï¼‰
```

---

## âœ… ä¸‹ä¸€æ­¥è¡ŒåŠ¨

### ç«‹å³å¯è¡Œ
1. **ä½¿ç”¨é˜ˆå€¼0.35** - å·²éªŒè¯Dice 0.7849
2. ä¿å­˜å½“å‰ç»“æœä½œä¸ºbaseline

### ä¸­æœŸç›®æ ‡  
1. ç ”ç©¶Sa2VAçš„forwardå‡½æ•°
2. é‡å†™è®­ç»ƒæ•°æ®åŠ è½½å™¨
3. å®ç°æ­£ç¡®çš„è®­ç»ƒå¾ªç¯

### é•¿æœŸç›®æ ‡
1. çœŸæ­£ä¼˜åŒ–Sa2VAæ¨¡å‹
2. è¾¾åˆ°Dice 0.84-0.86çš„ç›®æ ‡

---

**æ€»ç»“**: å½“å‰è®­ç»ƒè™½ç„¶è¿è¡Œå®Œæˆï¼Œä½†ç”±äºæ¢¯åº¦é—®é¢˜ï¼Œæ¨¡å‹å‚æ•°æœªæ›´æ–°ï¼Œè®­ç»ƒæ— æ•ˆã€‚éœ€è¦å®Œå…¨é‡å†™è®­ç»ƒæµç¨‹æ‰èƒ½çœŸæ­£ä¼˜åŒ–æ¨¡å‹ã€‚
