#!/usr/bin/env python3
"""
Sa2VA 26B å¯¹æ¯”å­¦ä¹ è®­ç»ƒ
ä½¿ç”¨Chosen/Rejected pairsè¿›è¡Œå¯¹æ¯”å¼ä¼˜åŒ–

æ ¸å¿ƒæ€æƒ³ï¼š
1. å¯¹chosen maskè®¡ç®—æ­£å‘Dice Lossï¼ˆé¼“åŠ±æ¨¡å‹é¢„æµ‹chosenï¼‰
2. å¯¹rejected maskè®¡ç®—è´Ÿå‘Dice Lossï¼ˆæƒ©ç½šæ¨¡å‹é¢„æµ‹rejectedï¼‰
"""

import os
import sys
import json
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from PIL import Image
from tqdm import tqdm

sys.path.insert(0, '/home/ubuntu/Sa2VA')

from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import LoraConfig, get_peft_model, TaskType


def dice_loss(pred: torch.Tensor, target: torch.Tensor, smooth: float = 1.0) -> torch.Tensor:
    """è®¡ç®—Dice Loss"""
    pred = pred.flatten()
    target = target.flatten()
    intersection = (pred * target).sum()
    return 1 - (2. * intersection + smooth) / (pred.sum() + target.sum() + smooth)


def contrastive_segmentation_loss(
    pred_mask: torch.Tensor, 
    chosen_mask: torch.Tensor, 
    rejected_mask: torch.Tensor,
    margin: float = 0.2
) -> torch.Tensor:
    """
    å¯¹æ¯”åˆ†å‰²æŸå¤±
    
    é¼“åŠ±ï¼špredæ¥è¿‘chosen
    æƒ©ç½šï¼špredæ¥è¿‘rejected
    
    Loss = Dice(pred, chosen) + max(0, margin - Dice(pred, rejected))
    """
    # ç¡®ä¿å°ºå¯¸ä¸€è‡´
    if pred_mask.shape != chosen_mask.shape:
        h, w = chosen_mask.shape[-2:]
        pred_mask = F.interpolate(
            pred_mask.unsqueeze(0).unsqueeze(0) if pred_mask.dim() == 2 else pred_mask.unsqueeze(0),
            size=(h, w),
            mode='bilinear',
            align_corners=False
        ).squeeze()
    
    # Sigmoidè½¬æ¢ä¸ºæ¦‚ç‡
    pred_prob = torch.sigmoid(pred_mask)
    
    # Chosen loss: æœ€å°åŒ–ä¸chosençš„å·®è·
    chosen_loss = dice_loss(pred_prob, chosen_mask)
    
    # Rejected loss: æœ€å¤§åŒ–ä¸rejectedçš„å·®è·ï¼ˆä½¿ç”¨marginï¼‰
    rejected_dice = 1 - dice_loss(pred_prob, rejected_mask)  # ç›¸ä¼¼åº¦
    rejected_loss = F.relu(rejected_dice - margin)  # å¦‚æœç›¸ä¼¼åº¦å¤ªé«˜åˆ™æƒ©ç½š
    
    return chosen_loss + 0.5 * rejected_loss


class Sa2VA26BTrainer:
    """Sa2VA 26Bè®­ç»ƒå™¨"""
    
    def __init__(
        self,
        model_path: str = "/home/ubuntu/Sa2VA/models/sa2va_vessel_hf",
        output_dir: str = "/home/ubuntu/Sa2VA/work_dirs/sa2va_26b_contrastive",
        lora_r: int = 16,
        lora_alpha: int = 32,
        learning_rate: float = 2e-5,
        num_epochs: int = 1,
        gradient_accumulation_steps: int = 8,
        max_samples: int = 500,
    ):
        self.model_path = model_path
        self.output_dir = output_dir
        self.lora_r = lora_r
        self.lora_alpha = lora_alpha
        self.learning_rate = learning_rate
        self.num_epochs = num_epochs
        self.gradient_accumulation_steps = gradient_accumulation_steps
        self.max_samples = max_samples
        
        os.makedirs(output_dir, exist_ok=True)
        
        print("=" * 60)
        print("ğŸ¯ Sa2VA 26B Contrastive Training")
        print("=" * 60)
        
        self._load_model()
        self._load_data()
        self._setup_optimizer()
    
    def _load_model(self):
        """åŠ è½½æ¨¡å‹"""
        print("\nğŸ“¥ Loading model...")
        
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path, trust_remote_code=True)
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_path,
            torch_dtype=torch.bfloat16,
            device_map='auto',
            trust_remote_code=True,
        )
        
        print("âœ… Model loaded!")
        
        # åº”ç”¨LoRAåˆ°language_model
        print("\nğŸ”§ Applying LoRA to language_model...")
        lora_config = LoraConfig(
            r=self.lora_r,
            lora_alpha=self.lora_alpha,
            lora_dropout=0.05,
            bias='none',
            task_type=TaskType.CAUSAL_LM,
            target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'],
        )
        
        self.model.language_model = get_peft_model(self.model.language_model, lora_config)
        
        # å†»ç»“è§†è§‰ç¼–ç å™¨
        self.model.vision_model.requires_grad_(False)
        
        # ä¿æŒSAM2 decoderå¯è®­ç»ƒï¼ˆå…³é”®éƒ¨åˆ†ï¼‰
        if hasattr(self.model, 'sam2'):
            for name, param in self.model.sam2.named_parameters():
                # åªè®­ç»ƒmask decoderçš„éƒ¨åˆ†å±‚
                if 'mask_decoder' in name or 'output_upscaling' in name:
                    param.requires_grad = True
                else:
                    param.requires_grad = False
        
        trainable = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        total = sum(p.numel() for p in self.model.parameters())
        print(f"âœ… Trainable: {trainable:,} / {total:,} ({100*trainable/total:.4f}%)")
    
    def _load_data(self):
        """åŠ è½½æ•°æ®"""
        print("\nğŸ“Š Loading data...")
        
        data_path = "/home/ubuntu/Sa2VA/data/dpo_vessel/dpo_annotations.json"
        data_root = "/home/ubuntu/Sa2VA/data/dpo_vessel"
        
        with open(data_path) as f:
            self.annotations = json.load(f)
        
        if self.max_samples:
            self.annotations = self.annotations[:self.max_samples]
        
        self.data_root = data_root
        print(f"   Loaded {len(self.annotations)} samples")
    
    def _setup_optimizer(self):
        """è®¾ç½®ä¼˜åŒ–å™¨"""
        self.optimizer = torch.optim.AdamW(
            [p for p in self.model.parameters() if p.requires_grad],
            lr=self.learning_rate,
            weight_decay=0.01,
        )
        
        total_steps = len(self.annotations) * self.num_epochs // self.gradient_accumulation_steps
        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer, T_max=max(total_steps, 1), eta_min=1e-7
        )
    
    def train_step(self, sample: dict) -> dict:
        """å•æ­¥è®­ç»ƒ"""
        # åŠ è½½æ•°æ®
        img_path = os.path.join(self.data_root, sample['image'])
        chosen_path = os.path.join(self.data_root, sample['chosen_mask'])
        rejected_path = os.path.join(self.data_root, sample['rejected_mask'])
        
        if not all(os.path.exists(p) for p in [img_path, chosen_path, rejected_path]):
            return None
        
        image = Image.open(img_path).convert('RGB')
        chosen_mask = torch.from_numpy(
            (np.array(Image.open(chosen_path).convert('L')) > 127).astype(np.float32)
        )
        rejected_mask = torch.from_numpy(
            (np.array(Image.open(rejected_path).convert('L')) > 127).astype(np.float32)
        )
        
        prompt = sample.get('prompt', '<image>Please segment the blood vessels.')
        
        # å‰å‘ä¼ æ’­ï¼ˆéœ€è¦æ¢¯åº¦ï¼‰
        try:
            # ä½¿ç”¨æ¨¡å‹çš„chatæ–¹æ³•è·å–é¢„æµ‹
            # è¿™é‡Œæˆ‘ä»¬ç›´æ¥è°ƒç”¨åº•å±‚çš„predict_forwardä½†å¯ç”¨æ¢¯åº¦
            with torch.set_grad_enabled(True):
                result = self.model.predict_forward(
                    image=image,
                    text=prompt,
                    tokenizer=self.tokenizer,
                )
            
            if not result.get('prediction_masks'):
                return None
            
            pred_mask = result['prediction_masks'][0][0]
            if isinstance(pred_mask, torch.Tensor):
                pred_mask = pred_mask.float()
            else:
                pred_mask = torch.from_numpy(pred_mask).float()
            
            # ç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡
            device = next(self.model.parameters()).device
            pred_mask = pred_mask.to(device)
            chosen_mask = chosen_mask.to(device)
            rejected_mask = rejected_mask.to(device)
            
            # è®¡ç®—å¯¹æ¯”æŸå¤±
            loss = contrastive_segmentation_loss(pred_mask, chosen_mask, rejected_mask)
            
            # è®¡ç®—æŒ‡æ ‡
            with torch.no_grad():
                pred_binary = (pred_mask > 0.5).float()
                chosen_dice = 1 - dice_loss(pred_binary, chosen_mask)
                rejected_dice = 1 - dice_loss(pred_binary, rejected_mask)
            
            return {
                'loss': loss,
                'chosen_dice': chosen_dice.item(),
                'rejected_dice': rejected_dice.item(),
            }
        
        except Exception as e:
            print(f"  Error: {e}")
            return None
    
    def train(self):
        """è®­ç»ƒ"""
        print("\nğŸš€ Starting training...")
        
        self.model.train()
        global_step = 0
        accumulated_loss = 0
        
        for epoch in range(self.num_epochs):
            print(f"\nğŸ“… Epoch {epoch + 1}/{self.num_epochs}")
            
            pbar = tqdm(self.annotations, desc="Training")
            
            for idx, sample in enumerate(pbar):
                result = self.train_step(sample)
                
                if result is None:
                    continue
                
                loss = result['loss']
                
                # æ£€æŸ¥lossæ˜¯å¦éœ€è¦æ¢¯åº¦
                if not loss.requires_grad:
                    # å¦‚æœæ²¡æœ‰æ¢¯åº¦ï¼Œåˆ›å»ºä¸€ä¸ªéœ€è¦æ¢¯åº¦çš„dummy loss
                    # è¿™ç§æƒ…å†µä¸‹æˆ‘ä»¬åªèƒ½é€šè¿‡å…¶ä»–æ–¹å¼è®­ç»ƒ
                    continue
                
                # åå‘ä¼ æ’­
                loss = loss / self.gradient_accumulation_steps
                loss.backward()
                accumulated_loss += loss.item()
                
                if (idx + 1) % self.gradient_accumulation_steps == 0:
                    torch.nn.utils.clip_grad_norm_(
                        [p for p in self.model.parameters() if p.requires_grad],
                        1.0
                    )
                    
                    self.optimizer.step()
                    self.scheduler.step()
                    self.optimizer.zero_grad()
                    
                    global_step += 1
                    
                    pbar.set_postfix({
                        'loss': f'{accumulated_loss:.4f}',
                        'chosen_dice': f'{result["chosen_dice"]:.4f}',
                        'rejected_dice': f'{result["rejected_dice"]:.4f}',
                    })
                    
                    accumulated_loss = 0
                
                # å®šæœŸä¿å­˜
                if global_step > 0 and global_step % 50 == 0:
                    self._save(f'step_{global_step}')
        
        # æœ€ç»ˆä¿å­˜
        self._save('final')
        
        print("\n" + "=" * 60)
        print("ğŸ‰ Training completed!")
        print(f"   Model saved to: {self.output_dir}")
        print("=" * 60)
    
    def _save(self, name: str):
        """ä¿å­˜æ¨¡å‹"""
        save_dir = os.path.join(self.output_dir, name)
        os.makedirs(save_dir, exist_ok=True)
        
        print(f"\nğŸ’¾ Saving to {save_dir}...")
        
        # åˆå¹¶LoRA
        self.model.language_model = self.model.language_model.merge_and_unload()
        
        self.model.save_pretrained(save_dir)
        self.tokenizer.save_pretrained(save_dir)
        
        # é‡æ–°åº”ç”¨LoRA
        lora_config = LoraConfig(
            r=self.lora_r,
            lora_alpha=self.lora_alpha,
            lora_dropout=0.05,
            bias='none',
            task_type=TaskType.CAUSAL_LM,
            target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'],
        )
        self.model.language_model = get_peft_model(self.model.language_model, lora_config)


def main():
    trainer = Sa2VA26BTrainer()
    trainer.train()


if __name__ == '__main__':
    main()
