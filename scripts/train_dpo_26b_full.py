#!/usr/bin/env python3
"""
Sa2VA 26B DPOè®­ç»ƒ - å®Œæ•´ç‰ˆ
åŸºäºå·²æœ‰å¾®è°ƒå¥½çš„26Bæ¨¡å‹ (Dice 0.82) è¿›è¡ŒDPOè®­ç»ƒ

ä½¿ç”¨ç®€åŒ–çš„DPOæŸå¤±ï¼šç›´æ¥æ¯”è¾ƒåˆ†å‰²maskçš„è´¨é‡
"""

import os
import sys
import json
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from PIL import Image
from tqdm import tqdm
from dataclasses import dataclass
from typing import Optional, Dict, List, Tuple
import cv2

sys.path.insert(0, '/home/ubuntu/Sa2VA')

from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import LoraConfig, get_peft_model, TaskType
from torch.utils.data import Dataset, DataLoader


@dataclass
class DPOConfig:
    """DPOè®­ç»ƒé…ç½®"""
    # æ¨¡å‹
    model_path: str = "/home/ubuntu/Sa2VA/models/sa2va_vessel_hf"
    output_dir: str = "/home/ubuntu/Sa2VA/work_dirs/sa2va_26b_dpo_vessel"
    
    # LoRA
    lora_r: int = 16
    lora_alpha: int = 32
    lora_dropout: float = 0.05
    
    # è®­ç»ƒ
    learning_rate: float = 2e-6
    num_epochs: int = 1
    batch_size: int = 1
    gradient_accumulation_steps: int = 8
    warmup_steps: int = 50
    max_grad_norm: float = 1.0
    
    # DPO
    beta: float = 0.1  # DPOæ¸©åº¦
    
    # æ•°æ®
    data_path: str = "/home/ubuntu/Sa2VA/data/dpo_vessel/dpo_annotations.json"
    max_samples: Optional[int] = 500  # é™åˆ¶æ ·æœ¬æ•°


class DPOVesselDataset(Dataset):
    """DPOè¡€ç®¡åˆ†å‰²æ•°æ®é›†"""
    
    def __init__(self, annotations_path: str, data_root: str, max_samples: Optional[int] = None):
        with open(annotations_path) as f:
            self.annotations = json.load(f)
        
        if max_samples:
            self.annotations = self.annotations[:max_samples]
        
        self.data_root = data_root
        
        print(f"Loaded {len(self.annotations)} DPO samples")
    
    def __len__(self):
        return len(self.annotations)
    
    def __getitem__(self, idx):
        ann = self.annotations[idx]
        
        # åŠ è½½å›¾ç‰‡
        img_path = os.path.join(self.data_root, ann['image'])
        image = Image.open(img_path).convert('RGB')
        
        # åŠ è½½chosenå’Œrejected masks
        chosen_path = os.path.join(self.data_root, ann['chosen_mask'])
        rejected_path = os.path.join(self.data_root, ann['rejected_mask'])
        
        chosen_mask = np.array(Image.open(chosen_path).convert('L'))
        rejected_mask = np.array(Image.open(rejected_path).convert('L'))
        
        # å½’ä¸€åŒ–åˆ°0-1
        chosen_mask = (chosen_mask > 127).astype(np.float32)
        rejected_mask = (rejected_mask > 127).astype(np.float32)
        
        return {
            'image': image,
            'chosen_mask': torch.from_numpy(chosen_mask),
            'rejected_mask': torch.from_numpy(rejected_mask),
            'chosen_iou': ann['chosen_iou'],
            'rejected_iou': ann['rejected_iou'],
            'prompt': ann.get('prompt', '<image>Please segment the blood vessels.'),
        }


def compute_mask_log_prob(pred_mask: torch.Tensor, target_mask: torch.Tensor) -> torch.Tensor:
    """
    è®¡ç®—é¢„æµ‹maskå’Œç›®æ ‡maskä¹‹é—´çš„logæ¦‚ç‡
    
    ä½¿ç”¨Binary Cross Entropyçš„è´Ÿå€¼ä½œä¸ºlog probability:
    log p(target|pred) = target * log(pred) + (1-target) * log(1-pred)
    """
    eps = 1e-7
    pred_mask = pred_mask.clamp(eps, 1 - eps)
    
    log_prob = target_mask * torch.log(pred_mask) + (1 - target_mask) * torch.log(1 - pred_mask)
    return log_prob.mean()


def compute_dice_score(pred: np.ndarray, target: np.ndarray) -> float:
    """è®¡ç®—Diceåˆ†æ•°"""
    pred = (pred > 0.5).astype(float)
    target = (target > 0.5).astype(float)
    intersection = (pred * target).sum()
    return 2 * intersection / (pred.sum() + target.sum() + 1e-8)


class DPOTrainer:
    """DPOè®­ç»ƒå™¨"""
    
    def __init__(self, config: DPOConfig):
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        print("=" * 60)
        print("ğŸ¯ Sa2VA 26B DPO Trainer")
        print("=" * 60)
        
        # åŠ è½½æ¨¡å‹
        self._load_model()
        
        # è®¾ç½®æ•°æ®
        self._setup_data()
        
        # è®¾ç½®ä¼˜åŒ–å™¨
        self._setup_optimizer()
    
    def _load_model(self):
        """åŠ è½½æ¨¡å‹å¹¶åº”ç”¨LoRA"""
        print("\nğŸ“¥ Loading model...")
        
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.config.model_path, 
            trust_remote_code=True
        )
        
        self.model = AutoModelForCausalLM.from_pretrained(
            self.config.model_path,
            torch_dtype=torch.bfloat16,
            device_map='auto',
            trust_remote_code=True,
        )
        
        print("âœ… Model loaded!")
        
        # åº”ç”¨LoRA
        print("\nğŸ”§ Applying LoRA...")
        lora_config = LoraConfig(
            r=self.config.lora_r,
            lora_alpha=self.config.lora_alpha,
            lora_dropout=self.config.lora_dropout,
            bias='none',
            task_type=TaskType.CAUSAL_LM,
            target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'],
        )
        
        self.model.language_model = get_peft_model(self.model.language_model, lora_config)
        
        # å†»ç»“å…¶ä»–éƒ¨åˆ†
        self.model.vision_model.requires_grad_(False)
        if hasattr(self.model, 'sam2'):
            self.model.sam2.requires_grad_(False)
        
        trainable = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        total = sum(p.numel() for p in self.model.parameters())
        print(f"âœ… LoRA applied! Trainable: {trainable:,} / {total:,} ({100*trainable/total:.4f}%)")
    
    def _setup_data(self):
        """è®¾ç½®æ•°æ®"""
        print("\nğŸ“Š Loading data...")
        data_root = os.path.dirname(self.config.data_path)
        self.dataset = DPOVesselDataset(
            self.config.data_path, 
            data_root, 
            self.config.max_samples
        )
    
    def _setup_optimizer(self):
        """è®¾ç½®ä¼˜åŒ–å™¨"""
        self.optimizer = torch.optim.AdamW(
            [p for p in self.model.parameters() if p.requires_grad],
            lr=self.config.learning_rate,
            weight_decay=0.01,
            betas=(0.9, 0.999),
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        total_steps = len(self.dataset) * self.config.num_epochs // self.config.gradient_accumulation_steps
        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer, T_max=total_steps, eta_min=1e-7
        )
    
    def get_model_prediction(self, image: Image.Image, prompt: str) -> Optional[np.ndarray]:
        """è·å–æ¨¡å‹é¢„æµ‹çš„mask"""
        try:
            with torch.cuda.amp.autocast(dtype=torch.bfloat16):
                result = self.model.predict_forward(
                    image=image,
                    text=prompt,
                    tokenizer=self.tokenizer,
                )
            
            if result.get('prediction_masks'):
                pred_mask = result['prediction_masks'][0][0]
                if isinstance(pred_mask, torch.Tensor):
                    pred_mask = pred_mask.cpu().numpy()
                return pred_mask
        except Exception as e:
            print(f"  Prediction error: {e}")
        
        return None
    
    def compute_dpo_loss(
        self, 
        pred_mask: torch.Tensor, 
        chosen_mask: torch.Tensor, 
        rejected_mask: torch.Tensor
    ) -> Tuple[torch.Tensor, Dict]:
        """
        è®¡ç®—DPOæŸå¤±
        
        DPO Loss = -log Ïƒ(Î² * (log Ï€(chosen) - log Ï€(rejected)))
        
        å¯¹äºåˆ†å‰²ä»»åŠ¡ï¼Œlog Ï€(mask) è¡¨ç¤ºæ¨¡å‹ç”Ÿæˆè¿™ä¸ªmaskçš„logæ¦‚ç‡
        æˆ‘ä»¬ç”¨é¢„æµ‹maskå’Œç›®æ ‡maskä¹‹é—´çš„ç›¸ä¼¼åº¦æ¥è¿‘ä¼¼
        """
        # ç¡®ä¿åœ¨åŒä¸€è®¾å¤‡ä¸Š
        pred_mask = pred_mask.float()
        chosen_mask = chosen_mask.float()
        rejected_mask = rejected_mask.float()
        
        # è°ƒæ•´å°ºå¯¸
        if pred_mask.shape != chosen_mask.shape:
            h, w = chosen_mask.shape
            pred_mask = F.interpolate(
                pred_mask.unsqueeze(0).unsqueeze(0),
                size=(h, w),
                mode='bilinear',
                align_corners=False
            ).squeeze()
        
        # è®¡ç®—ä¸chosenå’Œrejectedçš„logæ¦‚ç‡
        # è¿™é‡Œä½¿ç”¨è´ŸBCEä½œä¸ºlog probabilityçš„ä»£ç†
        chosen_log_prob = compute_mask_log_prob(pred_mask, chosen_mask)
        rejected_log_prob = compute_mask_log_prob(pred_mask, rejected_mask)
        
        # DPO Loss
        logits = self.config.beta * (chosen_log_prob - rejected_log_prob)
        loss = -F.logsigmoid(logits)
        
        # è®¡ç®—æŒ‡æ ‡
        with torch.no_grad():
            accuracy = (logits > 0).float()
            margin = (chosen_log_prob - rejected_log_prob).item()
        
        metrics = {
            'loss': loss.item(),
            'chosen_log_prob': chosen_log_prob.item(),
            'rejected_log_prob': rejected_log_prob.item(),
            'margin': margin,
            'accuracy': accuracy.item(),
        }
        
        return loss, metrics
    
    def train(self):
        """è®­ç»ƒå¾ªç¯"""
        print("\nğŸš€ Starting DPO training...")
        
        os.makedirs(self.config.output_dir, exist_ok=True)
        
        self.model.train()
        global_step = 0
        total_loss = 0
        accumulated_loss = 0
        
        all_metrics = []
        
        for epoch in range(self.config.num_epochs):
            print(f"\nğŸ“… Epoch {epoch + 1}/{self.config.num_epochs}")
            
            pbar = tqdm(range(len(self.dataset)), desc=f"Training")
            
            for idx in pbar:
                sample = self.dataset[idx]
                
                try:
                    # è·å–æ¨¡å‹é¢„æµ‹
                    pred_mask = self.get_model_prediction(sample['image'], sample['prompt'])
                    
                    if pred_mask is None:
                        continue
                    
                    # è½¬æ¢ä¸ºtensor
                    pred_tensor = torch.from_numpy(pred_mask).float()
                    
                    # ç¡®å®šè®¾å¤‡
                    device = next(self.model.parameters()).device
                    pred_tensor = pred_tensor.to(device)
                    chosen_tensor = sample['chosen_mask'].to(device)
                    rejected_tensor = sample['rejected_mask'].to(device)
                    
                    # è®¡ç®—DPOæŸå¤±
                    loss, metrics = self.compute_dpo_loss(
                        pred_tensor, 
                        chosen_tensor, 
                        rejected_tensor
                    )
                    
                    # åå‘ä¼ æ’­
                    loss = loss / self.config.gradient_accumulation_steps
                    loss.backward()
                    accumulated_loss += loss.item()
                    
                    # æ¢¯åº¦ç´¯ç§¯
                    if (idx + 1) % self.config.gradient_accumulation_steps == 0:
                        # æ¢¯åº¦è£å‰ª
                        torch.nn.utils.clip_grad_norm_(
                            [p for p in self.model.parameters() if p.requires_grad],
                            self.config.max_grad_norm
                        )
                        
                        self.optimizer.step()
                        self.scheduler.step()
                        self.optimizer.zero_grad()
                        
                        global_step += 1
                        total_loss += accumulated_loss
                        
                        # æ›´æ–°è¿›åº¦æ¡
                        pbar.set_postfix({
                            'loss': f'{accumulated_loss:.4f}',
                            'margin': f'{metrics["margin"]:.4f}',
                            'lr': f'{self.scheduler.get_last_lr()[0]:.2e}'
                        })
                        
                        all_metrics.append(metrics)
                        accumulated_loss = 0
                    
                    # å®šæœŸä¿å­˜
                    if global_step > 0 and global_step % 100 == 0:
                        self._save_checkpoint(global_step)
                
                except Exception as e:
                    print(f"\n  Error at step {idx}: {e}")
                    continue
        
        # æœ€ç»ˆä¿å­˜
        self._save_checkpoint(global_step, final=True)
        
        # æ‰“å°æ€»ç»“
        print("\n" + "=" * 60)
        print("ğŸ‰ Training completed!")
        print("=" * 60)
        print(f"Total steps: {global_step}")
        print(f"Average loss: {total_loss / max(global_step, 1):.4f}")
        print(f"Model saved to: {self.config.output_dir}")
    
    def _save_checkpoint(self, step: int, final: bool = False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        save_dir = self.config.output_dir if final else os.path.join(self.config.output_dir, f'step_{step}')
        os.makedirs(save_dir, exist_ok=True)
        
        print(f"\nğŸ’¾ Saving checkpoint to {save_dir}...")
        
        # åˆå¹¶LoRAæƒé‡
        self.model.language_model = self.model.language_model.merge_and_unload()
        
        # ä¿å­˜æ¨¡å‹
        self.model.save_pretrained(save_dir)
        self.tokenizer.save_pretrained(save_dir)
        
        # é‡æ–°åº”ç”¨LoRAï¼ˆå¦‚æœä¸æ˜¯æœ€ç»ˆä¿å­˜ï¼‰
        if not final:
            lora_config = LoraConfig(
                r=self.config.lora_r,
                lora_alpha=self.config.lora_alpha,
                lora_dropout=self.config.lora_dropout,
                bias='none',
                task_type=TaskType.CAUSAL_LM,
                target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'],
            )
            self.model.language_model = get_peft_model(self.model.language_model, lora_config)


def main():
    config = DPOConfig()
    
    trainer = DPOTrainer(config)
    trainer.train()


if __name__ == '__main__':
    main()
