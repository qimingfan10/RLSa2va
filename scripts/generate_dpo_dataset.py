#!/usr/bin/env python3
"""
ç”ŸæˆDPOåå¥½å¯¹æ•°æ®é›†

å·¥ä½œæµç¨‹ï¼š
1. åŠ è½½Sa2VAæ¨¡å‹
2. å¯¹æ¯å¼ å›¾åƒï¼Œä½¿ç”¨ä¸åŒçš„é‡‡æ ·ç­–ç•¥ç”Ÿæˆå¤šä¸ªmask
3. è®¡ç®—æ¯ä¸ªmaskä¸GTçš„IoU
4. æ„å»ºåå¥½å¯¹ï¼šIoUé«˜çš„ä¸ºchosenï¼ŒIoUä½çš„ä¸ºrejected
5. ä¿å­˜ä¸ºDPOæ•°æ®é›†æ ¼å¼
"""

import argparse
import json
import os
import sys
from pathlib import Path
from typing import List, Tuple

import numpy as np
import torch
from PIL import Image
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, '/home/ubuntu/Sa2VA')


def compute_iou(pred: np.ndarray, gt: np.ndarray) -> float:
    """è®¡ç®—IoU (Intersection over Union)"""
    pred_binary = (pred > 0.5).astype(bool)
    gt_binary = (gt > 0.5).astype(bool)
    
    intersection = np.logical_and(pred_binary, gt_binary).sum()
    union = np.logical_or(pred_binary, gt_binary).sum()
    
    if union == 0:
        return 0.0
    return float(intersection) / float(union)


def compute_dice(pred: np.ndarray, gt: np.ndarray) -> float:
    """è®¡ç®—Diceç³»æ•°"""
    pred_binary = (pred > 0.5).astype(bool)
    gt_binary = (gt > 0.5).astype(bool)
    
    intersection = np.logical_and(pred_binary, gt_binary).sum()
    
    if pred_binary.sum() + gt_binary.sum() == 0:
        return 0.0
    return float(2 * intersection) / float(pred_binary.sum() + gt_binary.sum())


class PreferencePairGenerator:
    """åå¥½å¯¹ç”Ÿæˆå™¨"""
    
    def __init__(
        self,
        model_path: str,
        device: str = "cuda:0",
        num_samples_per_image: int = 5,
        temperature_range: Tuple[float, float] = (0.3, 1.0),
        min_iou_gap: float = 0.05
    ):
        self.device = device
        self.num_samples = num_samples_per_image
        self.temp_range = temperature_range
        self.min_iou_gap = min_iou_gap
        
        # åŠ è½½Sa2VAæ¨¡å‹
        print(f"ğŸ“¦ åŠ è½½Sa2VAæ¨¡å‹: {model_path}")
        from transformers import AutoModel, AutoTokenizer
        
        self.model = AutoModel.from_pretrained(
            model_path,
            torch_dtype=torch.bfloat16,
            trust_remote_code=True
        ).to(device).eval()
        
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=True
        )
        
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    
    @torch.no_grad()
    def generate_masks(self, image: Image.Image, prompt: str) -> List[Tuple[np.ndarray, dict]]:
        """
        ä½¿ç”¨ä¸åŒé‡‡æ ·ç­–ç•¥ç”Ÿæˆå¤šä¸ªmask
        
        Returns:
            List of (mask, metadata) tuples
        """
        masks = []
        
        # ç­–ç•¥1: ä¸åŒæ¸©åº¦é‡‡æ ·
        temperatures = np.linspace(self.temp_range[0], self.temp_range[1], self.num_samples)
        
        for temp in temperatures:
            try:
                # è°ƒç”¨Sa2VAç”Ÿæˆmask
                result = self.model.chat(
                    self.tokenizer,
                    pixel_values=image,
                    question=prompt,
                    generation_config={
                        'temperature': temp,
                        'do_sample': temp > 0.1,
                        'max_new_tokens': 512
                    }
                )
                
                # æå–mask
                if hasattr(result, 'masks') and len(result.masks) > 0:
                    mask = result.masks[0].cpu().numpy()
                    masks.append((mask, {'temperature': temp, 'method': 'temperature_sampling'}))
                    
            except Exception as e:
                print(f"âš ï¸ ç”Ÿæˆå¤±è´¥ (temp={temp}): {e}")
                continue
        
        # ç­–ç•¥2: æ·»åŠ å™ªå£°æ‰°åŠ¨
        try:
            # åŸºç¡€é¢„æµ‹
            base_result = self.model.chat(
                self.tokenizer,
                pixel_values=image,
                question=prompt,
                generation_config={'temperature': 0.0, 'do_sample': False}
            )
            
            if hasattr(base_result, 'masks') and len(base_result.masks) > 0:
                base_mask = base_result.masks[0].cpu().numpy()
                
                # æ·»åŠ å½¢æ€å­¦æ‰°åŠ¨
                from scipy import ndimage
                
                # è†¨èƒ€
                dilated = ndimage.binary_dilation(base_mask > 0.5, iterations=2).astype(np.float32)
                masks.append((dilated, {'method': 'dilation'}))
                
                # è…èš€
                eroded = ndimage.binary_erosion(base_mask > 0.5, iterations=2).astype(np.float32)
                masks.append((eroded, {'method': 'erosion'}))
                
        except Exception as e:
            print(f"âš ï¸ æ‰°åŠ¨ç”Ÿæˆå¤±è´¥: {e}")
        
        return masks
    
    def build_preference_pairs(
        self,
        image_path: str,
        gt_path: str,
        output_dir: str,
        image_id: str
    ) -> List[dict]:
        """
        ä¸ºå•å¼ å›¾åƒæ„å»ºåå¥½å¯¹
        
        Returns:
            List of preference pair annotations
        """
        # åŠ è½½å›¾åƒå’ŒGT
        image = Image.open(image_path).convert('RGB')
        gt = np.array(Image.open(gt_path).convert('L'))
        gt = (gt > 127).astype(np.float32)
        
        # ç”Ÿæˆå¤šä¸ªmask
        prompt = "<image>Please segment the blood vessels."
        masks_with_meta = self.generate_masks(image, prompt)
        
        if len(masks_with_meta) < 2:
            return []
        
        # è®¡ç®—æ¯ä¸ªmaskçš„IoU
        mask_scores = []
        for i, (mask, meta) in enumerate(masks_with_meta):
            # ç¡®ä¿maskå°ºå¯¸ä¸GTä¸€è‡´
            if mask.shape != gt.shape:
                from PIL import Image as PILImage
                mask_pil = PILImage.fromarray((mask * 255).astype(np.uint8))
                mask_pil = mask_pil.resize((gt.shape[1], gt.shape[0]), PILImage.NEAREST)
                mask = np.array(mask_pil) / 255.0
            
            iou = compute_iou(mask, gt)
            dice = compute_dice(mask, gt)
            mask_scores.append({
                'mask': mask,
                'iou': iou,
                'dice': dice,
                'meta': meta,
                'index': i
            })
        
        # æŒ‰IoUæ’åº
        mask_scores.sort(key=lambda x: x['iou'], reverse=True)
        
        # æ„å»ºåå¥½å¯¹
        pairs = []
        masks_dir = os.path.join(output_dir, 'masks')
        os.makedirs(masks_dir, exist_ok=True)
        
        for i in range(len(mask_scores)):
            for j in range(i + 1, len(mask_scores)):
                chosen = mask_scores[i]
                rejected = mask_scores[j]
                
                # æ£€æŸ¥IoUå·®è·æ˜¯å¦è¶³å¤Ÿå¤§
                iou_gap = chosen['iou'] - rejected['iou']
                if iou_gap < self.min_iou_gap:
                    continue
                
                # ä¿å­˜masks
                chosen_filename = f"{image_id}_chosen_{i}_{j}.png"
                rejected_filename = f"{image_id}_rejected_{i}_{j}.png"
                
                chosen_path = os.path.join(masks_dir, chosen_filename)
                rejected_path = os.path.join(masks_dir, rejected_filename)
                
                Image.fromarray((chosen['mask'] * 255).astype(np.uint8)).save(chosen_path)
                Image.fromarray((rejected['mask'] * 255).astype(np.uint8)).save(rejected_path)
                
                # åˆ›å»ºannotation
                pair = {
                    'image': os.path.relpath(image_path, output_dir),
                    'chosen_mask': os.path.relpath(chosen_path, output_dir),
                    'rejected_mask': os.path.relpath(rejected_path, output_dir),
                    'chosen_iou': chosen['iou'],
                    'rejected_iou': rejected['iou'],
                    'chosen_dice': chosen['dice'],
                    'rejected_dice': rejected['dice'],
                    'iou_gap': iou_gap,
                    'chosen_meta': chosen['meta'],
                    'rejected_meta': rejected['meta'],
                    'prompt': prompt
                }
                pairs.append(pair)
        
        return pairs


def generate_from_existing_data(
    images_dir: str,
    gt_dir: str,
    output_dir: str,
    model_path: str,
    num_samples: int = 5,
    min_iou_gap: float = 0.05
):
    """ä»ç°æœ‰æ•°æ®ç”ŸæˆDPOæ•°æ®é›†"""
    
    os.makedirs(output_dir, exist_ok=True)
    
    # åˆå§‹åŒ–ç”Ÿæˆå™¨
    generator = PreferencePairGenerator(
        model_path=model_path,
        num_samples_per_image=num_samples,
        min_iou_gap=min_iou_gap
    )
    
    # æ”¶é›†æ‰€æœ‰å›¾åƒ
    image_files = []
    for ext in ['*.png', '*.jpg', '*.jpeg']:
        image_files.extend(Path(images_dir).glob(ext))
    
    print(f"ğŸ“Š æ‰¾åˆ° {len(image_files)} å¼ å›¾åƒ")
    
    # ç”Ÿæˆåå¥½å¯¹
    all_pairs = []
    
    for img_path in tqdm(image_files, desc="ç”Ÿæˆåå¥½å¯¹"):
        # æ‰¾åˆ°å¯¹åº”çš„GT
        gt_path = Path(gt_dir) / img_path.name
        if not gt_path.exists():
            # å°è¯•å…¶ä»–å‘½åæ ¼å¼
            gt_path = Path(gt_dir) / f"{img_path.stem}_mask.png"
        if not gt_path.exists():
            gt_path = Path(gt_dir) / f"{img_path.stem}.png"
        
        if not gt_path.exists():
            print(f"âš ï¸ æ‰¾ä¸åˆ°GT: {img_path.name}")
            continue
        
        # ç”Ÿæˆåå¥½å¯¹
        pairs = generator.build_preference_pairs(
            image_path=str(img_path),
            gt_path=str(gt_path),
            output_dir=output_dir,
            image_id=img_path.stem
        )
        
        all_pairs.extend(pairs)
        print(f"  âœ… {img_path.name}: {len(pairs)} å¯¹")
    
    # ä¿å­˜annotations
    ann_path = os.path.join(output_dir, 'dpo_annotations.json')
    with open(ann_path, 'w') as f:
        json.dump(all_pairs, f, indent=2)
    
    # æ‰“å°ç»Ÿè®¡
    print(f"\n{'='*60}")
    print(f"ğŸ“Š DPOæ•°æ®é›†ç”Ÿæˆå®Œæˆ!")
    print(f"{'='*60}")
    print(f"  - æ€»åå¥½å¯¹æ•°: {len(all_pairs)}")
    print(f"  - å¹³å‡IoUå·®è·: {np.mean([p['iou_gap'] for p in all_pairs]):.4f}")
    print(f"  - è¾“å‡ºç›®å½•: {output_dir}")
    print(f"  - Annotations: {ann_path}")
    
    return all_pairs


def generate_from_predictions(
    predictions_dir: str,
    gt_dir: str,
    output_dir: str,
    min_iou_gap: float = 0.05
):
    """
    ä»å·²æœ‰çš„å¤šä¸ªé¢„æµ‹ç»“æœç”ŸæˆDPOæ•°æ®é›†
    
    predictions_dir ç»“æ„:
    â”œâ”€â”€ method_1/
    â”‚   â”œâ”€â”€ image_001.png
    â”‚   â””â”€â”€ ...
    â”œâ”€â”€ method_2/
    â”‚   â””â”€â”€ ...
    â””â”€â”€ method_3/
        â””â”€â”€ ...
    """
    os.makedirs(output_dir, exist_ok=True)
    
    # æ”¶é›†æ‰€æœ‰é¢„æµ‹æ–¹æ³•
    methods = [d for d in Path(predictions_dir).iterdir() if d.is_dir()]
    print(f"ğŸ“Š æ‰¾åˆ° {len(methods)} ç§é¢„æµ‹æ–¹æ³•: {[m.name for m in methods]}")
    
    # æ”¶é›†æ‰€æœ‰å›¾åƒID
    all_image_ids = set()
    for method_dir in methods:
        for f in method_dir.glob('*.png'):
            all_image_ids.add(f.stem)
    
    print(f"ğŸ“Š å…± {len(all_image_ids)} å¼ å›¾åƒ")
    
    all_pairs = []
    masks_dir = os.path.join(output_dir, 'masks')
    os.makedirs(masks_dir, exist_ok=True)
    
    for image_id in tqdm(all_image_ids, desc="æ„å»ºåå¥½å¯¹"):
        # åŠ è½½GT
        gt_path = Path(gt_dir) / f"{image_id}.png"
        if not gt_path.exists():
            gt_path = Path(gt_dir) / f"{image_id}_mask.png"
        if not gt_path.exists():
            continue
            
        gt = np.array(Image.open(gt_path).convert('L'))
        gt = (gt > 127).astype(np.float32)
        
        # æ”¶é›†æ‰€æœ‰æ–¹æ³•çš„é¢„æµ‹å’ŒIoU
        predictions = []
        for method_dir in methods:
            pred_path = method_dir / f"{image_id}.png"
            if not pred_path.exists():
                continue
            
            pred = np.array(Image.open(pred_path).convert('L'))
            pred = (pred > 127).astype(np.float32)
            
            # è°ƒæ•´å°ºå¯¸
            if pred.shape != gt.shape:
                pred_pil = Image.fromarray((pred * 255).astype(np.uint8))
                pred_pil = pred_pil.resize((gt.shape[1], gt.shape[0]), Image.NEAREST)
                pred = np.array(pred_pil) / 255.0
            
            iou = compute_iou(pred, gt)
            predictions.append({
                'mask': pred,
                'iou': iou,
                'method': method_dir.name,
                'path': str(pred_path)
            })
        
        if len(predictions) < 2:
            continue
        
        # æŒ‰IoUæ’åº
        predictions.sort(key=lambda x: x['iou'], reverse=True)
        
        # æ„å»ºåå¥½å¯¹
        for i in range(len(predictions)):
            for j in range(i + 1, len(predictions)):
                chosen = predictions[i]
                rejected = predictions[j]
                
                iou_gap = chosen['iou'] - rejected['iou']
                if iou_gap < min_iou_gap:
                    continue
                
                # ä¿å­˜æˆ–å¼•ç”¨masks
                chosen_rel = os.path.relpath(chosen['path'], output_dir)
                rejected_rel = os.path.relpath(rejected['path'], output_dir)
                
                # æ‰¾åˆ°åŸå›¾
                image_path = None
                for ext in ['.jpg', '.png', '.jpeg']:
                    candidate = Path(gt_dir).parent / 'images' / f"{image_id}{ext}"
                    if candidate.exists():
                        image_path = str(candidate)
                        break
                
                if image_path is None:
                    continue
                
                pair = {
                    'image': os.path.relpath(image_path, output_dir),
                    'chosen_mask': chosen_rel,
                    'rejected_mask': rejected_rel,
                    'chosen_iou': chosen['iou'],
                    'rejected_iou': rejected['iou'],
                    'iou_gap': iou_gap,
                    'chosen_method': chosen['method'],
                    'rejected_method': rejected['method'],
                    'prompt': '<image>Please segment the blood vessels.'
                }
                all_pairs.append(pair)
    
    # ä¿å­˜
    ann_path = os.path.join(output_dir, 'dpo_annotations.json')
    with open(ann_path, 'w') as f:
        json.dump(all_pairs, f, indent=2)
    
    print(f"\n{'='*60}")
    print(f"ğŸ“Š DPOæ•°æ®é›†ç”Ÿæˆå®Œæˆ!")
    print(f"  - æ€»åå¥½å¯¹æ•°: {len(all_pairs)}")
    print(f"  - è¾“å‡º: {ann_path}")
    
    return all_pairs


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='ç”ŸæˆDPOåå¥½å¯¹æ•°æ®é›†')
    parser.add_argument('--mode', choices=['generate', 'from_predictions'], default='generate',
                        help='ç”Ÿæˆæ¨¡å¼')
    parser.add_argument('--images_dir', type=str, required=True, help='å›¾åƒç›®å½•')
    parser.add_argument('--gt_dir', type=str, required=True, help='GTç›®å½•')
    parser.add_argument('--output_dir', type=str, required=True, help='è¾“å‡ºç›®å½•')
    parser.add_argument('--model_path', type=str, 
                        default='/home/ubuntu/Sa2VA/models/sa2va_vessel_hf',
                        help='Sa2VAæ¨¡å‹è·¯å¾„')
    parser.add_argument('--num_samples', type=int, default=5, help='æ¯å¼ å›¾åƒé‡‡æ ·æ•°')
    parser.add_argument('--min_iou_gap', type=float, default=0.05, help='æœ€å°IoUå·®è·')
    
    args = parser.parse_args()
    
    if args.mode == 'generate':
        generate_from_existing_data(
            images_dir=args.images_dir,
            gt_dir=args.gt_dir,
            output_dir=args.output_dir,
            model_path=args.model_path,
            num_samples=args.num_samples,
            min_iou_gap=args.min_iou_gap
        )
    else:
        generate_from_predictions(
            predictions_dir=args.images_dir,  # è¿™é‡Œæ˜¯predictionsç›®å½•
            gt_dir=args.gt_dir,
            output_dir=args.output_dir,
            min_iou_gap=args.min_iou_gap
        )
