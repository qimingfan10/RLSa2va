#!/usr/bin/env python3
"""
ç›´æ¥æµ‹è¯•DPOæ¨¡å‹æ¨ç†æ•ˆæœ
"""

import os
import sys
import torch
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt

sys.path.insert(0, '/home/ubuntu/Sa2VA')

def load_model_with_checkpoint(config_path, checkpoint_path):
    """åŠ è½½æ¨¡å‹å¹¶åº”ç”¨checkpoint"""
    from mmengine.config import Config
    from mmengine.registry import MODELS
    
    print("ğŸ“ åŠ è½½é…ç½®...")
    cfg = Config.fromfile(config_path)
    
    print("ğŸ—ï¸ æ„å»ºæ¨¡å‹ï¼ˆåŒ…å«åŸºç¡€æƒé‡ï¼‰...")
    model = MODELS.build(cfg.model)
    
    # æ¨¡å‹æ„å»ºæ—¶å·²ç»åŠ è½½äº†åŸºç¡€æƒé‡ï¼Œç°åœ¨åŠ è½½DPOè®­ç»ƒçš„LoRAæƒé‡
    print(f"ğŸ“¥ åŠ è½½DPO LoRAæƒé‡: {checkpoint_path}")
    checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)
    
    if 'state_dict' in checkpoint:
        state_dict = checkpoint['state_dict']
    else:
        state_dict = checkpoint
    
    # åªåŠ è½½LoRAç›¸å…³çš„æƒé‡
    model_state = model.state_dict()
    loaded = 0
    lora_loaded = 0
    for key, value in state_dict.items():
        if key in model_state and value.shape == model_state[key].shape:
            model_state[key] = value
            loaded += 1
            if 'lora' in key.lower():
                lora_loaded += 1
    
    model.load_state_dict(model_state, strict=False)
    print(f"   åŠ è½½äº† {loaded} ä¸ªå‚æ•° (å…¶ä¸­LoRA: {lora_loaded})")
    
    model.eval()
    
    # ä½¿ç”¨å¤šGPU
    import torch.distributed as dist
    if torch.cuda.device_count() > 1:
        print(f"   ä½¿ç”¨ {torch.cuda.device_count()} ä¸ªGPU")
        # ä½¿ç”¨device_mapè‡ªåŠ¨åˆ†é…
        from accelerate import dispatch_model, infer_auto_device_map
        device_map = infer_auto_device_map(model, max_memory={i: "22GiB" for i in range(torch.cuda.device_count())})
        model = dispatch_model(model, device_map=device_map)
    else:
        model.to(torch.bfloat16)
        model.cuda()
    
    return model

def inference_single(model, image_path, prompt="Please segment the blood vessel in this image."):
    """å¯¹å•å¼ å›¾ç‰‡è¿›è¡Œæ¨ç†"""
    from transformers import AutoTokenizer
    
    image = Image.open(image_path).convert('RGB')
    
    # è·å–tokenizer
    tokenizer = model.mllm.tokenizer
    
    with torch.no_grad():
        try:
            # ä½¿ç”¨æ¨¡å‹çš„predict_forwardæ–¹æ³•
            result = model.mllm.model.predict_forward(
                image=image,
                text=f"<image>\n{prompt}",
                tokenizer=tokenizer,
            )
            
            pred_text = result.get('prediction', '')
            masks = result.get('prediction_masks', [])
            
            return {
                'text': pred_text,
                'masks': masks,
                'success': len(masks) > 0
            }
        except Exception as e:
            print(f"   æ¨ç†é”™è¯¯: {e}")
            return {'text': '', 'masks': [], 'success': False, 'error': str(e)}

def visualize_result(image_path, result, output_path):
    """å¯è§†åŒ–ç»“æœ"""
    image = Image.open(image_path).convert('RGB')
    img_array = np.array(image)
    
    fig, axes = plt.subplots(1, 2, figsize=(12, 5))
    
    axes[0].imshow(img_array)
    axes[0].set_title('Input Image')
    axes[0].axis('off')
    
    if result['masks']:
        mask = result['masks'][0]
        if len(mask.shape) == 3:
            mask = mask[0]
        if isinstance(mask, torch.Tensor):
            mask = mask.cpu().numpy()
        
        # Overlay mask
        overlay = img_array.copy().astype(float)
        mask_resized = np.array(Image.fromarray((mask * 255).astype(np.uint8)).resize(
            (img_array.shape[1], img_array.shape[0]), Image.NEAREST)) / 255.0
        
        overlay[:, :, 1] = np.clip(overlay[:, :, 1] + mask_resized * 100, 0, 255)
        
        axes[1].imshow(overlay.astype(np.uint8))
        axes[1].set_title(f'Prediction\n{result["text"][:50]}...')
    else:
        axes[1].imshow(img_array)
        axes[1].set_title(f'No mask\n{result["text"][:50]}...')
    
    axes[1].axis('off')
    
    plt.tight_layout()
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    plt.close()
    print(f"   ä¿å­˜: {output_path}")

def main():
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--checkpoint', type=str,
                       default='/home/ubuntu/Sa2VA/work_dirs/dpo_vessel_training/iter_1224.pth')
    parser.add_argument('--config', type=str,
                       default='/home/ubuntu/Sa2VA/projects/sa2va/configs/sa2va_dpo_finetune_v3.py')
    parser.add_argument('--test_dir', type=str,
                       default='/home/ubuntu/Sa2VA/data/dpo_vessel/images')
    parser.add_argument('--output_dir', type=str,
                       default='/home/ubuntu/Sa2VA/work_dirs/dpo_vessel_training/test_results')
    parser.add_argument('--num_samples', type=int, default=5)
    args = parser.parse_args()
    
    print("=" * 60)
    print("ğŸ§ª DPOæ¨¡å‹æ¨ç†æµ‹è¯•")
    print("=" * 60)
    
    os.makedirs(args.output_dir, exist_ok=True)
    
    # åŠ è½½æ¨¡å‹
    model = load_model_with_checkpoint(args.config, args.checkpoint)
    
    # è·å–æµ‹è¯•å›¾ç‰‡
    from pathlib import Path
    test_images = list(Path(args.test_dir).glob("*.jpg"))[:args.num_samples]
    
    print(f"\nğŸ“¸ æµ‹è¯• {len(test_images)} å¼ å›¾ç‰‡...")
    
    results = []
    for i, img_path in enumerate(test_images):
        print(f"\n[{i+1}/{len(test_images)}] {img_path.name}")
        
        result = inference_single(model, str(img_path))
        results.append(result)
        
        print(f"   è¾“å‡º: {result['text'][:80]}...")
        print(f"   æœ‰mask: {result['success']}")
        
        # å¯è§†åŒ–
        output_path = os.path.join(args.output_dir, f"result_{i+1}.png")
        visualize_result(str(img_path), result, output_path)
    
    # ç»Ÿè®¡
    success_count = sum(1 for r in results if r['success'])
    print(f"\n" + "=" * 60)
    print(f"ğŸ“Š æµ‹è¯•ç»“æœ: {success_count}/{len(results)} æˆåŠŸç”Ÿæˆmask")
    print(f"   ç»“æœä¿å­˜åœ¨: {args.output_dir}")
    print("=" * 60)

if __name__ == '__main__':
    main()
