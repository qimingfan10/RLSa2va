#!/usr/bin/env python3
"""
è¯„ä¼°DPOè®­ç»ƒåçš„æ¨¡å‹æ•ˆæœ - å¤šGPUç‰ˆæœ¬
"""

import os
import sys
import json
import torch
import numpy as np
from PIL import Image
from pathlib import Path

sys.path.insert(0, '/home/ubuntu/Sa2VA')

def calculate_iou(pred_mask, gt_mask):
    """è®¡ç®—IoU"""
    pred = pred_mask.flatten() > 0.5
    gt = gt_mask.flatten() > 0.5
    intersection = np.logical_and(pred, gt).sum()
    union = np.logical_or(pred, gt).sum()
    return intersection / (union + 1e-8)

def calculate_dice(pred_mask, gt_mask):
    """è®¡ç®—Dice"""
    pred = pred_mask.flatten() > 0.5
    gt = gt_mask.flatten() > 0.5
    intersection = np.logical_and(pred, gt).sum()
    return 2 * intersection / (pred.sum() + gt.sum() + 1e-8)

def load_model(config_path, checkpoint_path):
    """ä½¿ç”¨DeepSpeedåŠ è½½æ¨¡å‹"""
    from mmengine.config import Config
    from mmengine.registry import MODELS
    
    print("ğŸ“ åŠ è½½é…ç½®...")
    cfg = Config.fromfile(config_path)
    
    print("ğŸ—ï¸ æ„å»ºæ¨¡å‹...")
    # è®¾ç½®bf16ä»¥å‡å°‘å†…å­˜
    with torch.cuda.amp.autocast(dtype=torch.bfloat16):
        model = MODELS.build(cfg.model)
    
    print(f"ğŸ“¥ åŠ è½½DPO checkpoint: {checkpoint_path}")
    checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)
    state_dict = checkpoint.get('state_dict', checkpoint)
    
    # åŠ è½½LoRAæƒé‡
    model_state = model.state_dict()
    loaded, lora_cnt = 0, 0
    for k, v in state_dict.items():
        if k in model_state and v.shape == model_state[k].shape:
            model_state[k] = v
            loaded += 1
            if 'lora' in k.lower():
                lora_cnt += 1
    
    model.load_state_dict(model_state, strict=False)
    print(f"   åŠ è½½: {loaded} å‚æ•°, LoRA: {lora_cnt}")
    
    model.eval()
    model.to(torch.bfloat16)
    
    # åˆ†é…åˆ°å¤šGPU
    if torch.cuda.device_count() > 1:
        print(f"   åˆ†é…åˆ° {torch.cuda.device_count()} GPUs...")
        model = torch.nn.DataParallel(model)
    
    model.cuda()
    return model

def inference(model, image_path, tokenizer):
    """æ¨ç†å•å¼ å›¾ç‰‡"""
    image = Image.open(image_path).convert('RGB')
    prompt = "<image>\nPlease segment the blood vessel in this image."
    
    with torch.no_grad(), torch.cuda.amp.autocast(dtype=torch.bfloat16):
        # è·å–å®é™…æ¨¡å‹ï¼ˆDataParallelåŒ…è£…åï¼‰
        actual_model = model.module if hasattr(model, 'module') else model
        
        # ä½¿ç”¨Sa2VAModelçš„predictæ–¹æ³•
        if hasattr(actual_model, 'predict'):
            result = actual_model.predict(image, prompt)
        elif hasattr(actual_model, 'generate'):
            # å°è¯•ä½¿ç”¨generateæ–¹æ³•
            result = actual_model.generate(image, prompt, tokenizer)
        else:
            # ç›´æ¥ä½¿ç”¨chatæ¥å£
            mllm = actual_model.mllm.model
            response, _ = mllm.chat(
                tokenizer=tokenizer,
                pixel_values=None,  # ä¼šåœ¨å†…éƒ¨å¤„ç†
                question=prompt,
                generation_config=dict(max_new_tokens=256),
                history=[],
                return_history=True,
                IMG_CONTEXT_TOKEN='<IMG_CONTEXT>',
                IMG_START_TOKEN='<img>',
                IMG_END_TOKEN='</img>',
            )
            return {'text': response, 'masks': []}
    
    return {
        'text': result.get('prediction', '') if isinstance(result, dict) else str(result),
        'masks': result.get('prediction_masks', []) if isinstance(result, dict) else [],
    }

def main():
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--checkpoint', default='/home/ubuntu/Sa2VA/work_dirs/dpo_vessel_training/iter_1224.pth')
    parser.add_argument('--config', default='/home/ubuntu/Sa2VA/projects/sa2va/configs/sa2va_dpo_finetune_v3.py')
    parser.add_argument('--num_samples', type=int, default=10)
    args = parser.parse_args()
    
    print("=" * 60)
    print("ğŸ§ª DPOæ¨¡å‹æ¨ç†è¯„ä¼°")
    print("=" * 60)
    
    # åŠ è½½æ¨¡å‹
    model = load_model(args.config, args.checkpoint)
    
    # è·å–tokenizer
    actual_model = model.module if hasattr(model, 'module') else model
    tokenizer = actual_model.mllm.tokenizer
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    ann_path = '/home/ubuntu/Sa2VA/data/dpo_vessel/dpo_annotations.json'
    with open(ann_path) as f:
        annotations = json.load(f)
    
    # éšæœºé€‰æ‹©æ ·æœ¬
    import random
    random.seed(42)
    samples = random.sample(annotations, min(args.num_samples, len(annotations)))
    
    print(f"\nğŸ“¸ æµ‹è¯• {len(samples)} å¼ å›¾ç‰‡...")
    
    results = []
    for i, ann in enumerate(samples):
        img_path = os.path.join('/home/ubuntu/Sa2VA/data/dpo_vessel', ann['image'])
        if not os.path.exists(img_path):
            continue
        
        print(f"\n[{i+1}/{len(samples)}] {Path(img_path).name}")
        
        try:
            result = inference(model, img_path, tokenizer)
            has_mask = len(result['masks']) > 0
            print(f"   è¾“å‡º: {result['text'][:60]}...")
            print(f"   ç”Ÿæˆmask: {'âœ“' if has_mask else 'âœ—'}")
            
            results.append({
                'image': ann['image'],
                'has_mask': has_mask,
                'text': result['text'],
                'chosen_iou': ann['chosen_iou'],
                'rejected_iou': ann['rejected_iou'],
            })
        except Exception as e:
            print(f"   é”™è¯¯: {e}")
    
    # ç»Ÿè®¡
    success = sum(1 for r in results if r['has_mask'])
    print(f"\n" + "=" * 60)
    print(f"ğŸ“Š è¯„ä¼°ç»“æœ")
    print(f"   æˆåŠŸç”Ÿæˆmask: {success}/{len(results)}")
    print("=" * 60)

if __name__ == '__main__':
    main()
