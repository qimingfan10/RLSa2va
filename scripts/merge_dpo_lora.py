#!/usr/bin/env python3
"""
åˆå¹¶DPOè®­ç»ƒçš„LoRAæƒé‡åˆ°åŸºç¡€æ¨¡å‹
"""

import os
import sys
import torch
import argparse
from pathlib import Path

sys.path.insert(0, '/home/ubuntu/Sa2VA')

def merge_lora_weights(
    base_model_path: str,
    lora_checkpoint_path: str,
    output_path: str,
    config_path: str = None
):
    """åˆå¹¶LoRAæƒé‡åˆ°åŸºç¡€æ¨¡å‹"""
    from mmengine.config import Config
    from mmengine.registry import MODELS
    
    print("=" * 60)
    print("ğŸ”§ åˆå¹¶DPO LoRAæƒé‡")
    print("=" * 60)
    
    # åŠ è½½é…ç½®
    if config_path is None:
        config_path = '/home/ubuntu/Sa2VA/projects/sa2va/configs/sa2va_dpo_finetune_v3.py'
    
    print(f"\nğŸ“ åŠ è½½é…ç½®: {config_path}")
    cfg = Config.fromfile(config_path)
    
    # æ„å»ºæ¨¡å‹
    print("\nğŸ—ï¸ æ„å»ºæ¨¡å‹...")
    model = MODELS.build(cfg.model)
    
    # åŠ è½½LoRA checkpoint
    print(f"\nğŸ“¥ åŠ è½½LoRAæƒé‡: {lora_checkpoint_path}")
    checkpoint = torch.load(lora_checkpoint_path, map_location='cpu', weights_only=False)
    
    if 'state_dict' in checkpoint:
        state_dict = checkpoint['state_dict']
    else:
        state_dict = checkpoint
    
    print(f"   CheckpointåŒ…å« {len(state_dict)} ä¸ªå‚æ•°")
    
    # è¿‡æ»¤å¹¶åŠ è½½æƒé‡
    model_state = model.state_dict()
    loaded_keys = []
    skipped_keys = []
    
    for key, value in state_dict.items():
        if key in model_state:
            if value.shape == model_state[key].shape:
                model_state[key] = value
                loaded_keys.append(key)
            else:
                skipped_keys.append(f"{key}: {value.shape} vs {model_state[key].shape}")
        else:
            skipped_keys.append(f"{key}: not in model")
    
    print(f"   æˆåŠŸåŠ è½½: {len(loaded_keys)} ä¸ªå‚æ•°")
    print(f"   è·³è¿‡: {len(skipped_keys)} ä¸ªå‚æ•°")
    
    if skipped_keys[:5]:
        print("   è·³è¿‡çš„å‚æ•°ç¤ºä¾‹:")
        for k in skipped_keys[:5]:
            print(f"     - {k[:80]}")
    
    model.load_state_dict(model_state)
    
    # åˆå¹¶LoRAæƒé‡
    print("\nğŸ”€ åˆå¹¶LoRAæƒé‡...")
    
    try:
        # æ£€æŸ¥æ˜¯å¦æœ‰LoRA
        if hasattr(model.mllm, 'model') and hasattr(model.mllm.model, 'language_model'):
            llm = model.mllm.model.language_model
            if hasattr(llm, 'merge_and_unload'):
                print("   ä½¿ç”¨PEFT merge_and_unload...")
                model.mllm.model.language_model = llm.merge_and_unload()
                print("   âœ… LoRAåˆå¹¶æˆåŠŸ!")
            elif hasattr(llm, 'base_model'):
                print("   ä½¿ç”¨æ‰‹åŠ¨LoRAåˆå¹¶...")
                # æ‰‹åŠ¨åˆå¹¶
                for name, module in llm.named_modules():
                    if hasattr(module, 'merge'):
                        module.merge()
                print("   âœ… LoRAåˆå¹¶æˆåŠŸ!")
            else:
                print("   âš ï¸ æœªæ£€æµ‹åˆ°LoRAå±‚ï¼Œè·³è¿‡åˆå¹¶")
    except Exception as e:
        print(f"   âš ï¸ LoRAåˆå¹¶å¤±è´¥: {e}")
        print("   ç»§ç»­ä¿å­˜æœªåˆå¹¶çš„æƒé‡...")
    
    # ä¿å­˜åˆå¹¶åçš„æ¨¡å‹
    os.makedirs(output_path, exist_ok=True)
    
    # ä¿å­˜state_dict
    output_file = os.path.join(output_path, 'pytorch_model.bin')
    print(f"\nğŸ’¾ ä¿å­˜æ¨¡å‹: {output_file}")
    torch.save(model.state_dict(), output_file)
    
    # å¤åˆ¶tokenizer
    tokenizer_src = '/home/ubuntu/Sa2VA/tokenizer_with_special_tokens'
    if os.path.exists(tokenizer_src):
        import shutil
        for f in os.listdir(tokenizer_src):
            src = os.path.join(tokenizer_src, f)
            dst = os.path.join(output_path, f)
            if os.path.isfile(src):
                shutil.copy2(src, dst)
        print(f"   âœ… Tokenizerå·²å¤åˆ¶")
    
    print("\n" + "=" * 60)
    print("âœ… åˆå¹¶å®Œæˆ!")
    print(f"   è¾“å‡ºè·¯å¾„: {output_path}")
    print("=" * 60)
    
    return model

def quick_test(model, test_image_path):
    """å¿«é€Ÿæµ‹è¯•åˆå¹¶åçš„æ¨¡å‹"""
    print("\nğŸ§ª å¿«é€Ÿæµ‹è¯•...")
    
    from PIL import Image
    
    image = Image.open(test_image_path).convert('RGB')
    
    # ç®€å•æ£€æŸ¥æ¨¡å‹ç»“æ„
    print(f"   æ¨¡å‹ç±»å‹: {type(model)}")
    print(f"   MLLMç±»å‹: {type(model.mllm)}")
    
    # æ£€æŸ¥å‚æ•°æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"   æ€»å‚æ•°: {total_params:,}")
    print(f"   å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--checkpoint', type=str, 
                       default='/home/ubuntu/Sa2VA/work_dirs/dpo_vessel_training/iter_1224.pth')
    parser.add_argument('--output', type=str,
                       default='/home/ubuntu/Sa2VA/work_dirs/dpo_vessel_training/merged_model')
    parser.add_argument('--config', type=str,
                       default='/home/ubuntu/Sa2VA/projects/sa2va/configs/sa2va_dpo_finetune_v3.py')
    parser.add_argument('--test', action='store_true', help='è¿è¡Œå¿«é€Ÿæµ‹è¯•')
    args = parser.parse_args()
    
    model = merge_lora_weights(
        base_model_path=None,  # ä»configè·å–
        lora_checkpoint_path=args.checkpoint,
        output_path=args.output,
        config_path=args.config
    )
    
    if args.test:
        test_image = '/home/ubuntu/Sa2VA/data/dpo_vessel/images/An Cong Xue(0000932433)_1-3_1_051C3E6A_frame_000011.jpg'
        if os.path.exists(test_image):
            quick_test(model, test_image)

if __name__ == '__main__':
    main()
