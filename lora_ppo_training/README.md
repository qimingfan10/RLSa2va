# Sa2VA LoRA + PPOå¾®è°ƒ

**ç›®æ ‡**: é€šè¿‡å¼ºåŒ–å­¦ä¹ ç›´æ¥ä¼˜åŒ–DiceæŒ‡æ ‡ï¼Œä»0.78æå‡åˆ°0.87+

## ğŸ¯ æ–¹æ¡ˆæ¦‚è¿°

### æ ¸å¿ƒæ€æƒ³
1. **LoRAé€‚é…**: åªè®­ç»ƒ0.5%å‚æ•°ï¼ˆ~130Mï¼‰ï¼Œå†»ç»“Sa2VAä¸»å¹²
2. **PPOä¼˜åŒ–**: ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ç›´æ¥ä¼˜åŒ–Dice/Recall
3. **å¤šç›®æ ‡å¥–åŠ±**: ç»“åˆDice + Recall + æ‹“æ‰‘è¿é€šæ€§ + é•¿åº¦çº¦æŸ

### æŠ€æœ¯è·¯çº¿
```
Sa2VA-26B (å†»ç»“)
    â†“
LoRAé€‚é…å™¨ (å¯è®­ç»ƒ)
    â†“
é¢„æµ‹Mask
    â†“
å¤šç›®æ ‡å¥–åŠ±å‡½æ•°
    â†“
PPOä¼˜åŒ–
```

---

## ğŸ“ æ–‡ä»¶ç»“æ„

```
lora_ppo_training/
â”œâ”€â”€ reward_functions.py      # å¥–åŠ±å‡½æ•°ï¼ˆæ ¸å¿ƒï¼‰
â”œâ”€â”€ lora_config.py            # LoRAé…ç½®
â”œâ”€â”€ data_loader.py            # æ•°æ®åŠ è½½
â”œâ”€â”€ train_lora_ppo.py         # ä¸»è®­ç»ƒè„šæœ¬
â”œâ”€â”€ run_lora_ppo.sh           # è¿è¡Œè„šæœ¬
â”œâ”€â”€ README.md                 # æœ¬æ–‡ä»¶
â””â”€â”€ output/                   # è¾“å‡ºç›®å½•
    â”œâ”€â”€ sa2va_lora_ppo_*/     # è®­ç»ƒè¾“å‡º
    â”‚   â”œâ”€â”€ best_lora/        # æœ€ä½³æ¨¡å‹
    â”‚   â”œâ”€â”€ final_lora/       # æœ€ç»ˆæ¨¡å‹
    â”‚   â””â”€â”€ training_info.json
    â””â”€â”€ train_*.log           # æ—¥å¿—æ–‡ä»¶
```

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
pip install peft>=0.6.0
pip install scikit-image
pip install wandb  # å¯é€‰ï¼Œç”¨äºå®éªŒè¿½è¸ª
```

### 2. å¿«é€Ÿæµ‹è¯•ï¼ˆæ¨èå…ˆè¿è¡Œï¼‰

```bash
bash run_lora_ppo.sh quick
```

**é…ç½®**:
- è®­ç»ƒæ ·æœ¬: 50å¼ 
- éªŒè¯æ ·æœ¬: 20å¼ 
- è®­ç»ƒè½®æ•°: 1 epoch
- é¢„è®¡æ—¶é—´: ~30åˆ†é’Ÿ

**ç›®çš„**: éªŒè¯ä»£ç æ˜¯å¦æ­£å¸¸è¿è¡Œï¼Œæ— OOMé”™è¯¯

### 3. å®Œæ•´è®­ç»ƒ

```bash
bash run_lora_ppo.sh full
```

**é…ç½®**:
- è®­ç»ƒæ ·æœ¬: 1000å¼ 
- éªŒè¯æ ·æœ¬: 100å¼ 
- è®­ç»ƒè½®æ•°: 3 epochs
- é¢„è®¡æ—¶é—´: ~24-48å°æ—¶

---

## âš™ï¸ æ ¸å¿ƒç»„ä»¶

### 1. å¥–åŠ±å‡½æ•° (`reward_functions.py`)

#### MultiObjectiveReward (é»˜è®¤)
```python
reward = 0.5 * dice_reward +
         0.2 * recall_bonus +
         0.2 * topology_reward +
         0.1 * length_penalty
```

**å„é¡¹è¯´æ˜**:
- **Diceå¥–åŠ±** (50%): ä¸»è¦ä¼˜åŒ–ç›®æ ‡
- **Recallå¥–åŠ±** (20%): é’ˆå¯¹æ€§æå‡Recall (ç›®æ ‡0.85)
- **æ‹“æ‰‘å¥–åŠ±** (20%): ä¿è¯è¡€ç®¡è¿ç»­æ€§ï¼Œå‡å°‘æ–­è£‚
- **é•¿åº¦æƒ©ç½š** (10%): çº¦æŸè¡€ç®¡æ€»é•¿åº¦æ¥è¿‘GT

#### SimpleDiceReward
ä»…ä½¿ç”¨Diceä½œä¸ºå¥–åŠ±ï¼Œæœ€ç®€å•

#### RecallFocusedReward
ä¸“æ³¨äºæå‡Recallï¼Œæƒé‡70%

### 2. LoRAé…ç½® (`lora_config.py`)

**é¢„è®¾é…ç½®**:
```python
'medium': {
    'lora_rank': 32,
    'lora_alpha': 64,
    'target_modules': ["q_proj", "k_proj", "v_proj", "o_proj"]
}
```

**å¯è®­ç»ƒå‚æ•°**: ~130M (0.5% of 26B)

### 3. è®­ç»ƒå™¨ (`train_lora_ppo.py`)

**æ ¸å¿ƒæµç¨‹**:
```python
for epoch in epochs:
    for batch in dataloader:
        # 1. ä½¿ç”¨å½“å‰ç­–ç•¥é¢„æµ‹
        pred_mask = model.predict(image)
        
        # 2. è®¡ç®—å¥–åŠ±
        reward = reward_function(pred_mask, gt_mask)
        
        # 3. æ›´æ–°LoRAå‚æ•°
        loss = -reward
        loss.backward()
        optimizer.step()
```

**æ³¨æ„**: è¿™æ˜¯ç®€åŒ–ç‰ˆæœ¬ï¼Œå®Œæ•´çš„PPOå®ç°æ›´å¤æ‚

---

## ğŸ“Š ç›‘æ§æŒ‡æ ‡

### è®­ç»ƒæŒ‡æ ‡
- `train/reward`: è®­ç»ƒå¥–åŠ±
- `train/dice`: è®­ç»ƒDiceåˆ†æ•°
- `train/recall`: è®­ç»ƒRecall
- `train/precision`: è®­ç»ƒPrecision

### éªŒè¯æŒ‡æ ‡
- `val/dice`: éªŒè¯Dice â­ (ä¸»è¦å…³æ³¨)
- `val/recall`: éªŒè¯Recall
- `val/precision`: éªŒè¯Precision

### ç›®æ ‡
- Dice: 0.87+
- Recall: 0.85+
- Precision: 0.85+

---

## ğŸ”§ è¶…å‚æ•°è°ƒä¼˜

### LoRAå‚æ•°
```bash
# æ›´å¤§çš„rankï¼ˆæ›´å¤šå‚æ•°ï¼Œæ›´å¼ºè¡¨è¾¾åŠ›ï¼‰
--lora_rank 64 --lora_alpha 128

# æ›´å°çš„rankï¼ˆæ›´å°‘å‚æ•°ï¼Œæ›´å¿«è®­ç»ƒï¼‰
--lora_rank 16 --lora_alpha 32
```

### å­¦ä¹ ç‡
```bash
# æ¿€è¿›ï¼ˆå¿«é€Ÿæ”¶æ•›ï¼Œå¯èƒ½ä¸ç¨³å®šï¼‰
--learning_rate 1e-4

# ä¿å®ˆï¼ˆç¨³å®šè®­ç»ƒï¼Œæ”¶æ•›æ…¢ï¼‰
--learning_rate 1e-5
```

### å¥–åŠ±å‡½æ•°
```bash
# ä¸“æ³¨Recall
--reward_type recall_focused

# ä¸“æ³¨Dice
--reward_type simple_dice

# å¹³è¡¡ä¼˜åŒ–ï¼ˆæ¨èï¼‰
--reward_type multi_objective
```

### Recallç›®æ ‡
```bash
# æ›´é«˜çš„Recallç›®æ ‡
--recall_target 0.90

# æ”¾æ¾Recallè¦æ±‚
--recall_target 0.80
```

---

## ğŸ“ ä½¿ç”¨æŠ€å·§

### 1. æ¸è¿›å¼è®­ç»ƒ

**Step 1**: Quickæ¨¡å¼éªŒè¯ (50å¼ , 1 epoch)
```bash
bash run_lora_ppo.sh quick
```

**Step 2**: ä¸­ç­‰è§„æ¨¡ (200å¼ , 2 epochs)
```bash
bash run_lora_ppo.sh full
# ä½†ä¿®æ”¹è„šæœ¬ä¸­çš„ MAX_TRAIN_SAMPLES=200
```

**Step 3**: å®Œæ•´è®­ç»ƒ (1000å¼ , 3 epochs)
```bash
bash run_lora_ppo.sh full
```

### 2. Curriculum Learning

**é˜¶æ®µ1**: è®­ç»ƒç®€å•æ ·æœ¬ï¼ˆå¤§è¡€ç®¡ï¼‰
```python
# ä¿®æ”¹data_loader.pyï¼ŒæŒ‰è¡€ç®¡é¢ç§¯æ’åº
# å…ˆè®­ç»ƒå¤§è¡€ç®¡å›¾åƒ
```

**é˜¶æ®µ2**: åŠ å…¥ä¸­ç­‰å¤æ‚åº¦æ ·æœ¬

**é˜¶æ®µ3**: åŠ å…¥å›°éš¾æ ·æœ¬ï¼ˆç»†å°è¡€ç®¡ï¼‰

### 3. å¥–åŠ±å¡‘å½¢ï¼ˆReward Shapingï¼‰

**åŠ¨æ€æƒé‡è°ƒæ•´**:
```python
# åœ¨trainingä¸­
if current_recall < 0.80:
    recall_weight = 0.4  # æé«˜Recallæƒé‡
else:
    recall_weight = 0.2  # æ¢å¤æ­£å¸¸
```

---

## ğŸ› æ•…éšœæ’é™¤

### é—®é¢˜1: GPUå†…å­˜ä¸è¶³ (OOM)

**è§£å†³æ–¹æ¡ˆ**:
```bash
# 1. é™ä½LoRA rank
--lora_rank 16

# 2. ä½¿ç”¨æ›´å°çš„æ¨¡å‹ç²¾åº¦
--use_bf16

# 3. å‡å°‘batch sizeï¼ˆå·²ç»æ˜¯1äº†ï¼‰
# 4. ä½¿ç”¨gradient checkpointing
```

### é—®é¢˜2: è®­ç»ƒä¸æ”¶æ•›

**è§£å†³æ–¹æ¡ˆ**:
```bash
# 1. é™ä½å­¦ä¹ ç‡
--learning_rate 1e-5

# 2. å¢åŠ warmupæ­¥æ•°
# 3. ä½¿ç”¨ç®€å•çš„å¥–åŠ±å‡½æ•°
--reward_type simple_dice

# 4. æ£€æŸ¥æ•°æ®æ˜¯å¦æœ‰é—®é¢˜
```

### é—®é¢˜3: Recallæå‡ä½†Precisionä¸‹é™

**è§£å†³æ–¹æ¡ˆ**:
```bash
# è°ƒæ•´å¥–åŠ±æƒé‡ï¼Œå¢åŠ Precisionçº¦æŸ
--dice_weight 0.6 --recall_weight 0.1

# æˆ–è€…åœ¨å¥–åŠ±å‡½æ•°ä¸­æ·»åŠ Precisionæƒ©ç½š
```

### é—®é¢˜4: é¢„æµ‹å¤±è´¥

**æ£€æŸ¥**:
```bash
# æŸ¥çœ‹æ—¥å¿—
tail -100 output/train_*.log | grep "é¢„æµ‹å¤±è´¥"

# ç¡®è®¤promptæ ¼å¼
--prompt "<image>\nPlease segment the blood vessel."
```

---

## ğŸ“ˆ é¢„æœŸç»“æœ

### Quickæ¨¡å¼
```yaml
è®­ç»ƒæ—¶é—´: ~30åˆ†é’Ÿ
é¢„æœŸDice: 0.78-0.80
é¢„æœŸRecall: 0.74-0.76
ç»“è®º: éªŒè¯ä»£ç å¯è¡Œæ€§
```

### Fullæ¨¡å¼
```yaml
è®­ç»ƒæ—¶é—´: ~24-48å°æ—¶
é¢„æœŸDice: 0.85-0.87+
é¢„æœŸRecall: 0.83-0.85+
ç»“è®º: è¾¾åˆ°æˆ–æ¥è¿‘ç›®æ ‡
```

---

## ğŸ¯ ä¸‹ä¸€æ­¥

### è®­ç»ƒå®Œæˆå

1. **è¯„ä¼°æ¨¡å‹**
```bash
python evaluate_lora_model.py \
    --base_model /path/to/sa2va_vessel_hf \
    --lora_weights output/sa2va_lora_ppo_*/best_lora \
    --test_data /path/to/test_data
```

2. **å¯¹æ¯”å®éªŒ**
- ä¸å®éªŒä¸€ã€äºŒã€ä¸‰å¯¹æ¯”
- é€‰æ‹©æœ€ä¼˜æ–¹æ¡ˆ

3. **éƒ¨ç½²**
```bash
# åˆå¹¶LoRAæƒé‡ï¼ˆå¯é€‰ï¼‰
python merge_lora.py \
    --base_model /path/to/sa2va_vessel_hf \
    --lora_weights output/sa2va_lora_ppo_*/best_lora \
    --output_model /path/to/sa2va_merged
```

---

## ğŸ“š æŠ€æœ¯å‚è€ƒ

### æ ¸å¿ƒè®ºæ–‡
1. **LoRA**: "LoRA: Low-Rank Adaptation of Large Language Models" (ICLR 2022)
2. **PPO**: "Proximal Policy Optimization Algorithms" (2017)
3. **RLHF**: "Training language models to follow instructions" (NeurIPS 2022)

### å…³é”®æ¦‚å¿µ
- **Low-Rank Adaptation**: é€šè¿‡ä½ç§©çŸ©é˜µè¿‘ä¼¼å‡å°‘å¯è®­ç»ƒå‚æ•°
- **Policy Gradient**: ç›´æ¥ä¼˜åŒ–ç­–ç•¥çš„æ¢¯åº¦æ–¹æ³•
- **Reward Shaping**: è®¾è®¡å¥–åŠ±å‡½æ•°å¼•å¯¼å­¦ä¹ æ–¹å‘

---

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **è®­ç»ƒæ—¶é—´**: Fullæ¨¡å¼éœ€è¦24-48å°æ—¶ï¼Œè¯·ç¡®ä¿æœ‰è¶³å¤Ÿæ—¶é—´
2. **GPUéœ€æ±‚**: æ¨è4Ã—A100æˆ–2Ã—A100+DeepSpeed
3. **æ•°æ®è´¨é‡**: æ ‡æ³¨è´¨é‡ç›´æ¥å½±å“è®­ç»ƒæ•ˆæœ
4. **è¶…å‚æ•æ„Ÿ**: LoRAå’Œå­¦ä¹ ç‡éœ€è¦ä»”ç»†è°ƒä¼˜
5. **ç®€åŒ–å®ç°**: å½“å‰æ˜¯ç®€åŒ–ç‰ˆPPOï¼Œå®Œæ•´ç‰ˆéœ€è¦æ›´å¤æ‚é€»è¾‘

---

## ğŸ‰ æœŸå¾…æ•ˆæœ

é€šè¿‡LoRA + PPOå¾®è°ƒï¼Œé¢„æœŸè¾¾åˆ°ï¼š
- âœ… Dice: 0.87+ (ä»0.78æå‡11.5%)
- âœ… Recall: 0.85+ (ä»0.74æå‡14.9%)
- âœ… è¡€ç®¡è¿ç»­æ€§æ”¹å–„ï¼ˆæ‹“æ‰‘å¥–åŠ±ï¼‰
- âœ… ç»†å°è¡€ç®¡å®Œæ•´æ£€å‡º

**æˆåŠŸæ ‡å‡†**: Dice â‰¥ 0.87 ä¸” Recall â‰¥ 0.85

---

**åˆ›å»ºæ—¶é—´**: 2025-11-29  
**çŠ¶æ€**: âœ… ä»£ç å°±ç»ªï¼Œå‡†å¤‡è®­ç»ƒ  
**å»ºè®®**: å…ˆè¿è¡ŒQuickæ¨¡å¼éªŒè¯
