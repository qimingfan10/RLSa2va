# ğŸ¯ Sa2VA DPO è®­ç»ƒæŒ‡å—

## æ¦‚è¿°

DPO (Direct Preference Optimization) æ˜¯ä¸€ç§æ— éœ€Criticç½‘ç»œçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œç‰¹åˆ«é€‚åˆå¤§æ¨¡å‹å¾®è°ƒã€‚

### PPO vs DPO å¯¹æ¯”

| ç‰¹æ€§ | PPO | DPO |
|------|-----|-----|
| éœ€è¦Critic | âœ… éœ€è¦è®­ç»ƒUNet | âŒ ä¸éœ€è¦ |
| æ˜¾å­˜éœ€æ±‚ | ~48GB | ~24GB |
| è®­ç»ƒç¨³å®šæ€§ | è¾ƒå·® | è¾ƒå¥½ |
| BFloat16å…¼å®¹ | âŒ æœ‰é—®é¢˜ | âœ… å®Œç¾å…¼å®¹ |
| æ•°æ®æ ¼å¼ | (state, action, reward) | (chosen, rejected) |

---

## ğŸ”§ æ–‡ä»¶ç»“æ„

```
/home/ubuntu/Sa2VA/
â”œâ”€â”€ projects/sa2va/
â”‚   â”œâ”€â”€ configs/
â”‚   â”‚   â””â”€â”€ sa2va_dpo_vessel.py      # DPOè®­ç»ƒé…ç½®
â”‚   â”œâ”€â”€ datasets/
â”‚   â”‚   â””â”€â”€ dpo_vessel_dataset.py    # DPOæ•°æ®é›†ç±»
â”‚   â””â”€â”€ models/
â”‚       â””â”€â”€ sa2va_dpo_model.py       # DPOæ¨¡å‹wrapper
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ generate_dpo_dataset.py      # åå¥½å¯¹ç”Ÿæˆè„šæœ¬
â”œâ”€â”€ train_dpo_vessel.sh              # è®­ç»ƒå¯åŠ¨è„šæœ¬
â””â”€â”€ data/
    â””â”€â”€ dpo_vessel/                  # DPOæ•°æ®é›†
        â”œâ”€â”€ dpo_annotations.json     # åå¥½å¯¹æ ‡æ³¨
        â””â”€â”€ masks/                   # ç”Ÿæˆçš„mask
```

---

## ğŸ“Š æ•°æ®é›†æ ¼å¼

### Annotations JSONæ ¼å¼

```json
[
  {
    "image": "images/image_001.jpg",
    "chosen_mask": "masks/image_001_chosen_0_1.png",
    "rejected_mask": "masks/image_001_rejected_0_1.png",
    "chosen_iou": 0.85,
    "rejected_iou": 0.62,
    "iou_gap": 0.23,
    "prompt": "<image>Please segment the blood vessels."
  },
  ...
]
```

### å­—æ®µè¯´æ˜

| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| `image` | str | åŸå§‹å›¾åƒè·¯å¾„ |
| `chosen_mask` | str | èƒœè€…maskè·¯å¾„ï¼ˆIoUæ›´é«˜ï¼‰|
| `rejected_mask` | str | è´¥è€…maskè·¯å¾„ï¼ˆIoUæ›´ä½ï¼‰|
| `chosen_iou` | float | èƒœè€…IoUå€¼ |
| `rejected_iou` | float | è´¥è€…IoUå€¼ |
| `iou_gap` | float | IoUå·®è· |
| `prompt` | str | è¾“å…¥prompt |

---

## ğŸš€ ä½¿ç”¨æ­¥éª¤

### Step 1: ç”ŸæˆDPOæ•°æ®é›†

#### æ–¹å¼A: ä½¿ç”¨æ¨¡å‹ç”Ÿæˆå¤šæ ·åŒ–é¢„æµ‹

```bash
cd /home/ubuntu/Sa2VA

python scripts/generate_dpo_dataset.py \
    --mode generate \
    --images_dir /home/ubuntu/Sa2VA/data/merged_vessel_data/images \
    --gt_dir /home/ubuntu/Sa2VA/data/merged_vessel_data/masks \
    --output_dir /home/ubuntu/Sa2VA/data/dpo_vessel \
    --model_path /home/ubuntu/Sa2VA/models/sa2va_vessel_hf \
    --num_samples 5 \
    --min_iou_gap 0.05
```

#### æ–¹å¼B: ä»å·²æœ‰çš„å¤šç§é¢„æµ‹ç»“æœç”Ÿæˆ

å¦‚æœæ‚¨å·²ç»æœ‰å¤šä¸ªæ¨¡å‹/æ–¹æ³•çš„é¢„æµ‹ç»“æœï¼š

```bash
python scripts/generate_dpo_dataset.py \
    --mode from_predictions \
    --images_dir /path/to/predictions_dir \
    --gt_dir /path/to/gt_masks \
    --output_dir /home/ubuntu/Sa2VA/data/dpo_vessel \
    --min_iou_gap 0.05
```

### Step 2: å¯åŠ¨DPOè®­ç»ƒ

```bash
cd /home/ubuntu/Sa2VA
bash train_dpo_vessel.sh
```

æˆ–æ‰‹åŠ¨è¿è¡Œï¼š

```bash
# å•GPU
python tools/train.py projects/sa2va/configs/sa2va_dpo_vessel.py \
    --work-dir work_dirs/dpo_vessel_training

# å¤šGPU (4å¡)
torchrun --nproc_per_node=4 \
    tools/train.py projects/sa2va/configs/sa2va_dpo_vessel.py \
    --work-dir work_dirs/dpo_vessel_training \
    --launcher pytorch
```

### Step 3: è¯„ä¼°ç»“æœ

è®­ç»ƒå®Œæˆåï¼Œæ¨¡å‹ä¼šä¿å­˜åœ¨ `work_dirs/dpo_vessel_training/`

```bash
# æ¨ç†æµ‹è¯•
python tools/test.py \
    projects/sa2va/configs/sa2va_dpo_vessel.py \
    work_dirs/dpo_vessel_training/iter_XXX.pth \
    --work-dir work_dirs/dpo_vessel_eval
```

---

## âš™ï¸ å…³é”®é…ç½®å‚æ•°

### DPOè¶…å‚æ•° (sa2va_dpo_vessel.py)

```python
# DPOæ ¸å¿ƒå‚æ•°
beta = 0.1              # æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶åå¥½å¼ºåº¦
                        # å° â†’ æ›´æ¿€è¿›åœ°åå¥½chosen
                        # å¤§ â†’ æ›´ä¿å®ˆ

label_smoothing = 0.0   # æ ‡ç­¾å¹³æ»‘ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ

# å­¦ä¹ ç‡
lr = 5e-6               # DPOé€šå¸¸ä½¿ç”¨æ›´å°çš„å­¦ä¹ ç‡

# Epoch
max_epochs = 2          # DPOæ”¶æ•›å¿«ï¼Œé€šå¸¸1-3ä¸ªepochè¶³å¤Ÿ
```

### æ•°æ®é›†å‚æ•°

```python
min_iou_gap = 0.05      # æœ€å°IoUå·®è·é˜ˆå€¼
                        # å¤ªå° â†’ åå¥½ä¿¡å·å¼±
                        # å¤ªå¤§ â†’ æ•°æ®é‡å°‘
```

### DeepSpeedé…ç½®

```python
strategy = dict(
    type='DeepSpeedStrategy',
    zero_optimization=dict(
        stage=2,        # DPOå¯ç”¨ZeRO-2ï¼ˆæ— éœ€Criticï¼Œæ¨¡å‹æ›´å°ï¼‰
        offload_optimizer=dict(device='cpu'),
    ),
    bf16=dict(enabled=True),  # âœ… BFloat16å…¼å®¹
)
```

---

## ğŸ“ˆ DPO Lossè¯¦è§£

### æ•°å­¦å…¬å¼

**å®Œæ•´ç‰ˆï¼ˆæœ‰reference modelï¼‰:**
```
L_DPO = -E[log Ïƒ(Î² * ((log Ï€(y_w|x) - log Ï€_ref(y_w|x)) - 
                       (log Ï€(y_l|x) - log Ï€_ref(y_l|x))))]
```

**ç®€åŒ–ç‰ˆï¼ˆLoRAæ¨¡å¼ï¼Œæ— reference modelï¼‰:**
```
L_DPO = -E[log Ïƒ(Î² * (log Ï€(y_w|x) - log Ï€(y_l|x)))]
```

å…¶ä¸­ï¼š
- `Ï€`: å½“å‰ç­–ç•¥ï¼ˆæ­£åœ¨è®­ç»ƒçš„æ¨¡å‹ï¼‰
- `Ï€_ref`: å‚è€ƒç­–ç•¥ï¼ˆå†»ç»“çš„æ¨¡å‹ï¼‰
- `y_w`: chosenï¼ˆèƒœè€…ï¼‰
- `y_l`: rejectedï¼ˆè´¥è€…ï¼‰
- `Î²`: æ¸©åº¦å‚æ•°

### å¯¹äºåˆ†å‰²ä»»åŠ¡çš„é€‚é…

```python
# è®¡ç®—maskçš„logæ¦‚ç‡
log_prob = sum(y_i * log(p_i) + (1-y_i) * log(1-p_i)) / N

# y_i: GT maskçš„ç¬¬iä¸ªåƒç´ 
# p_i: é¢„æµ‹çš„ç¬¬iä¸ªåƒç´ æ¦‚ç‡
# N: åƒç´ æ€»æ•°
```

---

## ğŸ”„ è®­ç»ƒç›‘æ§

### å…³é”®æŒ‡æ ‡

| æŒ‡æ ‡ | å«ä¹‰ | ç†æƒ³è¶‹åŠ¿ |
|------|------|----------|
| `dpo_loss` | DPOæŸå¤± | ä¸‹é™ |
| `accuracy` | æ¨¡å‹åå¥½chosençš„å‡†ç¡®ç‡ | ä¸Šå‡åˆ°~0.7-0.9 |
| `margin` | chosenå’Œrejected rewardå·®è· | ä¸Šå‡ |
| `chosen_rewards` | chosençš„éšå¼å¥–åŠ± | ä¸Šå‡ |
| `rejected_rewards` | rejectedçš„éšå¼å¥–åŠ± | ç¨³å®šæˆ–ä¸‹é™ |

### TensorBoardæŸ¥çœ‹

```bash
tensorboard --logdir work_dirs/dpo_vessel_training
```

---

## ğŸ’¡ æŠ€å·§ä¸å»ºè®®

### 1. æ•°æ®è´¨é‡æ¯”æ•°é‡é‡è¦

- IoUå·®è·è¦è¶³å¤Ÿå¤§ï¼ˆæ¨è > 0.1ï¼‰
- chosenç¡®å®è¦æ¯”rejectedå¥½ï¼ˆäººå·¥æ£€æŸ¥ï¼‰
- é¿å…å™ªå£°æ ‡ç­¾

### 2. Betaè°ƒæ•´

```python
# å¦‚æœæ¨¡å‹ä¸å­¦ä¹ åå¥½ â†’ å¢å¤§beta
beta = 0.2

# å¦‚æœæ¨¡å‹è¿‡åº¦åå¥½chosenï¼ˆå´©æºƒï¼‰â†’ å‡å°beta
beta = 0.05
```

### 3. å­¦ä¹ ç‡

DPOå¯¹å­¦ä¹ ç‡æ•æ„Ÿï¼Œæ¨èä»å°å¼€å§‹ï¼š
```python
lr = 1e-6  # å¼€å§‹
lr = 5e-6  # æ­£å¸¸
lr = 1e-5  # è¾ƒå¤§
```

### 4. LoRA vs Full Fine-tuning

- **LoRAï¼ˆæ¨èï¼‰**: æ˜¾å­˜å°‘ï¼Œä¸éœ€è¦reference model
- **Full**: éœ€è¦reference modelï¼Œæ˜¾å­˜ç¿»å€

---

## â“ å¸¸è§é—®é¢˜

### Q1: ä¸ºä»€ä¹ˆä¸éœ€è¦Criticï¼Ÿ

DPOç›´æ¥ä»åå¥½å¯¹å­¦ä¹ ï¼Œä¸éœ€è¦æ˜¾å¼ä¼°è®¡value/rewardã€‚æ•°å­¦ä¸Šè¯æ˜DPOç­‰ä»·äºå¸¦çº¦æŸçš„reward maximizationã€‚

### Q2: å¦‚ä½•é€‰æ‹©betaï¼Ÿ

- é»˜è®¤0.1æ˜¯ä¸ªå¥½èµ·ç‚¹
- å¦‚æœè®­ç»ƒä¸ç¨³å®šï¼Œå°è¯•0.05
- å¦‚æœåå¥½å­¦ä¹ å¤ªæ…¢ï¼Œå°è¯•0.2

### Q3: éœ€è¦å¤šå°‘åå¥½å¯¹ï¼Ÿ

- æœ€å°‘ï¼š100-500å¯¹ï¼ˆå¯ä»¥å¼€å§‹è®­ç»ƒï¼‰
- æ¨èï¼š1000-5000å¯¹
- æ›´å¤šï¼šå¦‚æœIoUå·®è·è¾ƒå°

### Q4: DPOä¼šè¿‡æ‹Ÿåˆå—ï¼Ÿ

DPOæ¯”PPOæ›´ä¸å®¹æ˜“è¿‡æ‹Ÿåˆï¼Œä½†ä»éœ€æ³¨æ„ï¼š
- ä½¿ç”¨early stopping
- ç›‘æ§validation loss
- ä¸è¦è®­ç»ƒå¤ªå¤šepoch

---

## ğŸ“ è¾“å‡ºæ–‡ä»¶

```
work_dirs/dpo_vessel_training/
â”œâ”€â”€ iter_200.pth         # æ£€æŸ¥ç‚¹
â”œâ”€â”€ iter_400.pth
â”œâ”€â”€ iter_XXX.pth         # æœ€ç»ˆæ¨¡å‹
â”œâ”€â”€ 20231201_XXXXXX/     # æ—¥å¿—ç›®å½•
â”‚   â”œâ”€â”€ vis_data/
â”‚   â””â”€â”€ XXXXXX.log
â””â”€â”€ config.py            # ä¿å­˜çš„é…ç½®
```

---

## âœ… æ€»ç»“

DPOæ˜¯Sa2VAå¼ºåŒ–å­¦ä¹ å¾®è°ƒçš„æœ€ä½³é€‰æ‹©ï¼š

1. âœ… **ä¸éœ€è¦Critic** - æ˜¾å­˜å‡åŠ
2. âœ… **BFloat16å…¼å®¹** - å¤ç”¨MMEngineæ¡†æ¶
3. âœ… **è®­ç»ƒç¨³å®š** - æ— éœ€å¤æ‚çš„PPOè°ƒå‚
4. âœ… **æ•°æ®é«˜æ•ˆ** - ç›´æ¥ä»IoUæ¯”è¾ƒå­¦ä¹ 

**ä¸‹ä¸€æ­¥**: è¿è¡Œ `bash train_dpo_vessel.sh` å¼€å§‹è®­ç»ƒï¼
