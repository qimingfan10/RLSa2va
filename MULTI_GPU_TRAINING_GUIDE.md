# ğŸš€ 4-GPU LoRAè®­ç»ƒé…ç½®

**æ—¶é—´**: 2025-11-30 13:42  
**çŠ¶æ€**: ğŸŸ¢ å·²å¯åŠ¨4-GPUè®­ç»ƒ

---

## ğŸ“Š é…ç½®æ¦‚è§ˆ

### ç¡¬ä»¶é…ç½®
```yaml
GPUs: 4 Ã— RTX 3090 (24GB each)
æ€»æ˜¾å­˜: 96GB
ç­–ç•¥: DeepSpeed ZeRO-2 + CPU Offload
```

### è®­ç»ƒé…ç½®
```yaml
Batché…ç½®:
  Per-device batch size: 1
  Gradient accumulation: 2
  æœ‰æ•ˆbatch size: 4 GPU Ã— 1 Ã— 2 = 8

æ•°æ®:
  æ ·æœ¬æ•°: 1220
  é‡å¤æ¬¡æ•°: 5
  æ€»è®­ç»ƒæ ·æœ¬: 6100

Epochs: 10
å­¦ä¹ ç‡: 1e-4
```

### DeepSpeedé…ç½®

**æ–‡ä»¶**: `deepspeed_zero2_offload.json`

```json
{
  "zero_optimization": {
    "stage": 2,  // ZeRO-2: åˆ†ç‰‡optimizer states
    "offload_optimizer": {
      "device": "cpu",  // OptimizerçŠ¶æ€å¸è½½åˆ°CPU
      "pin_memory": true
    }
  },
  "bf16": {
    "enabled": true  // ä½¿ç”¨BF16æ··åˆç²¾åº¦
  }
}
```

**ä¼˜åŠ¿**:
- å‡å°‘GPUæ˜¾å­˜å ç”¨50%+
- Optimizer stateså­˜å‚¨åœ¨CPU
- æ”¯æŒæ›´å¤§çš„æ¨¡å‹å’Œbatch size

---

## ğŸ”§ è®­ç»ƒå‘½ä»¤

### å¯åŠ¨è®­ç»ƒ
```bash
cd /home/ubuntu/Sa2VA

CUDA_VISIBLE_DEVICES=0,1,2,3 \
DEEPSPEED=deepspeed_zero2_offload \
nohup bash tools/dist.sh train \
  projects/sa2va/configs/sa2va_vessel_lora_finetune.py 4 \
  > vessel_lora_training_4gpu.log 2>&1 &
```

### ç›‘æ§å‘½ä»¤
```bash
# æŸ¥çœ‹æ—¥å¿—
tail -f vessel_lora_training_4gpu.log

# æŸ¥çœ‹loss
grep "loss" vessel_lora_training_4gpu.log | grep "iter"

# æŸ¥çœ‹GPUä½¿ç”¨
watch -n 1 nvidia-smi

# æŸ¥çœ‹è¿›ç¨‹
ps aux | grep train.py
```

### åœæ­¢è®­ç»ƒ
```bash
# æ¸©æŸ”åœæ­¢
pkill -f train.py

# å¼ºåˆ¶åœæ­¢
pkill -9 -f train.py
```

---

## ğŸ“ˆ é¢„æœŸæ€§èƒ½

### æ˜¾å­˜ä½¿ç”¨
```yaml
å•GPUæ˜¾å­˜å ç”¨:
  æ¨¡å‹æƒé‡: ~8GB
  Activations: ~4GB
  æ¢¯åº¦: ~2GB (ZeRO-2åˆ†ç‰‡)
  Optimizer: å¸è½½åˆ°CPU
  æ€»è®¡: ~14GB/24GB âœ…

4 GPUæ€»æ˜¾å­˜: ~56GB/96GB
```

### è®­ç»ƒé€Ÿåº¦
```yaml
å•æ­¥æ—¶é—´: ~3-5ç§’
æ¯epoch: ~40-60åˆ†é’Ÿ
æ€»æ—¶é—´ (10 epochs): 7-10å°æ—¶

å¯¹æ¯”å•GPU:
  å•GPU: 20-30å°æ—¶
  4 GPU: 7-10å°æ—¶
  åŠ é€Ÿæ¯”: 2-3Ã—
```

### Lossé¢„æœŸ
```yaml
åˆå§‹:
  loss_mask: ~2.0
  loss_dice: ~0.5
  llm_loss: ~1.0
  total: ~3.5

æ”¶æ•› (epoch 10):
  loss_mask: ~0.3-0.5
  loss_dice: ~0.1-0.2
  llm_loss: ~0.3-0.5
  total: ~0.8-1.2

ç›®æ ‡: Val Dice > 0.80
```

---

## ğŸ” ç›‘æ§æŒ‡æ ‡

### å…³é”®æ—¥å¿—
```bash
# æŸ¥çœ‹åˆå§‹åŒ–
grep "GRADIENT STATUS" vessel_lora_training_4gpu.log

# æŸ¥çœ‹æ•°æ®åŠ è½½
grep "Loading" vessel_lora_training_4gpu.log | tail -20

# æŸ¥çœ‹è®­ç»ƒæ­¥éª¤
grep "iter:" vessel_lora_training_4gpu.log | tail -20

# æŸ¥çœ‹é”™è¯¯
grep -i "error\|exception\|traceback" vessel_lora_training_4gpu.log
```

### Checkpoint
```bash
# æŸ¥çœ‹ä¿å­˜çš„checkpoint
ls -lh work_dirs/sa2va_vessel_lora_finetune/

# æ¯500æ­¥ä¿å­˜ä¸€æ¬¡
# æœ€å¤šä¿ç•™3ä¸ªcheckpoint
```

---

## âš ï¸ æ•…éšœæ’æŸ¥

### å¦‚æœè¿˜æ˜¯OOM

**æ–¹æ¡ˆ1: å‡å°‘batch size**
```python
# ä¿®æ”¹é…ç½®æ–‡ä»¶
batch_size = 1
accumulative_counts = 1  # ä»2æ”¹ä¸º1
max_length = 4096  # ä»8192å‡åŠ
```

**æ–¹æ¡ˆ2: å¯ç”¨æ›´æ¿€è¿›çš„offload**
```json
{
  "zero_optimization": {
    "stage": 3,  // ZeRO-3: åˆ†ç‰‡æ‰€æœ‰å‚æ•°
    "offload_param": {
      "device": "cpu"
    },
    "offload_optimizer": {
      "device": "cpu"
    }
  }
}
```

**æ–¹æ¡ˆ3: å‡å°‘æ•°æ®åŠ è½½**
```python
dataloader_num_workers = 0  # ä¸ä½¿ç”¨å¤šè¿›ç¨‹
repeats = 2  # è¿›ä¸€æ­¥å‡å°‘é‡å¤
```

### å¦‚æœè®­ç»ƒå¤ªæ…¢

```python
# å¢åŠ accumulation
accumulative_counts = 4  # æ›´å¤§çš„æœ‰æ•ˆbatch

# å‡å°‘éªŒè¯é¢‘ç‡
save_steps = 1000  # ä»500æ”¹ä¸º1000

# å‡å°‘epoch
max_epochs = 5  # ä»10æ”¹ä¸º5
```

### å¦‚æœè¿›ç¨‹å¡ä½

```bash
# æ£€æŸ¥æ‰€æœ‰GPUè¿›ç¨‹
fuser -v /dev/nvidia*

# æ¸…ç†åƒµå°¸è¿›ç¨‹
pkill -9 -f train.py
pkill -9 -f deepspeed

# æ¸…ç†NCCL
pkill -9 -f nccl
```

---

## ğŸ“Š ä¸å•GPUå¯¹æ¯”

| æŒ‡æ ‡ | å•GPU | 4 GPU | æå‡ |
|------|-------|-------|------|
| æ˜¾å­˜å ç”¨ | 23GB (OOM) | 14GB/GPU | -40% |
| è®­ç»ƒé€Ÿåº¦ | OOM | ~4-5s/iter | N/A |
| æ€»æ—¶é—´ | N/A | 7-10å°æ—¶ | N/A |
| æœ‰æ•ˆbatch | 4 | 8 | 2Ã— |

---

## âœ… æ£€æŸ¥æ¸…å•

è®­ç»ƒå¯åŠ¨åæ£€æŸ¥ï¼š

- [ ] 4ä¸ªGPUéƒ½åœ¨ä½¿ç”¨ï¼ˆnvidia-smiï¼‰
- [ ] æ˜¾å­˜å ç”¨<20GB/GPU
- [ ] æ—¥å¿—ä¸­æœ‰"iter: X, loss: Y"
- [ ] æ²¡æœ‰OOMé”™è¯¯
- [ ] Lossåœ¨ä¸‹é™
- [ ] Checkpointæ­£å¸¸ä¿å­˜

---

## ğŸ¯ æˆåŠŸæ ‡å¿—

```bash
# è®­ç»ƒæ­£å¸¸çš„æ—¥å¿—ç¤ºä¾‹
iter: 10, loss_mask: 1.8, loss_dice: 0.45, llm_loss: 0.9, total: 3.15
iter: 20, loss_mask: 1.6, loss_dice: 0.42, llm_loss: 0.85, total: 2.87
iter: 30, loss_mask: 1.5, loss_dice: 0.40, llm_loss: 0.80, total: 2.70
...

# GPUä½¿ç”¨æ­£å¸¸
GPU 0: 14GB / 24GB
GPU 1: 14GB / 24GB
GPU 2: 14GB / 24GB
GPU 3: 14GB / 24GB
```

---

## ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶

```
/home/ubuntu/Sa2VA/
â”œâ”€â”€ vessel_lora_training_4gpu.log       # è®­ç»ƒæ—¥å¿—
â”œâ”€â”€ deepspeed_zero2_offload.json        # DeepSpeedé…ç½®
â”œâ”€â”€ work_dirs/
â”‚   â””â”€â”€ sa2va_vessel_lora_finetune/
â”‚       â”œâ”€â”€ iter_500.pth                 # Checkpoint
â”‚       â”œâ”€â”€ iter_1000.pth
â”‚       â””â”€â”€ ...
â””â”€â”€ projects/sa2va/configs/
    â””â”€â”€ sa2va_vessel_lora_finetune.py   # è®­ç»ƒé…ç½®
```

---

## ğŸš€ ä¸‹ä¸€æ­¥

1. **ç­‰å¾…åˆå§‹åŒ–å®Œæˆ** (~2-3åˆ†é’Ÿ)
   - æ¨¡å‹åŠ è½½åˆ°æ‰€æœ‰GPU
   - DeepSpeedåˆå§‹åŒ–
   - æ•°æ®åŠ è½½å™¨å‡†å¤‡

2. **ç›‘æ§å‰100æ­¥**
   - ç¡®è®¤losså¼€å§‹ä¸‹é™
   - æ£€æŸ¥æ˜¾å­˜ç¨³å®š
   - éªŒè¯é€Ÿåº¦ç¬¦åˆé¢„æœŸ

3. **é•¿æœŸç›‘æ§**
   - æ¯å°æ—¶æ£€æŸ¥ä¸€æ¬¡è¿›åº¦
   - é¢„è®¡7-10å°æ—¶å®Œæˆ
   - å…³æ³¨Val DiceæŒ‡æ ‡

---

**å½“å‰çŠ¶æ€**: ğŸŸ¢ è®­ç»ƒå·²å¯åŠ¨  
**æ—¥å¿—ä½ç½®**: `vessel_lora_training_4gpu.log`  
**é¢„è®¡å®Œæˆ**: ~10å°æ—¶å
