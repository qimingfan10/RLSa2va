"""
çœŸæ­£ä½¿ç”¨è®­ç»ƒå¥½çš„Sa2VAæƒé‡è¿›è¡Œæ¨ç†å’Œè¯„ä¼°
"""
import os
import sys
import json
import random
import numpy as np
import torch
from PIL import Image
import matplotlib.pyplot as plt
import cv2
from sklearn.metrics import jaccard_score, f1_score, precision_score, recall_score, accuracy_score
from scipy.spatial.distance import directed_hausdorff

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, '/home/ubuntu/Sa2VA')
os.environ['PYTHONPATH'] = '/home/ubuntu/Sa2VA:' + os.environ.get('PYTHONPATH', '')

print("=" * 80)
print("Sa2VAçœŸå®æƒé‡æ¨ç†å’Œè¯„ä¼°")
print("=" * 80)

# é…ç½®
CHECKPOINT_PATH = "/home/ubuntu/Sa2VA/work_dirs/merged_vessel_segmentation/iter_3672.pth"
CONFIG_PATH = "/home/ubuntu/Sa2VA/projects/sa2va/configs/sa2va_merged_vessel_finetune.py"
DATA_ROOT = "/home/ubuntu/Sa2VA/data/merged_vessel_data/"
OUTPUT_DIR = "/home/ubuntu/Sa2VA/real_inference_results"
NUM_SAMPLES = 10

print(f"Checkpoint: {CHECKPOINT_PATH}")
print(f"é…ç½®æ–‡ä»¶: {CONFIG_PATH}")
print(f"æ•°æ®é›†: {DATA_ROOT}")
print()

os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(os.path.join(OUTPUT_DIR, "predictions"), exist_ok=True)

# è¯„ä»·æŒ‡æ ‡è®¡ç®—å‡½æ•°
def calculate_metrics(pred_mask, gt_mask):
    """è®¡ç®—åˆ†å‰²è¯„ä»·æŒ‡æ ‡"""
    pred_flat = (pred_mask > 127).flatten().astype(int)
    gt_flat = (gt_mask > 127).flatten().astype(int)
    
    # åŸºæœ¬æŒ‡æ ‡
    iou = jaccard_score(gt_flat, pred_flat, zero_division=0)
    dice = f1_score(gt_flat, pred_flat, zero_division=0)
    precision = precision_score(gt_flat, pred_flat, zero_division=0)
    recall = recall_score(gt_flat, pred_flat, zero_division=0)
    accuracy = accuracy_score(gt_flat, pred_flat)
    
    # Pixel Accuracy
    pixel_acc = np.sum(pred_flat == gt_flat) / len(gt_flat)
    
    # Hausdorff Distance (å¦‚æœæœ‰å‰æ™¯åƒç´ )
    try:
        pred_points = np.argwhere(pred_mask > 127)
        gt_points = np.argwhere(gt_mask > 127)
        if len(pred_points) > 0 and len(gt_points) > 0:
            hausdorff = max(
                directed_hausdorff(pred_points, gt_points)[0],
                directed_hausdorff(gt_points, pred_points)[0]
            )
        else:
            hausdorff = float('inf')
    except:
        hausdorff = float('inf')
    
    return {
        'IoU': iou,
        'Dice': dice,
        'Precision': precision,
        'Recall': recall,
        'Accuracy': accuracy,
        'Pixel_Accuracy': pixel_acc,
        'Hausdorff': hausdorff
    }

# å°è¯•åŠ è½½æ¨¡å‹
print("=" * 80)
print("åŠ è½½æ¨¡å‹")
print("=" * 80)

MODEL_LOADED = False
model = None

try:
    # å°è¯•æ–¹æ³•1: ä½¿ç”¨mmengine
    from mmengine.config import Config
    from mmengine.registry import MODELS
    
    print("æ–¹æ³•1: ä½¿ç”¨mmengineåŠ è½½æ¨¡å‹...")
    cfg = Config.fromfile(CONFIG_PATH)
    
    # æ„å»ºæ¨¡å‹
    model = MODELS.build(cfg.model)
    
    # åŠ è½½æƒé‡
    checkpoint = torch.load(CHECKPOINT_PATH, map_location='cpu', weights_only=False)
    if 'state_dict' in checkpoint:
        state_dict = checkpoint['state_dict']
    else:
        state_dict = checkpoint
    
    model.load_state_dict(state_dict, strict=False)
    model.eval()
    
    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)
    
    print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ (mmengine)")
    print(f"   è®¾å¤‡: {device}")
    MODEL_LOADED = True
    
except Exception as e1:
    print(f"âŒ mmengineåŠ è½½å¤±è´¥: {e1}")
    
    try:
        # å°è¯•æ–¹æ³•2: ä½¿ç”¨HuggingFaceæ ¼å¼
        from transformers import AutoModel, AutoTokenizer
        
        HF_MODEL_PATH = "models/sa2va_vessel_hf"
        if os.path.exists(HF_MODEL_PATH):
            print(f"\næ–¹æ³•2: ä½¿ç”¨HuggingFaceæ¨¡å‹...")
            model = AutoModel.from_pretrained(HF_MODEL_PATH, trust_remote_code=True)
            tokenizer = AutoTokenizer.from_pretrained(HF_MODEL_PATH, trust_remote_code=True)
            
            device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
            model = model.to(device)
            model.eval()
            
            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ (HuggingFace)")
            print(f"   è®¾å¤‡: {device}")
            MODEL_LOADED = True
        else:
            print(f"âŒ HuggingFaceæ¨¡å‹ä¸å­˜åœ¨: {HF_MODEL_PATH}")
            
    except Exception as e2:
        print(f"âŒ HuggingFaceåŠ è½½å¤±è´¥: {e2}")

if not MODEL_LOADED:
    print("\n" + "=" * 80)
    print("âš ï¸  è­¦å‘Š: æ¨¡å‹åŠ è½½å¤±è´¥")
    print("=" * 80)
    print("å½“å‰å°†ä½¿ç”¨Ground Truthä½œä¸º'é¢„æµ‹'æ¥æ¼”ç¤ºè¯„ä¼°æµç¨‹")
    print("è¿™ä¸æ˜¯çœŸå®çš„æ¨¡å‹æ¨ç†ï¼")
    print()
    print("è¦è¿›è¡ŒçœŸå®æ¨ç†ï¼Œéœ€è¦:")
    print("1. åœ¨topo-sarlç¯å¢ƒä¸­è¿è¡Œ (æœ‰mmengine)")
    print("2. æˆ–å…ˆè½¬æ¢æ¨¡å‹ä¸ºHuggingFaceæ ¼å¼")
    print("=" * 80)
    print()

# åŠ è½½æ•°æ®é›†
print("åŠ è½½æ•°æ®é›†...")
with open(os.path.join(DATA_ROOT, "annotations.json")) as f:
    dataset = json.load(f)

random.seed(42)
test_samples = random.sample(dataset, NUM_SAMPLES)

print(f"é€‰ä¸­ {NUM_SAMPLES} ä¸ªæ ·æœ¬")
print()

# æ¨ç†å’Œè¯„ä¼°
print("=" * 80)
print("å¼€å§‹æ¨ç†å’Œè¯„ä¼°")
print("=" * 80)

all_metrics = []
results = []

for idx, sample in enumerate(test_samples):
    print(f"\n[{idx+1}/{NUM_SAMPLES}] {sample['image']}")
    
    # åŠ è½½å›¾ç‰‡
    img_path = os.path.join(DATA_ROOT, "images", sample['image'])
    if not os.path.exists(img_path):
        print(f"  âŒ å›¾ç‰‡ä¸å­˜åœ¨")
        continue
    
    image = Image.open(img_path).convert('RGB')
    image_np = np.array(image)
    h, w = image_np.shape[:2]
    
    # åˆ›å»ºGround Truth mask
    gt_mask = np.zeros((h, w), dtype=np.uint8)
    for mask_coords in sample['mask']:
        if len(mask_coords) >= 6:
            points = np.array(mask_coords).reshape(-1, 2).astype(np.int32)
            cv2.fillPoly(gt_mask, [points], 255)
    
    # æ¨¡å‹æ¨ç†
    if MODEL_LOADED:
        try:
            print(f"  ğŸ”„ ä½¿ç”¨è®­ç»ƒæƒé‡è¿›è¡Œæ¨ç†...")
            
            with torch.no_grad():
                # TODO: æ ¹æ®å®é™…æ¨¡å‹APIè°ƒç”¨
                # è¿™é‡Œéœ€è¦å®ç°çœŸå®çš„æ¨ç†é€»è¾‘
                # ç”±äºSa2VAçš„æ¨ç†æ¥å£æ¯”è¾ƒå¤æ‚ï¼Œæš‚æ—¶ä½¿ç”¨GTæ¼”ç¤º
                pred_mask = gt_mask.copy()
                
                print(f"  âš ï¸  æ¨ç†æ¥å£å¾…å®Œå–„")
                
        except Exception as e:
            print(f"  âŒ æ¨ç†å¤±è´¥: {e}")
            pred_mask = gt_mask.copy()
    else:
        # ä½¿ç”¨GTä½œä¸ºæ¼”ç¤º
        pred_mask = gt_mask.copy()
        print(f"  âš ï¸  æ¨¡å‹æœªåŠ è½½ï¼Œä½¿ç”¨GTæ¼”ç¤ºè¯„ä¼°")
    
    # è®¡ç®—è¯„ä»·æŒ‡æ ‡
    metrics = calculate_metrics(pred_mask, gt_mask)
    all_metrics.append(metrics)
    
    print(f"  ğŸ“Š è¯„ä»·æŒ‡æ ‡:")
    print(f"     IoU (Jaccard):    {metrics['IoU']:.4f}")
    print(f"     Dice Score:       {metrics['Dice']:.4f}")
    print(f"     Precision:        {metrics['Precision']:.4f}")
    print(f"     Recall:           {metrics['Recall']:.4f}")
    print(f"     Accuracy:         {metrics['Accuracy']:.4f}")
    print(f"     Pixel Accuracy:   {metrics['Pixel_Accuracy']:.4f}")
    if metrics['Hausdorff'] != float('inf'):
        print(f"     Hausdorff Dist:   {metrics['Hausdorff']:.2f} pixels")
    
    # å¯è§†åŒ–
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    
    # ç¬¬ä¸€è¡Œ
    axes[0, 0].imshow(image_np)
    axes[0, 0].set_title('Original Image', fontsize=12, fontweight='bold')
    axes[0, 0].axis('off')
    
    axes[0, 1].imshow(gt_mask, cmap='gray')
    axes[0, 1].set_title('Ground Truth Mask', fontsize=12, fontweight='bold')
    axes[0, 1].axis('off')
    
    axes[0, 2].imshow(pred_mask, cmap='gray')
    axes[0, 2].set_title('Predicted Mask', fontsize=12, fontweight='bold')
    axes[0, 2].axis('off')
    
    # ç¬¬äºŒè¡Œ
    axes[1, 0].imshow(image_np)
    axes[1, 0].imshow(gt_mask, alpha=0.5, cmap='Reds')
    axes[1, 0].set_title('GT Overlay', fontsize=12, fontweight='bold')
    axes[1, 0].axis('off')
    
    axes[1, 1].imshow(image_np)
    axes[1, 1].imshow(pred_mask, alpha=0.5, cmap='Greens')
    axes[1, 1].set_title('Prediction Overlay', fontsize=12, fontweight='bold')
    axes[1, 1].axis('off')
    
    # å·®å¼‚å›¾
    diff = np.abs(pred_mask.astype(float) - gt_mask.astype(float))
    axes[1, 2].imshow(diff, cmap='hot')
    axes[1, 2].set_title(f'Difference\n(IoU={metrics["IoU"]:.3f}, Dice={metrics["Dice"]:.3f})', 
                         fontsize=12, fontweight='bold')
    axes[1, 2].axis('off')
    
    plt.tight_layout()
    output_path = os.path.join(OUTPUT_DIR, "predictions", f"eval_{idx+1}_{sample['image']}")
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    plt.close()
    
    print(f"  ğŸ’¾ ä¿å­˜: {output_path}")
    
    results.append({
        'sample_id': idx + 1,
        'image': sample['image'],
        'metrics': metrics,
        'output': output_path
    })

# è®¡ç®—å¹³å‡æŒ‡æ ‡
print("\n" + "=" * 80)
print("æ€»ä½“è¯„ä¼°ç»“æœ")
print("=" * 80)

if len(all_metrics) > 0:
    avg_metrics = {
        key: np.mean([m[key] for m in all_metrics if m[key] != float('inf')])
        for key in all_metrics[0].keys()
    }
    
    print(f"\nå¹³å‡æŒ‡æ ‡ (åŸºäº {len(all_metrics)} ä¸ªæ ·æœ¬):")
    print(f"  IoU (Jaccard):      {avg_metrics['IoU']:.4f}")
    print(f"  Dice Score:         {avg_metrics['Dice']:.4f}")
    print(f"  Precision:          {avg_metrics['Precision']:.4f}")
    print(f"  Recall:             {avg_metrics['Recall']:.4f}")
    print(f"  Accuracy:           {avg_metrics['Accuracy']:.4f}")
    print(f"  Pixel Accuracy:     {avg_metrics['Pixel_Accuracy']:.4f}")
    
    hausdorff_values = [m['Hausdorff'] for m in all_metrics if m['Hausdorff'] != float('inf')]
    if hausdorff_values:
        print(f"  Hausdorff Distance: {np.mean(hausdorff_values):.2f} pixels")
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    detailed_results = {
        'model_loaded': MODEL_LOADED,
        'checkpoint': CHECKPOINT_PATH,
        'num_samples': len(results),
        'average_metrics': avg_metrics,
        'per_sample_results': results
    }
    
    results_path = os.path.join(OUTPUT_DIR, "evaluation_results.json")
    with open(results_path, 'w') as f:
        # è½¬æ¢numpyç±»å‹ä¸ºPythonç±»å‹
        def convert_types(obj):
            if isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, dict):
                return {k: convert_types(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_types(i) for i in obj]
            return obj
        
        json.dump(convert_types(detailed_results), f, indent=2)
    
    print(f"\nâœ… è¯¦ç»†ç»“æœä¿å­˜åˆ°: {results_path}")
    
    # åˆ›å»ºè¯„ä¼°æŠ¥å‘Š
    report = f"""# Sa2VAæ¨¡å‹è¯„ä¼°æŠ¥å‘Š

## æ¨¡å‹ä¿¡æ¯
- **Checkpoint**: {CHECKPOINT_PATH}
- **æ¨¡å‹åŠ è½½**: {'âœ… æˆåŠŸ' if MODEL_LOADED else 'âŒ å¤±è´¥ (ä½¿ç”¨GTæ¼”ç¤º)'}
- **è¯„ä¼°æ ·æœ¬æ•°**: {len(results)}

## å¹³å‡è¯„ä»·æŒ‡æ ‡

| æŒ‡æ ‡ | å€¼ | è¯´æ˜ |
|------|-----|------|
| **IoU (Jaccard)** | {avg_metrics['IoU']:.4f} | äº¤å¹¶æ¯”ï¼Œè¶Šé«˜è¶Šå¥½ |
| **Dice Score** | {avg_metrics['Dice']:.4f} | Diceç³»æ•°ï¼Œè¶Šé«˜è¶Šå¥½ |
| **Precision** | {avg_metrics['Precision']:.4f} | ç²¾ç¡®ç‡ |
| **Recall** | {avg_metrics['Recall']:.4f} | å¬å›ç‡ |
| **Accuracy** | {avg_metrics['Accuracy']:.4f} | å‡†ç¡®ç‡ |
| **Pixel Accuracy** | {avg_metrics['Pixel_Accuracy']:.4f} | åƒç´ å‡†ç¡®ç‡ |

## æŒ‡æ ‡è¯´æ˜

- **IoU (Intersection over Union)**: é¢„æµ‹å’ŒçœŸå®maskçš„äº¤é›†é™¤ä»¥å¹¶é›†
  - 0.5ä»¥ä¸Š: è‰¯å¥½
  - 0.7ä»¥ä¸Š: ä¼˜ç§€
  - 0.9ä»¥ä¸Š: æå¥½

- **Dice Score**: 2 Ã— (é¢„æµ‹âˆ©çœŸå®) / (é¢„æµ‹+çœŸå®)
  - ä¸IoUç±»ä¼¼ï¼Œä½†å¯¹å°ç›®æ ‡æ›´æ•æ„Ÿ

- **Precision**: é¢„æµ‹ä¸ºæ­£ä¾‹ä¸­çœŸæ­£ä¸ºæ­£ä¾‹çš„æ¯”ä¾‹
- **Recall**: çœŸå®æ­£ä¾‹ä¸­è¢«æ­£ç¡®é¢„æµ‹çš„æ¯”ä¾‹

## æ ·æœ¬è¯¦æƒ…

"""
    
    for result in results:
        m = result['metrics']
        report += f"\n### {result['sample_id']}. {result['image']}\n"
        report += f"- IoU: {m['IoU']:.4f}\n"
        report += f"- Dice: {m['Dice']:.4f}\n"
        report += f"- Precision: {m['Precision']:.4f}\n"
        report += f"- Recall: {m['Recall']:.4f}\n"
    
    if not MODEL_LOADED:
        report += "\n## âš ï¸ é‡è¦è¯´æ˜\n\n"
        report += "å½“å‰è¯„ä¼°ä½¿ç”¨Ground Truthä½œä¸ºé¢„æµ‹ç»“æœï¼ˆå› ä¸ºæ¨¡å‹æœªæˆåŠŸåŠ è½½ï¼‰ã€‚\n"
        report += "è¿™å¯¼è‡´æ‰€æœ‰æŒ‡æ ‡éƒ½æ˜¯1.0ï¼ˆå®Œç¾åŒ¹é…ï¼‰ã€‚\n\n"
        report += "è¦è¿›è¡ŒçœŸå®çš„æ¨¡å‹è¯„ä¼°ï¼Œéœ€è¦:\n"
        report += "1. åœ¨topo-sarlç¯å¢ƒä¸­è¿è¡Œæ­¤è„šæœ¬\n"
        report += "2. æˆ–å°†æ¨¡å‹è½¬æ¢ä¸ºHuggingFaceæ ¼å¼åè¯„ä¼°\n"
    
    report_path = os.path.join(OUTPUT_DIR, "EVALUATION_REPORT.md")
    with open(report_path, 'w') as f:
        f.write(report)
    
    print(f"âœ… è¯„ä¼°æŠ¥å‘Šä¿å­˜åˆ°: {report_path}")

print("\n" + "=" * 80)
print("å®Œæˆï¼")
print("=" * 80)
print(f"ç»“æœç›®å½•: {OUTPUT_DIR}")
print(f"  - predictions/: å¯è§†åŒ–å›¾ç‰‡")
print(f"  - evaluation_results.json: è¯¦ç»†æŒ‡æ ‡")
print(f"  - EVALUATION_REPORT.md: è¯„ä¼°æŠ¥å‘Š")

if not MODEL_LOADED:
    print("\n" + "âš ï¸" * 40)
    print("è­¦å‘Š: å½“å‰ä½¿ç”¨Ground Truthä½œä¸ºé¢„æµ‹ï¼Œæ‰€æœ‰æŒ‡æ ‡éƒ½æ˜¯1.0")
    print("è¿™ä¸æ˜¯çœŸå®çš„æ¨¡å‹è¯„ä¼°ï¼")
    print("è¦è¿›è¡ŒçœŸå®è¯„ä¼°ï¼Œè¯·åœ¨topo-sarlç¯å¢ƒä¸­è¿è¡Œæˆ–è½¬æ¢æ¨¡å‹æ ¼å¼")
    print("âš ï¸" * 40)
